{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_Agent_Rag\n"
     ]
    }
   ],
   "source": [
    "print('Multi_Agent_Rag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"openai_api_key\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"langsmith_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\n",
    "    \"attention_is_all_you_need.pdf\",\n",
    "    \"OOPS_Interview_Cheatsheet.pdf\",\n",
    "    \"DBMS_my_notes.pdf\",\n",
    "    \"OS_Full_Notes.pdf\",\n",
    "    \"DBMS_my_notes.pdf\",\n",
    "    \"SQL_notes.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Ignoring wrong pointing object 206 0 (offset 0)\n",
      "Ignoring wrong pointing object 210 0 (offset 0)\n",
      "Ignoring wrong pointing object 223 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 237 0 (offset 0)\n",
      "Ignoring wrong pointing object 241 0 (offset 0)\n",
      "Ignoring wrong pointing object 260 0 (offset 0)\n",
      "Ignoring wrong pointing object 262 0 (offset 0)\n",
      "Ignoring wrong pointing object 264 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 285 0 (offset 0)\n",
      "Ignoring wrong pointing object 296 0 (offset 0)\n",
      "Ignoring wrong pointing object 298 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 338 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 380 0 (offset 0)\n",
      "Ignoring wrong pointing object 383 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 399 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 412 0 (offset 0)\n",
      "Ignoring wrong pointing object 422 0 (offset 0)\n",
      "Ignoring wrong pointing object 440 0 (offset 0)\n",
      "Ignoring wrong pointing object 444 0 (offset 0)\n",
      "Ignoring wrong pointing object 447 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 453 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 464 0 (offset 0)\n",
      "Ignoring wrong pointing object 472 0 (offset 0)\n",
      "Ignoring wrong pointing object 474 0 (offset 0)\n",
      "Ignoring wrong pointing object 478 0 (offset 0)\n",
      "Ignoring wrong pointing object 485 0 (offset 0)\n",
      "Ignoring wrong pointing object 494 0 (offset 0)\n",
      "Ignoring wrong pointing object 496 0 (offset 0)\n",
      "Ignoring wrong pointing object 498 0 (offset 0)\n",
      "Ignoring wrong pointing object 511 0 (offset 0)\n",
      "Ignoring wrong pointing object 518 0 (offset 0)\n",
      "Ignoring wrong pointing object 528 0 (offset 0)\n",
      "Ignoring wrong pointing object 530 0 (offset 0)\n",
      "Ignoring wrong pointing object 535 0 (offset 0)\n",
      "Ignoring wrong pointing object 544 0 (offset 0)\n",
      "Ignoring wrong pointing object 553 0 (offset 0)\n",
      "Ignoring wrong pointing object 558 0 (offset 0)\n",
      "Ignoring wrong pointing object 561 0 (offset 0)\n",
      "Ignoring wrong pointing object 566 0 (offset 0)\n",
      "Ignoring wrong pointing object 571 0 (offset 0)\n",
      "Ignoring wrong pointing object 579 0 (offset 0)\n",
      "Ignoring wrong pointing object 588 0 (offset 0)\n",
      "Ignoring wrong pointing object 593 0 (offset 0)\n",
      "Ignoring wrong pointing object 600 0 (offset 0)\n",
      "Ignoring wrong pointing object 608 0 (offset 0)\n",
      "Ignoring wrong pointing object 625 0 (offset 0)\n",
      "Ignoring wrong pointing object 632 0 (offset 0)\n",
      "Ignoring wrong pointing object 638 0 (offset 0)\n",
      "Ignoring wrong pointing object 655 0 (offset 0)\n",
      "Ignoring wrong pointing object 658 0 (offset 0)\n",
      "Ignoring wrong pointing object 661 0 (offset 0)\n",
      "Ignoring wrong pointing object 664 0 (offset 0)\n",
      "Ignoring wrong pointing object 666 0 (offset 0)\n",
      "Ignoring wrong pointing object 669 0 (offset 0)\n",
      "Ignoring wrong pointing object 672 0 (offset 0)\n",
      "Ignoring wrong pointing object 679 0 (offset 0)\n",
      "Ignoring wrong pointing object 684 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 696 0 (offset 0)\n",
      "Ignoring wrong pointing object 705 0 (offset 0)\n",
      "Ignoring wrong pointing object 716 0 (offset 0)\n",
      "Ignoring wrong pointing object 721 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 755 0 (offset 0)\n",
      "Ignoring wrong pointing object 758 0 (offset 0)\n",
      "Ignoring wrong pointing object 761 0 (offset 0)\n",
      "Ignoring wrong pointing object 764 0 (offset 0)\n",
      "Ignoring wrong pointing object 768 0 (offset 0)\n",
      "Ignoring wrong pointing object 772 0 (offset 0)\n",
      "Ignoring wrong pointing object 778 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 792 0 (offset 0)\n",
      "Ignoring wrong pointing object 797 0 (offset 0)\n",
      "Ignoring wrong pointing object 800 0 (offset 0)\n",
      "Ignoring wrong pointing object 803 0 (offset 0)\n",
      "Ignoring wrong pointing object 807 0 (offset 0)\n",
      "Ignoring wrong pointing object 811 0 (offset 0)\n",
      "Ignoring wrong pointing object 814 0 (offset 0)\n",
      "Ignoring wrong pointing object 822 0 (offset 0)\n",
      "Ignoring wrong pointing object 824 0 (offset 0)\n",
      "Ignoring wrong pointing object 830 0 (offset 0)\n",
      "Ignoring wrong pointing object 835 0 (offset 0)\n",
      "Ignoring wrong pointing object 838 0 (offset 0)\n",
      "Ignoring wrong pointing object 858 0 (offset 0)\n",
      "Ignoring wrong pointing object 863 0 (offset 0)\n",
      "Ignoring wrong pointing object 867 0 (offset 0)\n",
      "Ignoring wrong pointing object 871 0 (offset 0)\n",
      "Ignoring wrong pointing object 875 0 (offset 0)\n",
      "Ignoring wrong pointing object 882 0 (offset 0)\n",
      "Ignoring wrong pointing object 895 0 (offset 0)\n",
      "Ignoring wrong pointing object 902 0 (offset 0)\n",
      "Ignoring wrong pointing object 909 0 (offset 0)\n",
      "Ignoring wrong pointing object 917 0 (offset 0)\n",
      "Ignoring wrong pointing object 934 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 940 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 955 0 (offset 0)\n",
      "Ignoring wrong pointing object 958 0 (offset 0)\n",
      "Ignoring wrong pointing object 965 0 (offset 0)\n",
      "Ignoring wrong pointing object 967 0 (offset 0)\n",
      "Ignoring wrong pointing object 970 0 (offset 0)\n",
      "Ignoring wrong pointing object 978 0 (offset 0)\n",
      "Ignoring wrong pointing object 980 0 (offset 0)\n",
      "Ignoring wrong pointing object 984 0 (offset 0)\n",
      "Ignoring wrong pointing object 1008 0 (offset 0)\n",
      "Ignoring wrong pointing object 1010 0 (offset 0)\n",
      "Ignoring wrong pointing object 1014 0 (offset 0)\n",
      "Ignoring wrong pointing object 1047 0 (offset 0)\n",
      "Ignoring wrong pointing object 1049 0 (offset 0)\n",
      "Ignoring wrong pointing object 1051 0 (offset 0)\n",
      "Ignoring wrong pointing object 1055 0 (offset 0)\n",
      "Ignoring wrong pointing object 1058 0 (offset 0)\n",
      "Ignoring wrong pointing object 1087 0 (offset 0)\n",
      "Ignoring wrong pointing object 1115 0 (offset 0)\n",
      "Ignoring wrong pointing object 1123 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1130 0 (offset 0)\n",
      "Ignoring wrong pointing object 1143 0 (offset 0)\n",
      "Ignoring wrong pointing object 1160 0 (offset 0)\n",
      "Ignoring wrong pointing object 1162 0 (offset 0)\n",
      "Ignoring wrong pointing object 1166 0 (offset 0)\n",
      "Ignoring wrong pointing object 1189 0 (offset 0)\n",
      "Ignoring wrong pointing object 1191 0 (offset 0)\n",
      "Ignoring wrong pointing object 1197 0 (offset 0)\n",
      "Ignoring wrong pointing object 1234 0 (offset 0)\n",
      "Ignoring wrong pointing object 1236 0 (offset 0)\n",
      "Ignoring wrong pointing object 1240 0 (offset 0)\n",
      "Ignoring wrong pointing object 1291 0 (offset 0)\n",
      "Ignoring wrong pointing object 1297 0 (offset 0)\n",
      "Ignoring wrong pointing object 1319 0 (offset 0)\n",
      "Ignoring wrong pointing object 1329 0 (offset 0)\n",
      "Ignoring wrong pointing object 1337 0 (offset 0)\n",
      "Ignoring wrong pointing object 1351 0 (offset 0)\n",
      "Ignoring wrong pointing object 1359 0 (offset 0)\n",
      "Ignoring wrong pointing object 1375 0 (offset 0)\n",
      "Ignoring wrong pointing object 1380 0 (offset 0)\n",
      "Ignoring wrong pointing object 1387 0 (offset 0)\n",
      "Ignoring wrong pointing object 1403 0 (offset 0)\n",
      "Ignoring wrong pointing object 1408 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 93 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Ignoring wrong pointing object 206 0 (offset 0)\n",
      "Ignoring wrong pointing object 210 0 (offset 0)\n",
      "Ignoring wrong pointing object 223 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 237 0 (offset 0)\n",
      "Ignoring wrong pointing object 241 0 (offset 0)\n",
      "Ignoring wrong pointing object 260 0 (offset 0)\n",
      "Ignoring wrong pointing object 262 0 (offset 0)\n",
      "Ignoring wrong pointing object 264 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 285 0 (offset 0)\n",
      "Ignoring wrong pointing object 296 0 (offset 0)\n",
      "Ignoring wrong pointing object 298 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 338 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 380 0 (offset 0)\n",
      "Ignoring wrong pointing object 383 0 (offset 0)\n",
      "Ignoring wrong pointing object 386 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 399 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 412 0 (offset 0)\n",
      "Ignoring wrong pointing object 422 0 (offset 0)\n",
      "Ignoring wrong pointing object 440 0 (offset 0)\n",
      "Ignoring wrong pointing object 444 0 (offset 0)\n",
      "Ignoring wrong pointing object 447 0 (offset 0)\n",
      "Ignoring wrong pointing object 450 0 (offset 0)\n",
      "Ignoring wrong pointing object 453 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n",
      "Ignoring wrong pointing object 461 0 (offset 0)\n",
      "Ignoring wrong pointing object 464 0 (offset 0)\n",
      "Ignoring wrong pointing object 472 0 (offset 0)\n",
      "Ignoring wrong pointing object 474 0 (offset 0)\n",
      "Ignoring wrong pointing object 478 0 (offset 0)\n",
      "Ignoring wrong pointing object 485 0 (offset 0)\n",
      "Ignoring wrong pointing object 494 0 (offset 0)\n",
      "Ignoring wrong pointing object 496 0 (offset 0)\n",
      "Ignoring wrong pointing object 498 0 (offset 0)\n",
      "Ignoring wrong pointing object 511 0 (offset 0)\n",
      "Ignoring wrong pointing object 518 0 (offset 0)\n",
      "Ignoring wrong pointing object 528 0 (offset 0)\n",
      "Ignoring wrong pointing object 530 0 (offset 0)\n",
      "Ignoring wrong pointing object 535 0 (offset 0)\n",
      "Ignoring wrong pointing object 544 0 (offset 0)\n",
      "Ignoring wrong pointing object 553 0 (offset 0)\n",
      "Ignoring wrong pointing object 558 0 (offset 0)\n",
      "Ignoring wrong pointing object 561 0 (offset 0)\n",
      "Ignoring wrong pointing object 566 0 (offset 0)\n",
      "Ignoring wrong pointing object 571 0 (offset 0)\n",
      "Ignoring wrong pointing object 579 0 (offset 0)\n",
      "Ignoring wrong pointing object 588 0 (offset 0)\n",
      "Ignoring wrong pointing object 593 0 (offset 0)\n",
      "Ignoring wrong pointing object 600 0 (offset 0)\n",
      "Ignoring wrong pointing object 608 0 (offset 0)\n",
      "Ignoring wrong pointing object 625 0 (offset 0)\n",
      "Ignoring wrong pointing object 632 0 (offset 0)\n",
      "Ignoring wrong pointing object 638 0 (offset 0)\n",
      "Ignoring wrong pointing object 655 0 (offset 0)\n",
      "Ignoring wrong pointing object 658 0 (offset 0)\n",
      "Ignoring wrong pointing object 661 0 (offset 0)\n",
      "Ignoring wrong pointing object 664 0 (offset 0)\n",
      "Ignoring wrong pointing object 666 0 (offset 0)\n",
      "Ignoring wrong pointing object 669 0 (offset 0)\n",
      "Ignoring wrong pointing object 672 0 (offset 0)\n",
      "Ignoring wrong pointing object 679 0 (offset 0)\n",
      "Ignoring wrong pointing object 684 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 696 0 (offset 0)\n",
      "Ignoring wrong pointing object 705 0 (offset 0)\n",
      "Ignoring wrong pointing object 716 0 (offset 0)\n",
      "Ignoring wrong pointing object 721 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 755 0 (offset 0)\n",
      "Ignoring wrong pointing object 758 0 (offset 0)\n",
      "Ignoring wrong pointing object 761 0 (offset 0)\n",
      "Ignoring wrong pointing object 764 0 (offset 0)\n",
      "Ignoring wrong pointing object 768 0 (offset 0)\n",
      "Ignoring wrong pointing object 772 0 (offset 0)\n",
      "Ignoring wrong pointing object 778 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 792 0 (offset 0)\n",
      "Ignoring wrong pointing object 797 0 (offset 0)\n",
      "Ignoring wrong pointing object 800 0 (offset 0)\n",
      "Ignoring wrong pointing object 803 0 (offset 0)\n",
      "Ignoring wrong pointing object 807 0 (offset 0)\n",
      "Ignoring wrong pointing object 811 0 (offset 0)\n",
      "Ignoring wrong pointing object 814 0 (offset 0)\n",
      "Ignoring wrong pointing object 822 0 (offset 0)\n",
      "Ignoring wrong pointing object 824 0 (offset 0)\n",
      "Ignoring wrong pointing object 830 0 (offset 0)\n",
      "Ignoring wrong pointing object 835 0 (offset 0)\n",
      "Ignoring wrong pointing object 838 0 (offset 0)\n",
      "Ignoring wrong pointing object 858 0 (offset 0)\n",
      "Ignoring wrong pointing object 863 0 (offset 0)\n",
      "Ignoring wrong pointing object 867 0 (offset 0)\n",
      "Ignoring wrong pointing object 871 0 (offset 0)\n",
      "Ignoring wrong pointing object 875 0 (offset 0)\n",
      "Ignoring wrong pointing object 882 0 (offset 0)\n",
      "Ignoring wrong pointing object 895 0 (offset 0)\n",
      "Ignoring wrong pointing object 902 0 (offset 0)\n",
      "Ignoring wrong pointing object 909 0 (offset 0)\n",
      "Ignoring wrong pointing object 917 0 (offset 0)\n",
      "Ignoring wrong pointing object 934 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 940 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 955 0 (offset 0)\n",
      "Ignoring wrong pointing object 958 0 (offset 0)\n",
      "Ignoring wrong pointing object 965 0 (offset 0)\n",
      "Ignoring wrong pointing object 967 0 (offset 0)\n",
      "Ignoring wrong pointing object 970 0 (offset 0)\n",
      "Ignoring wrong pointing object 978 0 (offset 0)\n",
      "Ignoring wrong pointing object 980 0 (offset 0)\n",
      "Ignoring wrong pointing object 984 0 (offset 0)\n",
      "Ignoring wrong pointing object 1008 0 (offset 0)\n",
      "Ignoring wrong pointing object 1010 0 (offset 0)\n",
      "Ignoring wrong pointing object 1014 0 (offset 0)\n",
      "Ignoring wrong pointing object 1047 0 (offset 0)\n",
      "Ignoring wrong pointing object 1049 0 (offset 0)\n",
      "Ignoring wrong pointing object 1051 0 (offset 0)\n",
      "Ignoring wrong pointing object 1055 0 (offset 0)\n",
      "Ignoring wrong pointing object 1058 0 (offset 0)\n",
      "Ignoring wrong pointing object 1087 0 (offset 0)\n",
      "Ignoring wrong pointing object 1115 0 (offset 0)\n",
      "Ignoring wrong pointing object 1123 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1130 0 (offset 0)\n",
      "Ignoring wrong pointing object 1143 0 (offset 0)\n",
      "Ignoring wrong pointing object 1160 0 (offset 0)\n",
      "Ignoring wrong pointing object 1162 0 (offset 0)\n",
      "Ignoring wrong pointing object 1166 0 (offset 0)\n",
      "Ignoring wrong pointing object 1189 0 (offset 0)\n",
      "Ignoring wrong pointing object 1191 0 (offset 0)\n",
      "Ignoring wrong pointing object 1197 0 (offset 0)\n",
      "Ignoring wrong pointing object 1234 0 (offset 0)\n",
      "Ignoring wrong pointing object 1236 0 (offset 0)\n",
      "Ignoring wrong pointing object 1240 0 (offset 0)\n",
      "Ignoring wrong pointing object 1291 0 (offset 0)\n",
      "Ignoring wrong pointing object 1297 0 (offset 0)\n",
      "Ignoring wrong pointing object 1319 0 (offset 0)\n",
      "Ignoring wrong pointing object 1329 0 (offset 0)\n",
      "Ignoring wrong pointing object 1337 0 (offset 0)\n",
      "Ignoring wrong pointing object 1351 0 (offset 0)\n",
      "Ignoring wrong pointing object 1359 0 (offset 0)\n",
      "Ignoring wrong pointing object 1375 0 (offset 0)\n",
      "Ignoring wrong pointing object 1380 0 (offset 0)\n",
      "Ignoring wrong pointing object 1387 0 (offset 0)\n",
      "Ignoring wrong pointing object 1403 0 (offset 0)\n",
      "Ignoring wrong pointing object 1408 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in pdf_files:\n",
    "\n",
    "    loader=PyPDFLoader(pdf_file)\n",
    "    documents=loader.load()\n",
    "    \n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "    split_documents=text_splitter.split_documents(documents)\n",
    "    \n",
    "    db_all=Chroma.from_documents(split_documents,OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_community.document_loaders import PyPDFLoader\\nloader=PyPDFLoader(\"attention_is_all_you_need.pdf\")\\nAttention=loader.load()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "Attention=loader.load()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\ntext_splitter=RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=200)\\nAttention_split=text_splitter.split_documents(Attention)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=200)\n",
    "Attention_split=text_splitter.split_documents(Attention)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain_openai import OpenAIEmbeddings\\nfrom langchain_community.vectorstores import Chroma\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAttention_db=Chroma.from_documents(Attention_split,OpenAIEmbeddings())\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Attention_db=Chroma.from_documents(Attention_split,OpenAIEmbeddings())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F1D700FD70>, search_kwargs={})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_retriever=db_all.as_retriever()\n",
    "pdf_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "pdf_retriever_tool=create_retriever_tool(pdf_retriever,\"Pdf_search\",\"A tool to search for and extract information from PDF documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='Pdf_search', description='A tool to search for and extract information from PDF documents', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001F1CE8EE8E0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F1D700FD70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001F1CEA704A0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F1D700FD70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=[pdf_retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4-turbo\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "Prompt=hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "Prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=False, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F1BE3D8A40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001F1BE3D8A40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001F1D7D32960>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001F1D612D400>, root_client=<openai.OpenAI object at 0x000001F1D696A1B0>, root_async_client=<openai.AsyncOpenAI object at 0x000001F1D700E3C0>, model_name='gpt-4-turbo', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Pdf_search', 'description': 'A tool to search for and extract information from PDF documents', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='Pdf_search', description='A tool to search for and extract information from PDF documents', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001F1CE8EE8E0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F1D700FD70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001F1CEA704A0>, retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001F1D700FD70>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n'))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=False)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Who are authors of paper Attention is all you need ?',\n",
       " 'output': 'The authors of the paper \"Attention is All You Need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Who are authors of paper Attention is all you need ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me two things 1)Who are authors of paper Attention is all you need ? and 2) What is attention ?',\n",
       " 'output': '### 1) Authors of the paper \"Attention is All You Need\"\\nThe authors of the paper \"Attention is All You Need\" are:\\n- Ashish Vaswani\\n- Noam Shazeer\\n- Niki Parmar\\n- Jakob Uszkoreit\\n- Llion Jones\\n- Aidan N. Gomez\\n- Łukasz Kaiser\\n- Illia Polosukhin\\n\\n### 2) What is Attention?\\nThe concept of \"attention\" in the context of the paper \"Attention is All You Need\" refers to a mechanism in neural networks that allows the model to focus on different parts of the input sequence when predicting parts of the output sequence. This mechanism is crucial for tasks such as translation, where the relevance of input elements can vary depending on the context. The attention mechanism helps the model to dynamically weigh the importance of input elements and is a key component of the Transformer architecture introduced in the paper.'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me two things 1)Who are authors of paper Attention is all you need ? and 2) What is attention ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Answer me various questions 1)Who are authors of paper Attention is all you need ? and 2) What is const keyword ? and 3) What is master slave database concept ? 4) Expalin difference between 32 bit and 64 bit OS ? 5)What are co-realted sub queries ?',\n",
       " 'output': 'Here are the answers to your questions:\\n\\n1) **Authors of the paper \"Attention is All You Need\"**:\\n   - Ashish Vaswani\\n   - Noam Shazeer\\n   - Niki Parmar\\n   - Jakob Uszkoreit\\n   - Llion Jones\\n   - Aidan N. Gomez\\n   - Łukasz Kaiser\\n   - Illia Polosukhin\\n\\n2) **What is the `const` keyword?**\\n   The `const` keyword in programming languages like C, C++, and JavaScript is used to declare variables that cannot be reassigned after their initial assignment. This means that the value of a `const` variable must remain constant throughout the scope in which it is declared. In C and C++, `const` can also be used to make function parameters and object methods promise not to modify the data they are called with.\\n\\n3) **Master-Slave Database Concept**:\\n   The master-slave database architecture is a replication model where one database server (the master) handles all the write operations while one or more other servers (the slaves) handle all read operations. Data written to the master server is asynchronously replicated to the slave servers. This model helps in load balancing as read operations can be distributed across multiple servers, and it enhances data redundancy and availability.\\n\\n4) **Difference between 32-bit and 64-bit OS**:\\n   - **Memory Addressing**: A 32-bit operating system can address up to 4 GB of RAM, while a 64-bit operating system can theoretically address up to 16 exabytes of RAM, though practical limits are much lower and dependent on the system architecture.\\n   - **Performance**: 64-bit processors can handle more data per clock cycle, making them faster and more efficient with applications that can utilize the larger bit width.\\n   - **Software Compatibility**: 64-bit operating systems can run both 32-bit and 64-bit software (though the reverse is not true), and they are generally better at handling large amounts of memory efficiently.\\n\\n5) **Co-related Subqueries**:\\n   A co-related subquery is a type of SQL query where the inner query (subquery) depends on the outer query for its values. This means that the subquery uses values from the outer query to complete its execution. The subquery is executed repeatedly, once for each row that might be selected by the outer query. This can be useful for performing complex filters and conditions that depend on the values in each row of the main query\\'s table.\\n\\nThese answers should provide a comprehensive overview of the topics you asked about!'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Answer me various questions 1)Who are authors of paper Attention is all you need ? and 2) What is const keyword ? and 3) What is master slave database concept ? 4) Expalin difference between 32 bit and 64 bit OS ? 5)What are co-realted sub queries ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Answer me various questions in short answers only max one line per question 1)Is Abhi Arora authors of paper Attention is all you need ? and 2) Are joins faster than subqueries ? and 3) Which keyword is used to modify a methond,property and indexer or event declaration ? 4)How many unique memory address a 64 bit register can hold ? 5) Who is Varun ?',\n",
       " 'output': '1) No, Abhi Arora is not an author of the paper \"Attention is All You Need\"; the authors include Ashish Vaswani, Noam Shazeer, and others.\\n2) Joins are generally faster than subqueries as they maximize calculation burden on the DBMS.\\n3) The keyword used to modify a method, property, indexer, or event declaration is not specified in the provided data.\\n4) A 64-bit register can hold 2^64 unique memory addresses.\\n5) Information about \"Who is Varun?\" is not provided in the data.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Answer me various questions in short answers only max one line per question 1)Is Abhi Arora authors of paper Attention is all you need ? and 2) Are joins faster than subqueries ? and 3) Which keyword is used to modify a methond,property and indexer or event declaration ? 4)How many unique memory address a 64 bit register can hold ? 5) Who is Varun ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "llama_key=os.getenv(\"llama_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=LlamaParse(api_key=llama_key,result_type=\"markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id c631e86b-5f54-4aa8-a0ae-f76ea75b825b\n"
     ]
    }
   ],
   "source": [
    "documents=parser.load_data(\"pokemon_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt=OpenAI(model=\"gpt-4-turbo\")\n",
    "Settings.llm=gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index=VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kyurem Black Kyurem and Kyurem White Kyurem differ primarily in their attack and special attack stats. Kyurem Black Kyurem has a higher attack stat at 170 compared to Kyurem White Kyurem's 120, while Kyurem White Kyurem has a higher special attack stat at 170 compared to Kyurem Black Kyurem's 120. Both forms have the same HP, defense, special defense, and speed stats.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"Difference between KyuremBlack Kyurem and KyuremWhite Kyurem ?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1) The stats for Ivysaur are as follows: HP: 60, Attack: 62, Defense: 63, Special Attack: 80, Special Defense: 80, Speed: 60.\\n\\n2) Comparing Lugia and Magmar:\\n   - Lugia's stats are: HP: 106, Attack: 90, Defense: 130, Special Attack: 90, Special Defense: 154, Speed: 110.\\n   - Magmar's stats are: HP: 65, Attack: 95, Defense: 57, Special Attack: 100, Special Defense: 85, Speed: 93.\\n\\nLugia is stronger than Magmar based on overall higher stats in HP, Defense, Special Defense, and Speed.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"Tell me two things 1)What are stats of Ivysaur and 2)Who is stronger Lugia or Magmar and also their stats ? ?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ivysaur has the following stats:\\n- HP: 60\\n- Attack: 62\\n- Defense: 63\\n- Special Attack: 80\\n- Special Defense: 80\\n- Speed: 60'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"What are stats of pokemon Ivysaur?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Starmie has the following stats: HP 60, Attack 75, Defense 85, Special Attack 100, Special Defense 85, Speed 115.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"What are stats of Starmie?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Golurk has the following stats:\\n- HP: 89\\n- Attack: 124\\n- Defense: 80\\n- Special Attack: 55\\n- Special Defense: 80\\n- Speed: 55'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=query_engine.query(\"What are stats of Golurk?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "def excel_search_tool(input_text:str):\n",
    "    response=query_engine.query(input_text)\n",
    "    return str(response)\n",
    "\n",
    "excel_retriever_tool=Tool(\n",
    "    name=\"Excel_search\",\n",
    "    func=excel_search_tool,\n",
    "    description=\"A tool to search for and extract information from Excel documents\"\n",
    ")\n",
    "\n",
    "tools=[pdf_retriever_tool,excel_retriever_tool]\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Pdf_search` with `{'query': 'Ivysaur stats'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "\n",
      "We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\n",
      "Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\n",
      "using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n",
      "[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\n",
      "for the semi-supervised setting.\n",
      "We performed only a small number of experiments to select the dropout, both attention and residual\n",
      "(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\n",
      "remained unchanged from the English-to-German base translation model. During inference, we\n",
      "9\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\n",
      "\n",
      "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\n",
      "of WSJ)\n",
      "Parser Training WSJ 23 F1\n",
      "Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\n",
      "Petrov et al. (2006) [29] WSJ only, discriminative 90.4\n",
      "Zhu et al. (2013) [40] WSJ only, discriminative 90.4\n",
      "Dyer et al. (2016) [8] WSJ only, discriminative 91.7\n",
      "Transformer (4 layers) WSJ only, discriminative 91.3\n",
      "Zhu et al. (2013) [40] semi-supervised 91.3\n",
      "Huang & Harper (2009) [14] semi-supervised 91.3\n",
      "McClosky et al. (2006) [26] semi-supervised 92.1\n",
      "Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\n",
      "Transformer (4 layers) semi-supervised 92.7\n",
      "Luong et al. (2015) [23] multi-task 93.0\n",
      "Dyer et al. (2016) [8] generative 93.3\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Pdf_search` with `{'query': 'Lugia stats'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "ModelBLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0·1020\n",
      "GNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\n",
      "ConvS2S [9] 25.16 40.46 9.6·10181.5·1020\n",
      "MoE [32] 26.03 40.56 2.0·10191.2·1020\n",
      "Deep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\n",
      "GNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\n",
      "ConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "\n",
      "Transformer (base model) 27.3 38.1 3.3·1018\n",
      "Transformer (big) 28.4 41.8 2.3·1019\n",
      "Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop= 0.1.\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Pdf_search` with `{'query': 'Magmar stats'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "\n",
      "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\n",
      "corpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\n",
      "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\n",
      "Proceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\n",
      "pages 152–159. ACL, June 2006.\n",
      "[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing , 2016.\n",
      "[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304 , 2017.\n",
      "\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\n",
      "\n",
      "the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\n",
      "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
      "13\u001b[0m\u001b[32;1m\u001b[1;3mIt seems there was an issue retrieving the specific stats for Ivysaur, Lugia, and Magmar from the PDF documents. The search results did not return relevant information regarding their stats or comparisons. \n",
      "\n",
      "To assist you better, you might want to check a dedicated Pokémon database or resource that provides detailed stats and comparisons between Pokémon like Lugia and Magmar. Websites such as Bulbapedia, Serebii, or the official Pokémon website can be very helpful for this kind of information.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me two things 1)What are stats of Ivysaur and 2)Who is stronger Lugia or Magmar and also their stats ? ?', 'output': 'It seems there was an issue retrieving the specific stats for Ivysaur, Lugia, and Magmar from the PDF documents. The search results did not return relevant information regarding their stats or comparisons. \\n\\nTo assist you better, you might want to check a dedicated Pokémon database or resource that provides detailed stats and comparisons between Pokémon like Lugia and Magmar. Websites such as Bulbapedia, Serebii, or the official Pokémon website can be very helpful for this kind of information.'}\n"
     ]
    }
   ],
   "source": [
    "response=agent_executor.invoke({\"input\":\"Tell me two things 1)What are stats of Ivysaur and 2)Who is stronger Lugia or Magmar and also their stats ? ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Pdf_search` with `{'query': 'authors of the paper Attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mProvided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n",
      "\n",
      "[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS) , 2016.\n",
      "[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR) , 2016.\n",
      "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n",
      "2017.\n",
      "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "InInternational Conference on Learning Representations , 2017.\n",
      "[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Pdf_search` with `{'query': 'definition of attention in the paper Attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\n",
      "and 6. Note that the attentions are very sharp for this word.\n",
      "14\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\u001b[0m\u001b[32;1m\u001b[1;3m### 1) Authors of the paper \"Attention is All You Need\"\n",
      "The authors of the paper \"Attention is All You Need\" are:\n",
      "- Ashish Vaswani\n",
      "- Noam Shazeer\n",
      "- Niki Parmar\n",
      "- Jakob Uszkoreit\n",
      "- Llion Jones\n",
      "- Aidan N. Gomez\n",
      "- Łukasz Kaiser\n",
      "- Illia Polosukhin\n",
      "\n",
      "### 2) Definition of Attention\n",
      "The search did not return a direct definition of \"attention\" from the paper. However, in the context of the paper, \"attention\" generally refers to a mechanism in neural networks that allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This mechanism is central to the Transformer model introduced in the paper, which relies solely on attention mechanisms without the use of recurrence or convolution.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me two things 1)Who are authors of paper Attention is all you need ? and 2) What is attention ?', 'output': '### 1) Authors of the paper \"Attention is All You Need\"\\nThe authors of the paper \"Attention is All You Need\" are:\\n- Ashish Vaswani\\n- Noam Shazeer\\n- Niki Parmar\\n- Jakob Uszkoreit\\n- Llion Jones\\n- Aidan N. Gomez\\n- Łukasz Kaiser\\n- Illia Polosukhin\\n\\n### 2) Definition of Attention\\nThe search did not return a direct definition of \"attention\" from the paper. However, in the context of the paper, \"attention\" generally refers to a mechanism in neural networks that allows the model to focus on different parts of the input sequence when generating each part of the output sequence. This mechanism is central to the Transformer model introduced in the paper, which relies solely on attention mechanisms without the use of recurrence or convolution.'}\n"
     ]
    }
   ],
   "source": [
    "response=agent_executor.invoke({\"input\":\"Tell me two things 1)Who are authors of paper Attention is all you need ? and 2) What is attention ?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=agent_executor.invoke({\"input\":\"Tell me two things 1)Who are authors of paper Attention is all you need ? and 2) What is attention ?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
