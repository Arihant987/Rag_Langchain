{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giskard_testing\n"
     ]
    }
   ],
   "source": [
    "print(\"giskard_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "loader=PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "attention=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=text_splitter.split_documents(attention)\n",
    "vectorstore=DocArrayInMemorySearch.from_documents(\n",
    "    documents,embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch at 0x167c3a44230>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from giskard.rag import KnowledgeBase \n",
    "\n",
    "df=pd.DataFrame([file.page_content for file in documents],columns=[\"text\"])\n",
    "knowledge_base=KnowledgeBase(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provided proper attribution is provided, Googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mechanism. We propose a new simple network arc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best models from the literature. We show that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>efficient inference and visualizations. Lukasz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Introduction\\nRecurrent neural networks, lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sequential nature precludes parallelization wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In this work we propose the Transformer, a mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in the distance between positions, linearly fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>textual entailment and learning task-independe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Here, the encoder maps an input sequence of sy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Provided proper attribution is provided, Googl...\n",
       "1  mechanism. We propose a new simple network arc...\n",
       "2  best models from the literature. We show that ...\n",
       "3  efficient inference and visualizations. Lukasz...\n",
       "4  1 Introduction\\nRecurrent neural networks, lon...\n",
       "5  sequential nature precludes parallelization wi...\n",
       "6  In this work we propose the Transformer, a mod...\n",
       "7  in the distance between positions, linearly fo...\n",
       "8  textual entailment and learning task-independe...\n",
       "9  Here, the encoder maps an input sequence of sy..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 22:06:14,597 pid:7268 MainThread giskard.rag  INFO     Finding topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Desktop\\Rag_Langchain\\virtualenv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 22:06:20,419 pid:7268 MainThread giskard.rag  INFO     Found 3 topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|██████████| 60/60 [02:41<00:00,  2.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import generate_testset \n",
    "\n",
    "testset=generate_testset(\n",
    "    knowledge_base,\n",
    "    num_questions=60,\n",
    "    agent_description=\"A Chat bot answering questions related to Pdf uploaded by Users\"\n",
    ")\n",
    "test_set_df=testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the complexity per layer for self-attention and recurrent layers?\n",
      "Reference answer: The complexity per layer for self-attention is O(n2 · d), and for recurrent layers, it is O(n · d2).\n",
      "Reference context:\n",
      "Document 20: Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 · d) O(1) O(1)\n",
      "Recurrent O(n · d2) O(n) O(n)\n",
      "Convolutional O(k · n · d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "******************\n",
      "\n",
      "Question 2: What are some state of the art approaches in sequence modeling and transduction problems?\n",
      "Reference answer: Recurrent neural networks, long short-term memory, and gated recurrent neural networks are established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\n",
      "Reference context:\n",
      "Document 4: 1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "******************\n",
      "\n",
      "Question 3: Who are the authors of the paper on layer normalization?\n",
      "Reference answer: Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
      "Reference context:\n",
      "Document 40: comments, corrections and inspiration.\n",
      "References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450, 2016.\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "10\n",
      "******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_df = testset.to_pandas()\n",
    "\n",
    "for index, row in enumerate(test_set_df.head(3).iterrows()):\n",
    "    print(f\"Question {index + 1}: {row[1]['question']}\")\n",
    "    print(f\"Reference answer: {row[1]['reference_answer']}\")\n",
    "    print(\"Reference context:\")\n",
    "    print(row[1]['reference_context'])\n",
    "    print(\"******************\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.save(\"test-set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question:{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query=\"Who are authors of Attention is all you need paper ?\"\n",
    "retrieved_results=vectorstore.similarity_search(query)\n",
    "context=retrieved_results[0].page_content\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors of the \"Attention Is All You Need\" paper are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|ChatOpenAI()|StrOutputParser()\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"context\":context,\n",
    "        \"input\":query\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(query,history=None):\n",
    "    query=query\n",
    "    retrieved_results=vectorstore.similarity_search(query)\n",
    "    context=retrieved_results[0].page_content\n",
    "    chain=prompt|ChatOpenAI()|StrOutputParser()\n",
    "    return chain.invoke({\"context\":context,\"input\":query })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 60/60 [02:14<00:00,  2.25s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 60/60 [01:09<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import evaluate\n",
    "\n",
    "report=evaluate(answer_fn,testset=testset,knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e5f00449-e318-41dd-803a-da4b73af8760\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e5f00449-e318-41dd-803a-da4b73af8760\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e5f00449-e318-41dd-803a-da4b73af8760\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        70.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=70.0 class=\" progress-green \">70.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        70.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=70.0 class=\" progress-green \">70.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        56.67%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=56.666666666666664 class=\" progress-orange \">56.67%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        71.76%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=71.75925925925925 class=\" progress-green \">71.76%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">60%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">Focus on improving the performance of the Generator, particularly for conversational and complex questions, as these have the lowest scores. Additionally, ensure balanced coverage across all topics, particularly &#34;Others,&#34; to enhance the overall knowledge base quality.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"0063af25-3453-40cb-a777-39f5cc1d9b9c\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1190\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1192\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1199\",\"attributes\":{\"factors\":[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Neural Networks\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1200\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1201\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1197\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1219\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1187\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1188\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1189\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"chzHcRzHS0CqqqqqqupGQExoL6G9hFJA\"},\"shape\":[3],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Neural Networks\"]],[\"colors\",[\"#fdc473\",\"#a50026\",\"#006837\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1220\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1221\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1216\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1217\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1218\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1228\",\"attributes\":{\"data_source\":{\"id\":\"p1187\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1229\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1230\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1225\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1226\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1227\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1238\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1232\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1233\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1234\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1239\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1240\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1235\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1236\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1237\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1198\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1212\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1207\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1208\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1209\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1210\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1202\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1203\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1204\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1205\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1206\",\"attributes\":{\"axis\":{\"id\":\"p1202\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1211\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1207\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1231\",\"attributes\":{\"location\":60.0,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1241\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1242\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1238\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"0063af25-3453-40cb-a777-39f5cc1d9b9c\",\"roots\":{\"p1190\":\"d3acfca2-e07d-4438-b630-5dae401b6a81\"},\"root_ids\":[\"p1190\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"d3acfca2-e07d-4438-b630-5dae401b6a81\" data-root-id=\"p1190\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"5b8ed35c-d336-40aa-937c-8dd1b31191c5\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1121\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1051\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1013\",\"attributes\":{\"start\":9.418643712997437,\"end\":17.75198745727539}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1014\",\"attributes\":{\"start\":-7.760392904281616,\"end\":0.8044438362121582}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1016\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1044\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"wiI2Qf41S0FVdkJBH4A4QXvoKkFLvjtBQe9EQbXATEGnmT1B+SE1QYRwREHgS0RBXc5fQcHQYkG5zGpBAYxrQdTUWUEjlE1BMWE+QT7pNUGJXFNBUMQ+Qbu0SEGu3kxBVtBbQRUHUUGsfVRBcplLQZYAVEEEJ1lBMxpRQaMOYkHkcVtBVnRiQcEAZEFDfkVB3OQ/QYoGRUHj/k5Bh4o8QXVJJEEk6B9BM9ckQQsNKUFDuC5BOAgyQX8uJkEcvRpB3ZowQXfKWEHfBlVB7HNRQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LbtfwOu0LsBrbzLATRUHwL71fsBNv2jA1ZJUwC5Rl8Ca9IbAVkKewMqPhcB9G5zA9hqYwG8yiMCZT5fAlg14wCmphMBD94fAoRWkwD8ztsD/osTAUTG9wCcys8BIp8rA1m6ywJgAr8CaMJzA1H4fv+GMfL9Srvy/OobEv47zrL8SD3u/Rg4KwISKQMD2xgHARQ2Cv9MbxL8o1hbAzocnwLDKCMDY8mPArbxJwKMsHcDt4j7A4uyhv8Q2xL/0ZRfAf+X7v4R2ZMC9PFHAqU9jwA==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"length n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npa...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"and interpretable tree annotation. In Proceedings of the 21st International Conference on\\\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\\\\u2013440. ACL, July\\\\n2006.\\\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859, 2016.\\\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\\\n[32] Noam Shazeer, Azalia Mirhoseini,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\\\nAdvances in Neural Information Processing Systems, 2015.\\\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144, 2016.\\\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. D...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#aec7e8\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#1f77b4\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1045\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1046\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1041\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1042\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1012\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1028\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1029\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1030\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1031\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1037\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1022\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1023\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1024\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1025\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1017\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1018\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1019\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1020\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1021\",\"attributes\":{\"axis\":{\"id\":\"p1017\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1026\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1022\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1047\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1048\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Attention Mechanisms in Neural Networks\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":12}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1049\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":4}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1050\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Neural Networks\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":0}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1120\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1056\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1065\",\"attributes\":{\"start\":9.418643712997437,\"end\":17.75198745727539}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1066\",\"attributes\":{\"start\":-7.760392904281616,\"end\":0.8044438362121582}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1067\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1068\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1063\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1095\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1052\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1053\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1054\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"iVxTQXvoKkF1SSRBHL0aQQQnWUELDSlBH4A4QTMaUUFDuC5Bh4o8Qd8GVUEBjGtBMxpRQYRwREF76CpBucxqQdzkP0FQxD5BXc5fQfkhNUEcvRpBBCdZQcHQYkEjlE1B3OQ/QVV2QkHkcVtBOAgyQTFhPkEk6B9BM9ckQexzUUFW0FtB3wZVQUN+RUGjDmJB4/5OQVV2QkG1wExBwQBkQT7pNUEcvRpBOAgyQUu+O0GnmT1Bh4o8QV3OX0FDuC5BwiI2Qax9VEGHijxBQe9EQfkhNUFdzl9BVnRiQQsNKUE+6TVBH4A4Qf41S0HB0GJB\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"/6LEwL71fsCwygjA9GUXwFKu/L+jLB3ATRUHwDqGxL/t4j7AzocnwL08UcCWDXjAOobEv8qPhcC+9X7AmU+XwEUNgr9RMb3A9hqYwFZCnsD0ZRfAUq78v28yiMBD94fARQ2Cv2tvMsASD3u/4uyhv6EVpMDY8mPArbxJwKlPY8DWbrLAvTxRwPbGAcCO86y/KNYWwGtvMsAuUZfAhIpAwD8ztsD0ZRfA4uyhv02/aMCa9IbAzocnwPYamMDt4j7ALbtfwJownMDOhyfA1ZJUwFZCnsD2GpjARg4KwKMsHcA/M7bATRUHwOu0LsBvMojA\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Others\",\"Transformer Neural Networks\",\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\"]],[\"correctness\",[false,true,true,true,false,true,true,false,true,true,true,true,false,true,false,false,false,true,true,false,false,false,true,true,true,true,true,true,true,false,false,true,true,false,true,true,true,true,true,false,true,true,true,true,true,false,true,true,true,true,false,true,false,false,false,false,false,false,false,false]],[\"questions\",[\"What is the complexity per layer for self-attention and recurrent layers?\",\"What are some state of the art approaches in sequence modeling and transduction problems?\",\"Who are the authors of the paper on layer normalization?\",\"Who are the authors of the paper titled &#x27;Sequence to sequence learning with neural networks&#x27;?\",\"What BLEU scores does the Transformer (big) model achieve on the English-to-German and English-to-French tests?\",\"Who are the authors of the paper titled &#x27;Exploring the limits of language modeling&#x27;?\",\"What is the name of the codebase that Lukasz and Aidan worked on?\",\"What is the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task?\",\"Who are the authors of the paper titled &#x27;Structured attention networks&#x27;?\",\"What tasks did the authors achieve a new state of the art in?\",\"In the context provided, what specific linguistic task do the attention heads in layer 5 of 6 seem to be performing, particularly when focusing on the word &#x27;its&#x27;?\",\"In the context of attention mechanisms, particularly when dealing with large values of dk, why is it necessary to scale the dot product by 1/\\\\u221adk, and what impact does this scaling have on the gradients of the softmax function?\",\"Under the condition that it surpassed previous models by over 2.0 BLEU, what specific BLEU score did the large-scale transformer model achieve for the English-to-German translation task in the WMT 2014 competition?\",\"Considering the architecture of the Transformer model, specifically focusing on the encoder component, how many identical layers are stacked within it, and what are the functions of the sub-layers within each layer?\",\"Considering the latest advancements, what are some cutting-edge methodologies utilized in sequence modeling and transduction tasks, particularly focusing on language modeling and machine translation, while also addressing the challenges of parallelization in training?\",\"Considering the context provided, which two attention mechanisms are most frequently utilized, and under what conditions might their performance vary?\",\"In the context of the semi-supervised setting described, what was the total number of tokens included in the vocabulary, considering only the high-confidence and BerkleyParser corpora?\",\"Considering the model&#x27;s architecture, which specific mathematical functions are employed for the positional encodings, and under what conditions are these functions applied to the input embeddings?\",\"In the context of attention mechanisms, under the constraint that both the query and key-value pairs are vector representations, what specific transformation does an attention function apply to derive the output vector?\",\"In the context of the encoder&#x27;s functionality, what specific transformation occurs when it converts an input sequence of symbol representations into another form, and what constraints or conditions does this process involve?\",\"Considering the significance of encoder-decoder attention mechanisms in sequence-to-sequence models, can you identify the authors of the paper &#x27;End-to-end memory networks&#x27;?\",\"Under the constraint of using the Transformer (big) model instead of recurrent neural networks for translation tasks, what BLEU scores are attained for English-to-German and English-to-French translations?\",\"Under the constraint of optimizing training costs and translation quality, what role does the softmax function play in the Scaled Dot-Product Attention mechanism within the Transformer model?\",\"In the context of the attention mechanism described as mapping a query and a set of key-value pairs to an output, what specific role do the encoder-decoder attention layers play within the Transformer model?\",\"In the context of managing large dot product magnitudes in attention mechanisms, what was the size of the vocabulary utilized in the semi-supervised setting?\",\"Under the condition of using either large or limited training data, to which task did the Transformer model demonstrate its generalization capabilities effectively?\",\"In the context of evaluating the impact of different components of the Transformer on English-to-German translation performance, what is the rationale behind varying the base model used in the study?\",\"Considering the challenges in scaling dot product attention for larger values of dk, what is the title of the work authored by Romain Paulus, Caiming Xiong, and Richard Socher that might address such complex issues in abstractive summarization?\",\"In the context of using dot-product attention with a scaling factor, how does masking in the decoder&#x27;s self-attention layers contribute to maintaining the auto-regressive property?\",\"Considering the research presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017), who are the authors of the paper titled &#x27;Empirical evaluation of gated recurrent neural networks on sequence modeling&#x27;?\",\"Hi, I&#x27;m preparing a presentation on neural networks for my computer science class. Could you tell me the title of the 2016 paper authored by Kaiming He and others?\",\"As I&#x27;m diving into this document on legal theory, can you tell me what the focus of the attention heads in the encoder self-attention at layer 5 is?\",\"Hi there, I&#x27;m researching computational efficiency in NLP models. Could you tell me which type of layer, self-attention or recurrent, is faster in terms of computational complexity?\",\"Hi, I&#x27;m preparing some talking points for a university presentation on justice reform. Could you explain what the focus of the two attention heads in layer 5 of 6 is?\",\"As I&#x27;m reviewing my notes on neural network architectures for my computer science project, could you tell me what task was used to evaluate if the Transformer can generalize to other tasks?\",\"Hi, I&#x27;m conducting a comparative study on state-of-the-art machine translation systems. Could you tell me what BLEU score the big model achieved on the WMT 2014 English-to-French translation task?\",\"Hi, in the study I&#x27;m reviewing, which model was found to outperform the Berkeley-Parser when trained solely on the WSJ training set?\",\"Hi, as a research student exploring Transformer models, I&#x27;m curious to know what specific task the Transformer model was able to successfully apply to, showing its generalization capability?\",\"Hi, I&#x27;m a graduate student researching machine learning models for reading comprehension tasks. Could you tell me what self-attention is sometimes called?\",\"As I&#x27;m refining my machine translation model, could you tell me what the BLEU score was when positional embedding was used instead of sinusoids?\",\"What is the dimensionality of the input and output in the FFN function described and how are the input tokens converted to vectors in the sequence transduction model?\",\"What is the title of the paper authored by Sainbayar Sukhbaatar and colleagues in 2015 and who are the authors of the paper &#x27;Sequence to sequence learning with neural networks&#x27;?\",\"What is the focus of the paper by Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini published in 1993, and which paper discusses a deep reinforced model for abstractive summarization?\",\"What is a fundamental constraint in sequence modeling despite recent improvements, and what is the main feature of the Transformer model architecture proposed in the document?\",\"What is the main mechanism that end-to-end memory networks use and what distinguishes the Transformer from other transduction models?\",\"What translation tasks did the model achieve a new state of the art on, and what are the future plans for attention-based models according to the text?\",\"What is an attention function in the context provided and how is the output computed?\",\"Who are the authors of the paper titled &#x27;Structured attention networks&#x27; and which paper discusses &#x27;Factorization tricks for LSTM networks&#x27;?\",\"What permission does Google grant regarding the reproduction of tables and figures in the paper &#x27;Attention Is All You Need&#x27; and what is the main architectural innovation proposed in the paper?\",\"What is the complexity of separable convolutions compared to self-attention layers, and what is a side benefit of using self-attention in models?\",\"What were those?\",\"What is the main benefit of this compared to those?\",\"What does it map?\",\"What is the role of that?\",\"What is it?\",\"Who are they?\",\"What is it?\",\"What was their involvement?\",\"What is it?\",\"What is the purpose of doing this?\"]],[\"agent_answer\",[\"The complexity per layer for self-attention is O(n^2), as it requires O(n^2) operations to compute the attention scores between all pairs of elements in the sequence. On the other hand, the complexity per layer for a recurrent layer is O(n), as it requires O(n) sequential operations to process each element in the sequence one by one. So, in terms of computational complexity, self-attention layers are faster than recurrent layers.\",\"Some state of the art approaches in sequence modeling and transduction problems include recurrent neural networks, long short-term memory networks, and gated recurrent neural networks. These approaches have been firmly established as effective methods for tasks such as language modeling and machine translation. Researchers have also made efforts to push the boundaries of recurrent language models and encoder-decoder architectures to further improve performance in these areas.\",\"The authors of the paper on layer normalization are Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\",\"The authors of the paper titled &#x27;Sequence to sequence learning with neural networks&#x27; are Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.\",\"Based on the provided context, the Transformer (big) model achieves a BLEU score of 41.0 on the English-to-French translation task. However, there is no mention of the BLEU scores achieved by the Transformer (big) model on the English-to-German translation task in the given information.\",\"The authors of the paper titled &#x27;Exploring the limits of language modeling&#x27; are Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\",\"The codebase that Lukasz and Aidan worked on is called tensor2tensor.\",\"Based on the provided context, the BLEU score achieved by the big transformer model on the WMT 2014 English-to-French translation task is 41.0. The context does not mention the BLEU score achieved by the big transformer model on the English-to-German translation task.\",\"The authors of the paper titled &#x27;Structured attention networks&#x27; are Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.\",\"The authors achieved a new state of the art in machine translation tasks, specifically in the WMT 2014 English-to-German translation task and the WMT 2014 English-to-French translation task. They were able to achieve superior quality results and set new single-model state-of-the-art BLEU scores in these tasks using their Transformer network architecture.\",\"The attention heads in layer 5 of 6 seem to be performing anaphora resolution, specifically when focusing on the word &#x27;its&#x27;. Anaphora resolution is the task of identifying what a pronoun or noun phrase refers to in the text. The sharp attentions for the word &#x27;its&#x27; suggest that the attention heads are trying to establish a clear connection between &#x27;its&#x27; and its antecedent in the text.\",\"In the context of attention mechanisms, it is necessary to scale the dot product by 1/\\\\u221adk for large values of dk to prevent the dot products from growing too large in magnitude. When the dot products become too large, it can push the softmax function into regions where it has extremely small gradients, making it difficult for the model to learn effectively.\\\\n\\\\nBy scaling the dot products, we can control the magnitude of the values being passed into the softmax function, ensuring that the gradients do not become too small. This scaling helps stabilize the training process and allows for more efficient learning by preventing the softmax function from becoming saturated in regions with small gradients.\",\"Based on the provided context, the large-scale transformer model achieved a BLEU score of 41.0 for the English-to-French translation task in the WMT 2014 competition. However, the specific BLEU score for the English-to-German translation task is not mentioned in the context.\",\"Based on the provided context, the encoder of the Transformer model is composed of a stack of N = 6 identical layers. Each layer consists of two sub-layers. The first sub-layer is a multi-head self-attention mechanism, and the second sub-layer is a simple, position-wise fully connected feed-forward network. \\\\n\\\\nTherefore, there are 6 identical layers stacked within the encoder, and the functions of the sub-layers within each layer are multi-head self-attention mechanism and position-wise fully connected feed-forward network.\",\"Some cutting-edge methodologies utilized in sequence modeling and transduction tasks, particularly in language modeling and machine translation, include recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and gated recurrent neural networks. These approaches have been established as state-of-the-art in handling sequence data.\\\\n\\\\nIn order to address the challenges of parallelization in training, recent advancements have focused on improving the efficiency of recurrent models by factorizing computation along the symbol positions of input and output sequences. By aligning positions to steps in computation time, these models generate a sequence of hidden states based on the previous hidden state and input for each position.\\\\n\\\\nAdditionally, efforts have been made to optimize memory constraints and enable batching across examples, especially at longer sequence lengths. This is crucial for parallelization within training examples, as it allows for more efficient processing of large datasets.\\\\n\\\\nOverall, the latest methodologies in sequence modeling and transduction tasks aim to improve the performance and scalability of models, particularly in the context of language modeling and machine translation, by leveraging advancements in recurrent neural networks and addressing challenges related to parallelization in training.\",\"The two attention mechanisms most frequently utilized are factorization tricks and conditional computation. Their performance may vary depending on the length of the sequence being processed. Factorization tricks can improve computational efficiency but may not be as effective for longer sequence lengths due to memory constraints. On the other hand, conditional computation can improve both computational efficiency and model performance, making it a more versatile option for handling sequences of varying lengths.\",\"To determine the total number of tokens included in the vocabulary for the semi-supervised setting, we need to add the vocabulary sizes for the high-confidence and BerkleyParser corpora. \\\\n\\\\nThe vocabulary size for the WSJ only setting was 16K tokens, and for the semi-supervised setting, it was 32K tokens. Therefore, the total number of tokens included in the vocabulary for the semi-supervised setting (considering only the high-confidence and BerkleyParser corpora) would be 16K + 32K = 48K tokens.\",\"The specific mathematical functions employed for the positional encodings are sine and cosine functions of different frequencies. These functions are applied to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, allowing them to be summed together. The positional encodings are calculated using the following formulas:\\\\n- PE(pos,2i) = sin(pos/10000^(2i/dmodel))\\\\n- PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))\\\\nHere, pos represents the position and i represents the dimension. Each dimension of the positional encoding corresponds to a sinusoid, with wavelengths forming a geometric progression from 2\\\\u03c0 to 10000\\\\u00b72\\\\u03c0. These functions are chosen to allow the model to easily learn to attend by relative positions, as PE(pos+k) can be represented as a linear function of PE(pos).\",\"An attention function applies a specific transformation to the query and key-value pairs in order to derive the output vector. This transformation involves computing a weighted sum of the values, where the weights are calculated based on the similarity between the query and the keys. The output vector is then generated by combining the values according to these weights, resulting in a context vector that captures the relevant information from the key-value pairs for the given query.\",\"When the encoder maps an input sequence of symbol representations to a sequence of continuous representations, a specific transformation known as encoding takes place. This transformation involves converting discrete symbol representations into continuous representations in a way that captures the underlying relationships and patterns within the input sequence. \\\\n\\\\nThe process of encoding in the encoder involves constraints or conditions such as maintaining the sequential order of the input symbols, preserving the semantic meaning of the symbols in the continuous representation, and ensuring that the continuous representations capture relevant information for the decoder to generate the output sequence accurately. Additionally, the encoder must also balance the trade-off between capturing detailed information from the input sequence and avoiding overfitting or loss of generalization ability.\",\"Based on the provided context, the paper &#x27;End-to-end memory networks&#x27; is not mentioned. The authors mentioned in the context are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \\\\u0141ukasz Kaiser, and Illia Polosukhin. Therefore, the authors of the paper &#x27;End-to-end memory networks&#x27; are not identifiable within the given information.\",\"Based on the context provided, the BLEU score attained for English-to-French translation using the Transformer (big) model is 41.0. However, the context does not specify the BLEU score attained for English-to-German translation using the same model. It is important to note that the BLEU score for English-to-German translation is not explicitly mentioned in the given information.\",\"Step 1: Understand the context\\\\nIn the provided context, it is explained that the Scaled Dot-Product Attention mechanism in the Transformer model involves computing dot products of queries with keys, dividing by the square root of the dimension of keys, and applying a softmax function to obtain weights on the values. This mechanism is used to compute the attention scores in the Transformer model.\\\\n\\\\nStep 2: Determine the role of the softmax function\\\\nThe softmax function is applied to the dot products of queries with keys in order to obtain weights on the values. The softmax function ensures that the weights assigned to the values are normalized and represent a probability distribution. By using the softmax function, the attention mechanism can focus on relevant parts of the input data by assigning higher weights to values that are more important for the task at hand. This helps in optimizing the training process by ensuring that the model pays attention to the most relevant information during the translation process.\\\\n\\\\nStep 3: Conclusion\\\\nIn conclusion, the softmax function plays a crucial role in the Scaled Dot-Product Attention mechanism within the Transformer model by normalizing the attention scores and allowing the model to focus on important information during the translation process. This helps in optimizing training costs and improving translation quality by ensuring that the model efficiently attends to relevant parts of the input data.\",\"The encoder-decoder attention layers in the Transformer model allow for every position in the decoder to attend over all positions in the input sequence. This means that the queries come from the previous decoder layer, while the memory keys and values come from the output of the encoder. This mechanism enables the decoder to focus on relevant parts of the input sequence during the translation process in a sequence-to-sequence model. By allowing the decoder to access information from the encoder, the encoder-decoder attention layers facilitate the generation of accurate translations or outputs by ensuring that the decoder can attend to all relevant information from the input sequence.\",\"In the semi-supervised setting, a vocabulary of 32K tokens was utilized.\",\"The Transformer model demonstrated its generalization capabilities effectively to English constituency parsing, both with large and limited training data.\",\"The rationale behind varying the base model in the study is to evaluate the importance of different components of the Transformer on English-to-German translation performance. By making changes to the base model and measuring the resulting impact on translation quality, researchers can determine which components are most crucial for achieving optimal performance. This process allows for a better understanding of how each aspect of the Transformer architecture contributes to the overall translation quality and can help in further optimizing the model for better results.\",\"The title of the work authored by Romain Paulus, Caiming Xiong, and Richard Socher that might address the challenges in scaling dot product attention for larger values of dk in abstractive summarization is \\\\\"A Deep Reinforced Model for Abstractive Summarization.\\\\\"\",\"Masking in the decoder&#x27;s self-attention layers contributes to maintaining the auto-regressive property by preventing leftward information flow. By masking out (setting to -\\\\u221e) all values in the input of the softmax which correspond to illegal connections, the decoder is restricted from attending to positions that come after the current position. This ensures that each position in the decoder can only attend to positions up to and including that position, preserving the auto-regressive nature of the model where predictions are made based on previously generated tokens. This restriction helps to ensure that the decoder generates outputs in a sequential and ordered manner, following the natural flow of the input data.\",\"Based on the provided context, the authors of the paper titled &#x27;Empirical evaluation of gated recurrent neural networks on sequence modeling&#x27; presented at the 31st Conference on Neural Information Processing Systems (NIPS 2017) are not mentioned. The context only includes references to various research papers and authors in the field of machine learning and neural networks, but does not specifically mention the authors of the mentioned paper.\",\"The 2016 paper authored by Kaiming He and others is titled \\\\\"Google&#x27;s neural machine translation system: Bridging the gap between human and machine translation.\\\\\"\",\"The focus of the attention heads in the encoder self-attention at layer 5 seems to be related to the structure of the sentence. The attention heads exhibit behavior that is indicative of learning to perform different tasks within the context of legal theory.\",\"Based on the provided context, self-attention layers are faster in terms of computational complexity compared to recurrent layers. This is because self-attention layers require fewer operations to be executed compared to recurrent layers. Self-attention layers have a constant time complexity, while recurrent layers have a sequential time complexity. Therefore, in terms of computational efficiency, self-attention layers are faster than recurrent layers.\",\"To answer this question, we need to analyze the provided context. The text mentions that in layer 5 of 6, two attention heads are apparently involved in anaphora resolution. Anaphora resolution refers to the process of determining what a pronoun or noun phrase refers to in a text. \\\\n\\\\nSpecifically, the text mentions that the attentions from just the word &#x27;its&#x27; are very sharp for attention heads 5 and 6. This indicates that these attention heads in layer 5 of 6 are particularly focused on resolving references to the word &#x27;its&#x27; within the text. \\\\n\\\\nTherefore, the focus of the two attention heads in layer 5 of 6 appears to be on resolving anaphora related to the word &#x27;its&#x27; in the context of the discussion on the imperfection of the law and the importance of just application. These attention heads likely play a role in understanding and processing the relationships between different elements in the text related to justice reform.\",\"The task used to evaluate if the Transformer can generalize to other tasks was English constituency parsing.\",\"The big model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task.\",\"The Transformer model was found to outperform the Berkeley-Parser when trained solely on the WSJ training set.\",\"The Transformer model was successfully applied to English constituency parsing, both with large and limited training data, showcasing its generalization capability for this task.\",\"Self-attention is sometimes called intra-attention.\",\"Based on the provided context, the BLEU score achieved by the Transformer model was 28.4 on the WMT 2014 English-to-German translation task and 41.8 on the WMT 2014 English-to-French translation task. The model was able to outperform existing models and achieve state-of-the-art results with significantly less training time and cost. The Transformer model was also shown to generalize well to other tasks such as English constituency parsing. However, there is no specific mention of the BLEU score when positional embedding was used instead of sinusoids in the context provided. To determine the impact of using positional embedding instead of sinusoids on the BLEU score, further experimentation and analysis would be necessary.\",\"The dimensionality of the input and output in the FFN function described is dmodel = 512. In the sequence transduction model, the input tokens are converted to vectors of dimension dmodel using learned embeddings. This involves multiplying the weights in the embedding layers by the square root of dmodel.\",\"The title of the paper authored by Sainbayar Sukhbaatar and colleagues in 2015 is &#x27;End-to-end memory networks&#x27;. The authors of the paper &#x27;Sequence to sequence learning with neural networks&#x27; are Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.\",\"The focus of the paper by Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini published in 1993 is on building a large annotated corpus of English, specifically the Penn Treebank. \\\\n\\\\nThe paper by Romain Paulus, Caiming Xiong, and Richard Socher published in 2017 discusses a deep reinforced model for abstractive summarization.\",\"The fundamental constraint in sequence modeling despite recent improvements is the sequential nature that precludes parallelization within training examples, especially at longer sequence lengths due to memory constraints limiting batching across examples. \\\\n\\\\nThe main feature of the Transformer model architecture proposed in the document is that it eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for modeling of dependencies without regard to their distance in the input or output sequences.\",\"The main mechanism that end-to-end memory networks use is a recurrent attention mechanism. On the other hand, what distinguishes the Transformer from other transduction models is that it relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. This allows the Transformer to have advantages over models that do use RNNs or convolution, as discussed in the context provided.\",\"The model achieved a new state of the art on the WMT 2014 English-to-French translation task, establishing a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs. \\\\n\\\\nAccording to the text, the future plans for attention-based models involve generalizing well to other tasks by successfully applying them to English constituency parsing both with large and limited training data.\",\"In the context provided, an attention function is a function that takes a query and a set of key-value pairs as input and maps them to an output. The query, keys, values, and output are all vectors. \\\\n\\\\nThe output of the attention function is computed as a weighted sum. This means that the function assigns a weight to each key based on how well it matches the query. These weights are then used to compute a weighted sum of the corresponding values, which results in the final output of the attention function. \\\\n\\\\nTherefore, the attention function essentially focuses on certain key-value pairs based on the query and computes an output by combining the values with different weights.\",\"The authors of the paper titled &#x27;Structured attention networks&#x27; are Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. The paper that discusses &#x27;Factorization tricks for LSTM networks&#x27; is authored by Oleksii Kuchaiev and Boris Ginsburg.\",\"Google grants permission to reproduce the tables and figures in the paper &#x27;Attention Is All You Need&#x27; solely for use in journalistic or scholarly works. \\\\n\\\\nThe main architectural innovation proposed in the paper is the Transformer network architecture, which is based solely on attention mechanisms and dispenses with recurrence and convolutions.\",\"The complexity of separable convolutions is O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d^2), which can be reduced considerably compared to regular convolutions. In the case where k = n, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer. \\\\n\\\\nA side benefit of using self-attention in models is that it could yield more interpretable models. By inspecting attention distributions from the models, it is possible to see that individual attention heads learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of the sentences, making the model more interpretable.\",\"Based on the context provided, the two examples mentioned were related to the structure of the sentence. The attention heads from two different heads from the encoder self-attention at layer 5 seemed to have learned to perform different tasks.\",\"The main benefit of the Transformer network architecture compared to traditional models is that it is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. This results in superior quality translations, increased parallelizability, and significantly reduced training time. Additionally, the Transformer model achieves better results on machine translation tasks with less computational resources compared to existing models.\",\"The attention function maps a query and a set of key-value pairs to an output.\",\"The provided context does not explicitly mention a specific role of \\\\\"that.\\\\\" However, based on the context provided, it seems that the focus is on the application of the law being just, despite the fact that the law itself may never be perfect. The statement suggests that while the law may have its flaws, ensuring that its application is fair and just is crucial.\",\"Based on the context provided, \\\\\"it\\\\\" refers to the application of the law being just. The opinion expressed is that while the law may never be perfect, its application should be fair and just.\",\"Based on the context provided, \\\\\"they\\\\\" refers to the attention heads from two different heads from the encoder self-attention at layer 5 of 6. These attention heads exhibit behavior related to the structure of the sentence and have clearly learned to perform different tasks.\",\"Based on the context provided, \\\\\"it\\\\\" refers to the application of the Law. The statement discusses how the Law may never be perfect, but its application should be just, indicating that there is a need for fairness and justice in the implementation of laws.\",\"Their involvement was apparently in anaphora resolution, specifically focusing on the word &#x27;its&#x27;. The attentions from attention heads 5 and 6 were very sharp for this word, indicating that they were responsible for resolving anaphora in the context provided.\",\"Based on the context provided, \\\\\"it\\\\\" refers to the application of the Law being just. The statement emphasizes that while the Law may never be perfect, its application should be fair and just. This is what the speaker believes is missing in their opinion.\",\"The purpose of using attention visualizations and analyzing the attention mechanism in the encoder self-attention is to understand how the model processes and assigns importance to different parts of the input sequence, particularly in capturing long-distance dependencies. By examining the attention heads and their focus on specific words or phrases, researchers can gain insights into how the model makes predictions and handles complex linguistic structures. This analysis can help improve the performance and interpretability of the model.\"]],[\"reference_answer\",[\"The complexity per layer for self-attention is O(n2 \\\\u00b7 d), and for recurrent layers, it is O(n \\\\u00b7 d2).\",\"Recurrent neural networks, long short-term memory, and gated recurrent neural networks are established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\",\"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\",\"Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.\",\"The Transformer (big) model achieves a BLEU score of 28.4 on the English-to-German test and 41.8 on the English-to-French test.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\",\"tensor2tensor\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.\",\"On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, they achieved a new state of the art.\",\"The attention heads, particularly in layer 5 of 6, are apparently involved in anaphora resolution. The attentions are very sharp for the word &#x27;its&#x27;.\",\"The dot products are scaled by 1/\\\\u221adk to counteract the effect of large magnitudes pushing the softmax function into regions where it has extremely small gradients.\",\"The big transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"The encoder is composed of a stack of N = 6 identical layers.\",\"Recurrent neural networks, long short-term memory, and gated recurrent neural networks are established as state-of-the-art approaches in sequence modeling and transduction problems such as language modeling and machine translation.\",\"The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"Sine and cosine functions of different frequencies are used for positional encodings.\",\"An attention function maps a query and a set of key-value pairs to an output.\",\"The encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn).\",\"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The softmax function is applied to obtain the weights on the values.\",\"In &#x27;encoder-decoder attention&#x27; layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"The Transformer model successfully applied to English constituency parsing both with large and limited training data.\",\"To evaluate the importance of different components of the Transformer by measuring the change in performance on English-to-German translation.\",\"A deep reinforced model for abstractive summarization.\",\"The purpose of masking in the decoder&#x27;s self-attention layers is to prevent leftward information flow in the decoder to preserve the auto-regressive property. This is implemented by masking out (setting to \\\\u2212\\\\u221e) all values in the input of the softmax which correspond to illegal connections.\",\"Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio.\",\"Deep residual learning for image recognition.\",\"Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.\",\"Self-attention layers are faster than recurrent layers in terms of computational complexity.\",\"The two attention heads in layer 5 of 6 are apparently involved in anaphora resolution.\",\"English constituency parsing was used to evaluate if the Transformer can generalize to other tasks.\",\"The big model achieves a BLEU score of 41.0.\",\"The Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences.\",\"The Transformer model successfully applied to English constituency parsing both with large and limited training data.\",\"Self-attention is sometimes called intra-attention.\",\"4.92\",\"The dimensionality of input and output is dmodel = 512, and learned embeddings are used to convert the input tokens to vectors of dimension dmodel.\",\"The title of the paper authored by Sainbayar Sukhbaatar and colleagues in 2015 is &#x27;End-to-end memory networks&#x27;, and the authors of the paper &#x27;Sequence to sequence learning with neural networks&#x27; are Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.\",\"The paper by Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini published in 1993 focuses on building a large annotated corpus of English, known as the Penn Treebank, and the paper by Romain Paulus, Caiming Xiong, and Richard Socher, titled &#x27;A deep reinforced model for abstractive summarization,&#x27; discusses a deep reinforced model for abstractive summarization.\",\"The fundamental constraint of sequential computation remains, which precludes parallelization within training examples at longer sequence lengths due to memory constraints, and the main feature of the Transformer model architecture is that it eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output.\",\"End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence, while the Transformer is distinguished as the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, and the future plans include applying attention-based models to other tasks, extending the Transformer to problems involving input and output modalities other than text, investigating local, restricted attention mechanisms for large inputs and outputs such as images, audio, and video, and making generation less sequential.\",\"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors, and the output is computed as a weighted sum.\",\"The authors of the paper titled &#x27;Structured attention networks&#x27; are Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush, and the paper discussing &#x27;Factorization tricks for LSTM networks&#x27; is authored by Oleksii Kuchaiev and Boris Ginsburg, available as an arXiv preprint with the identifier arXiv:1703.10722, published in 2017.\",\"Google grants permission to reproduce the tables and figures in the paper solely for use in journalistic or scholarly works, provided proper attribution is given, and the main architectural innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, and a side benefit of using self-attention is that it could yield more interpretable models.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"The Transformer model architecture eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization.\",\"The encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn).\",\"An attention function maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum.\",\"The BLEU score for the &#x27;big&#x27; variation is 26.4.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\",\"The dimensionality of input and output is dmodel = 512.\",\"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing the earlier codebase, greatly improving results and massively accelerating research.\",\"The Transformer model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.\",\"The purpose of dividing the dot products by the square root of dk is to obtain the weights on the values after applying the softmax function.\"]],[\"id\",[20,4,40,47,29,43,3,30,44,39,50,15,30,10,4,14,36,21,12,9,47,29,13,17,36,2,32,45,18,41,42,51,24,50,35,31,38,2,7,34,19,47,45,5,8,39,12,44,0,26,39,6,9,12,33,43,19,3,1,13]],[\"content\",[\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\"]],[\"color\",[\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1096\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1097\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1092\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1093\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1094\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1110\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1101\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1102\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1103\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"wiI2Qf41S0FVdkJBH4A4QXvoKkFLvjtBQe9EQbXATEGnmT1B+SE1QYRwREHgS0RBXc5fQcHQYkG5zGpBAYxrQdTUWUEjlE1BMWE+QT7pNUGJXFNBUMQ+Qbu0SEGu3kxBVtBbQRUHUUGsfVRBcplLQZYAVEEEJ1lBMxpRQaMOYkHkcVtBVnRiQcEAZEFDfkVB3OQ/QYoGRUHj/k5Bh4o8QXVJJEEk6B9BM9ckQQsNKUFDuC5BOAgyQX8uJkEcvRpB3ZowQXfKWEHfBlVB7HNRQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LbtfwOu0LsBrbzLATRUHwL71fsBNv2jA1ZJUwC5Rl8Ca9IbAVkKewMqPhcB9G5zA9hqYwG8yiMCZT5fAlg14wCmphMBD94fAoRWkwD8ztsD/osTAUTG9wCcys8BIp8rA1m6ywJgAr8CaMJzA1H4fv+GMfL9Srvy/OobEv47zrL8SD3u/Rg4KwISKQMD2xgHARQ2Cv9MbxL8o1hbAzocnwLDKCMDY8mPArbxJwKMsHcDt4j7A4uyhv8Q2xL/0ZRfAf+X7v4R2ZMC9PFHAqU9jwA==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1111\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1112\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1107\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1108\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1109\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1064\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1079\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1080\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1081\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1082\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1087\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1088\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1055\",\"attributes\":{\"renderers\":[{\"id\":\"p1095\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1074\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1075\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1076\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1077\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1069\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1070\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1071\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1072\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1073\",\"attributes\":{\"axis\":{\"id\":\"p1069\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1078\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1074\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1098\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1099\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1095\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1100\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1095\"}],\"index\":1}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1116\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1113\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1114\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1115\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"2yZAQUdcXEE=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"8hdpwBZBgcA=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Neural Networks\",\"Attention Mechanisms in Neural Networks\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"5b8ed35c-d336-40aa-937c-8dd1b31191c5\",\"roots\":{\"p1121\":\"f0aa2bf0-4ccd-4f00-928c-b7c39bfa5ea5\"},\"root_ids\":[\"p1121\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"f0aa2bf0-4ccd-4f00-928c-b7c39bfa5ea5\" data-root-id=\"p1121\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x167c3d684d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.5\n",
       "conversational               0.1\n",
       "distracting element          0.7\n",
       "double                       0.9\n",
       "simple                       0.7\n",
       "situational                  0.7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
