{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity\n"
     ]
    }
   ],
   "source": [
    "print(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_api_key=os.getenv(\"perplexity_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model=\"llama-3.1-sonar-huge-128k-online\"\n",
    "prompt=\"What is Rag system?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an artificial intelligence assistant and you need to \"\n",
    "            \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            # user will ask a question\n",
    "            prompt\n",
    "        ),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=OpenAI(api_key=perplexity_api_key,base_url=\"https://api.perplexity.ai\")\n",
    "response=client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Retrieval-Augmented Generation (RAG) system is an AI framework that combines the capabilities of large language models (LLMs) with external data sources to provide more accurate, up-to-date, and context-specific responses. Here’s a detailed overview:\\n\\n1. **Two Phases: Ingestion and Retrieval**\\n   - **Ingestion Phase:** This involves creating an index of external data sources, such as databases, documents, or web pages. The data is processed into dense vector representations (embeddings) that allow for efficient retrieval[1][2].\\n   - **Retrieval Phase:** When a user asks a question, the system retrieves relevant information from the indexed data sources. This information is then integrated into the LLM's input, enabling it to generate more accurate and contextually relevant responses[1][2].\\n\\n2. **Key Benefits:**\\n   - **Grounded Generation:** RAG ensures that LLMs are grounded on the most current and reliable facts, reducing the risk of generating false or outdated information[2][3].\\n   - **Transparency:** Users can access the sources used by the LLM, making it easier to verify the accuracy of the responses[2][3].\\n   - **Domain-Specific Responses:** RAG allows LLMs to provide responses tailored to an organization's proprietary or domain-specific data[3][5].\\n\\n3. **Applications:**\\n   - **Customer Service Chatbots:** RAG can be used to create chatbots that provide accurate and personalized responses based on corporate policies, customer account data, and other sources[1][4].\\n   - **Enterprise Knowledge Management:** RAG systems can retrieve relevant information from across an organization, synthesize it, and provide actionable insights to employees[1][4].\\n   - **Drafting Assistants:** RAG can help employees draft reports and documents by retrieving relevant company-specific data and prepopulating sections of the document[1][5].\\n\\n4. **Implementation Challenges:**\\n   - **Integration Complexity:** Integrating RAG with existing systems can be complex and requires careful planning[4].\\n   - **Scalability:** RAG systems need to be scalable to handle large volumes of data and user queries[4].\\n   - **Data Quality:** Ensuring the quality and accuracy of the external data sources is crucial for the effectiveness of RAG systems[4].\\n\\nOverall, RAG systems enhance the capabilities of LLMs by providing them with access to external knowledge bases, leading to more accurate, reliable, and context-specific responses.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 22:14:08,777 pid:8608 MainThread giskard.rag  INFO     Finding topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Desktop\\Rag_Langchain\\virtualenv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-25 22:14:15,580 pid:8608 MainThread giskard.rag  INFO     Found 5 topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|██████████| 60/60 [02:38<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase \n",
    "from giskard.rag import generate_testset \n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "loader=PyPDFLoader(\"attention_is_all_you_need.pdf\")\n",
    "attention=loader.load()\n",
    "documents=text_splitter.split_documents(attention)\n",
    "vectorstore=DocArrayInMemorySearch.from_documents(\n",
    "    documents,embedding=OpenAIEmbeddings()\n",
    ")\n",
    "df=pd.DataFrame([file.page_content for file in documents],columns=[\"text\"])\n",
    "knowledge_base=KnowledgeBase(df)\n",
    "testset=generate_testset(\n",
    "    knowledge_base,\n",
    "    num_questions=60,\n",
    "    agent_description=\"A Chat bot answering questions related to Pdf uploaded by Users\"\n",
    ")\n",
    "test_set_df=testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.save(\"test-set.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question:{input}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query=\"Who are authors of Attention is all you need paper ? ?\"\n",
    "retrieved_results=vectorstore.similarity_search(query)\n",
    "context=retrieved_results[0].page_content\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors of the \"Attention Is All You Need\" paper are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt|ChatOpenAI()|StrOutputParser()\n",
    "response=chain.invoke(\n",
    "    {\n",
    "        \"context\":context,\n",
    "        \"input\":query\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(query,history=None):\n",
    "    query=query\n",
    "    retrieved_results=vectorstore.similarity_search(query)\n",
    "    context=retrieved_results[0].page_content\n",
    "    chain=prompt|ChatOpenAI()|StrOutputParser()\n",
    "    return chain.invoke({\"context\":context,\"input\":query })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 60/60 [02:23<00:00,  2.39s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 60/60 [01:07<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import evaluate\n",
    "\n",
    "report=evaluate(answer_fn,testset=testset,knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a1ccba31-84f7-413e-99e1-99447d16addd\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a1ccba31-84f7-413e-99e1-99447d16addd\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a1ccba31-84f7-413e-99e1-99447d16addd\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        72.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=72.0 class=\" progress-green \">72.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        55.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=55.00000000000001 class=\" progress-orange \">55.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        50.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=50.0 class=\" progress-orange \">50.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        55.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=55.00000000000001 class=\" progress-orange \">55.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">60%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">Focus on improving the conversational question handling by enhancing the Rewriter component, as it currently scores 0.0 and is likely impacting overall performance. Additionally, address the poor performance in the &#34;Transformer Model in Machine Translation&#34; topic to ensure more balanced knowledge base coverage.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"26f28ac1-6f3d-4f2f-a933-3b5b3b14761a\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1192\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1194\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1201\",\"attributes\":{\"factors\":[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1202\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1203\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1199\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1221\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1189\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1190\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1191\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JEmSJEmSTECqqqqqqqpQQAAAAAAAQE9AAAAAAADAUkAAAAAAAAA+QA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]],[\"colors\",[\"#d7ee89\",\"#5ab760\",\"#98d268\",\"#006837\",\"#a50026\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1222\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1223\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1218\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1219\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1220\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1230\",\"attributes\":{\"data_source\":{\"id\":\"p1189\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1231\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1232\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1227\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1228\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1229\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1240\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1234\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1235\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1236\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1241\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1242\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1237\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1238\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1239\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1200\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1214\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1209\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1210\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1211\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1212\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1204\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1205\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1206\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1207\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1208\",\"attributes\":{\"axis\":{\"id\":\"p1204\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1213\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1209\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1233\",\"attributes\":{\"location\":60.0,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1243\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1244\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1240\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"26f28ac1-6f3d-4f2f-a933-3b5b3b14761a\",\"roots\":{\"p1192\":\"bb1fd669-a163-4002-955c-8aa2c8347244\"},\"root_ids\":[\"p1192\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"bb1fd669-a163-4002-955c-8aa2c8347244\" data-root-id=\"p1192\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"fefdd209-931f-4658-8df5-4f17462e6e5e\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1123\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1053\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1013\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1014\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1016\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1044\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Others\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"length n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npa...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"and interpretable tree annotation. In Proceedings of the 21st International Conference on\\\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\\\\u2013440. ACL, July\\\\n2006.\\\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859, 2016.\\\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\\\n[32] Noam Shazeer, Azalia Mirhoseini,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\\\nAdvances in Neural Information Processing Systems, 2015.\\\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144, 2016.\\\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. D...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#ff7f0e\",\"#999\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#999\",\"#999\",\"#999\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#999\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#aec7e8\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1045\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1046\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1041\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1042\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1012\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1028\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1029\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1030\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1031\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1037\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1022\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1023\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1024\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1025\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1017\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1018\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1019\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1020\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1021\",\"attributes\":{\"axis\":{\"id\":\"p1017\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1026\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1022\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1047\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1048\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Attention Mechanisms in Neural Networks\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":12}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1049\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1050\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Self-Attention Mechanism\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1051\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Learning\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1052\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Translation\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":27}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1122\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1058\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1067\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1068\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1069\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1070\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1065\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1097\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1054\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1055\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1056\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xypyQKQPbT+/loBAXjtkQCckPUCjf7FA4MEFQNNf6z+IZrBAvrk1QCUcpz51hmg/5917QDGfTUCIZrBA1m8RQBGRnUBEzo1AdDQ/QKKnl0Dn3XtAInq1P765NUBPGY1ASbmjQF47ZEBI/IZAJRynPtjqeEDgwQVAyjv4P7+WgEDTX+s/RM6NQDBwAECjf7FAInq1P9jqeEDtYw9AKsy1PrZ/5T90BYlAA9JgQCUcpz51hmg/Inq1PyckPUDbqKg/NP9KQODBBUA0/0pAdAWJQO1jD0Ca3ZhA1uNZP9UPBUDxxIRAhJCKQF47ZEDbqKg/\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LBDxQKMsF0FTqgZBt2HHQOjDDEHpHeNAhvEOQS9oIUFtlhJBGJzhQJRwAkH/Jw5BspvIQEZsz0BtlhJBg4UcQb9T60A0evxAJwQEQQD3CUGym8hA2VkGQRic4UBOib5ANQ7+QLdhx0D88N5AlHACQY1G20CG8Q5BYRTkQFOqBkEvaCFBNHr8QETHyEDpHeNA2VkGQY1G20AJPAFBnucTQVh47kCVbhRBcB4cQZRwAkH/Jw5B2VkGQejDDEGPK/ZAZRv5QIbxDkFlG/lAlW4UQQk8AUHYW9dAdV8CQYVhB0GBBStB4hEiQbdhx0CPK/ZA\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Learning\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Translation\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\"]],[\"correctness\",[true,true,false,false,true,true,true,false,true,false,true,true,false,true,true,true,false,true,true,false,false,true,true,false,true,false,true,false,true,false,true,true,true,true,false,true,false,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false,false]],[\"questions\",[\"What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\",\"What does the attention head behavior in the encoder self-attention at layer 5 of 6 indicate?\",\"What translation tasks did the model achieve a new state of the art on?\",\"What BLEU scores does the Transformer (big) model achieve for English-to-German and English-to-French translations?\",\"What new network architecture is proposed in the paper &#x27;Attention Is All You Need&#x27;?\",\"What is the size of the vocabulary used in the semi-supervised setting?\",\"How many parallel attention layers or heads are employed in the described model?\",\"What are the two most commonly used attention functions mentioned in the context?\",\"Who are the authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27;?\",\"What is the impact of using positional embedding instead of sinusoids according to Table 3?\",\"Under what circumstances might using self-attention in models provide an additional advantage, specifically in terms of model interpretability, and how does this relate to the tasks performed by individual attention heads?\",\"In the context of anaphora resolution, what specific function do the attention heads in the fifth of six layers appear to perform, particularly when analyzing the word &#x27;its&#x27;?\",\"Considering the hardware and hyperparameters used, what was the total duration required to fully train both the base models and the big models, and how does the step time differ between them?\",\"In the context of evaluating English-to-German translation performance, why is it significant to alter the base model in various ways, and what specific insights or outcomes does this variation aim to reveal in the study?\",\"Identify the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27;, ensuring to include all authors as listed in the provided context.\",\"In the context of attention mechanisms, particularly when dealing with large values of dk, why is it necessary to scale the dot product by 1/\\\\u221adk, considering the impact on the softmax function and gradient behavior?\",\"Under the semi-supervised setting for English constituency parsing, what F1 score did the Transformer model achieve, and how does it compare to the scores of other models listed in the same context?\",\"Considering the context, what is the primary benefit of using the Transformer model over other models like RNNs or convolutional layers specifically for translation tasks, and how does it impact training speed and performance?\",\"In the context of transduction models, what is the fundamental mechanism that the Transformer model uniquely utilizes, avoiding traditional sequence-aligned RNNs or convolution methods?\",\"Identify the individuals who were instrumental in both the design and implementation phases of tensor2tensor, and specify any affiliations they had during this work.\",\"Considering the auto-regressive nature of the model&#x27;s decoder, how long did it take to train the base models, and how many steps were involved in this process?\",\"Considering the use of multi-head attention in Transformers, why do self-attention layers exhibit faster computational complexity compared to recurrent layers, especially when dealing with large input sequences?\",\"In the context of the Transformer architecture that relies solely on attention mechanisms, how does reducing the attention key size dk impact the model quality, and are there any specific conditions under which this effect is observed?\",\"Given the constraint of using the Section 22 development set for hyperparameter tuning, what is the BLEU score achieved by the &#x27;big&#x27; Transformer model variation on the English-to-German translation task?\",\"Under the condition that the evaluation involves a comparison with neural machine translation architectures, what specific task was employed to assess whether the Transformer model can generalize to other tasks?\",\"Under the condition that proper attribution is provided, could you specify the BLEU scores achieved by the Transformer (big) model for both English-to-German and English-to-French translations?\",\"Under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, what BLEU score did it achieve on the WMT 2014 English-to-French translation task?\",\"Considering the ability of self-attention mechanisms to follow long-distance dependencies, as demonstrated in attention visualizations, what is a potential side benefit of using self-attention in models?\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, what is the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task?\",\"Considering the model&#x27;s decoder configuration with its unique sub-layer structure, how many parallel attention layers or heads are implemented in this model?\",\"Hi, I&#x27;m studying for my computer science exam and I&#x27;m trying to understand the role of masking in the decoder&#x27;s self-attention layers. Could you explain what its purpose is?\",\"As an AI researcher working on attention-based models for translating text to audio, I&#x27;m curious to know what tasks the model achieved a new state of the art in?\",\"Hi, I&#x27;m going through my lecture notes on neural networks and I&#x27;m a bit confused. Could you explain the main difference between dot-product attention and additive attention? I need to understand this for my upcoming exam.\",\"Hi, I&#x27;m diving into the latest research on neural network models for my graduate studies. Can you explain how the Transformer model performs compared to RNN sequence-to-sequence models and the Berkeley-Parser, especially in the context of translating English into German and French?\",\"Hi, I&#x27;m trying to understand a neural network architecture from a research paper I&#x27;m studying late at night. Could you tell me what the dimensionality of the input and output is in the FFN described in the document?\",\"Hello, as I&#x27;m working on improving sentence parsing accuracy in machine translation systems, could you tell me what the size of the vocabulary is in the semi-supervised setting?\",\"Hi, I&#x27;m reviewing my notes for an upcoming exam and I&#x27;m a bit confused. Why are self-attention layers considered faster than recurrent layers in terms of computational complexity?\",\"Hi, I&#x27;m working on my thesis about advancements in natural language processing, and I&#x27;m analyzing different machine translation models. Could you tell me what BLEU score the big transformer model achieved on the WMT 2014 English-to-German translation task?\",\"Hi, I&#x27;m a computer science student trying to grasp how self-attention mechanisms enhance tasks such as reading comprehension and learning sentence representations. Can you explain what self-attention is and in which tasks it has been used successfully?\",\"Hi, I&#x27;m working on a research paper about changes in voter registration laws and how they impact electoral processes. Could you explain what Figure 3 illustrates about the attention mechanism in the encoder?\",\"What is the purpose of the third sub-layer in the decoder and how does the decoder ensure that predictions for position i depend only on known outputs at positions less than i?\",\"Who explored the limits of language modeling in a 2016 preprint and what is the title of the work by Nal Kalchbrenner and colleagues related to neural machine translation?\",\"Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; and what is the title of the paper authored by Francois Chollet in 2016?\",\"How do separable convolutions affect computational complexity in neural networks and what is a potential benefit of using self-attention in models?\",\"What is the main point expressed about the law in the context and what does Figure 4 illustrate regarding attention heads in layer 5?\",\"How does the computational complexity of self-attention layers compare to recurrent layers and what kind of operations do recurrent layers require?\",\"What permission does Google grant regarding the reproduction of tables and figures in the paper &#x27;Attention Is All You Need&#x27;, and what is the main innovation proposed in the paper?\",\"Why did the researchers choose the sinusoidal version of positional embeddings and what are the three desiderata considered for using self-attention?\",\"What is the main architectural feature of the Transformer model proposed in the work and how does it compare to other models like Extended Neural GPU, ByteNet, and ConvS2S in terms of parallelization?\",\"What is the purpose of multi-head attention in the model described and how many parallel attention layers or heads are employed?\",\"What is the main method for achieving this?\",\"What is it?\",\"What is it and where has it been applied?\",\"Which one was utilized?\",\"What are they?\",\"What are they?\",\"Who are they?\",\"What is it?\",\"What are those scores?\",\"Why was that option selected?\"]],[\"agent_answer\",[\"The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"To answer the question, we need to analyze the provided context which mentions that many of the attention heads exhibit behavior related to the structure of the sentence in the encoder self-attention at layer 5 of 6. Two examples are given from two different heads, showing that the heads learned to perform different tasks.\\\\n\\\\nBased on this information, we can infer that the attention head behavior in the encoder self-attention at layer 5 of 6 indicates that the model is able to focus on different aspects or components of the input sentence at that specific layer. Each attention head may be responsible for attending to specific parts of the sentence&#x27;s structure or content, indicating a level of specialization or task allocation within the neural network. This behavior suggests that the model is processing and analyzing the input text in a nuanced and structured manner, which can contribute to its overall performance in tasks such as language understanding and generation.\",\"The model achieved a new state of the art on the WMT 2014 English-to-French translation task, establishing a new single-model BLEU score of 41.8 after training for 3.5 days on eight GPUs.\",\"The provided context only mentions the BLEU score achieved by the Transformer (big) model for English-to-French translation, which is 41.0. There is no information given about the BLEU score achieved for English-to-German translation.\",\"The new network architecture proposed in the paper &#x27;Attention Is All You Need&#x27; is called the Transformer. It is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The size of the vocabulary used in the semi-supervised setting is 32K tokens.\",\"In the described model, 8 parallel attention layers or heads are employed.\",\"Based on the context provided, the two most commonly used attention functions are:\\\\n1. Dot-product attention\\\\n2. Scaled dot-product attention\\\\n\\\\nExplanation:\\\\n- Dot-product attention is a type of attention mechanism where the query vector is multiplied by the key vector to compute the attention scores.\\\\n- Scaled dot-product attention is similar to dot-product attention but includes scaling by the square root of the dimension of the key vectors to prevent the gradients from becoming too small during training.\",\"The authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Based on the information provided, the impact of using positional embedding instead of sinusoids is an improvement in BLEU score from 4.92 to 25.7. This suggests that using positional embeddings is more effective than using sinusoids in this context.\",\"Using self-attention in models can provide an additional advantage in terms of model interpretability when trying to understand how the model is making predictions or capturing relationships within the input data. Self-attention allows the model to focus on different parts of the input sequence when generating output, and this attention mechanism can provide insights into which parts of the input are most important for making predictions. \\\\n\\\\nIn the context of individual attention heads, each head in a multi-head attention mechanism is responsible for learning different patterns and relationships within the input data. By analyzing the outputs of individual attention heads, we can gain a better understanding of the specific features or relationships that each head is focusing on. This can help in interpreting how the model is processing the input data and making predictions, as well as in identifying any biases or inconsistencies in the learned representations.\",\"The attention heads in the fifth layer of six appear to be involved in anaphora resolution, specifically when analyzing the word &#x27;its&#x27;. The attentions for the word &#x27;its&#x27; are very sharp, indicating that these attention heads are focusing on resolving references to this specific word and its antecedents within the text. This suggests that these attention heads in layer 5 are responsible for identifying and connecting pronouns like &#x27;its&#x27; to their corresponding nouns or antecedents in order to establish coherence and understanding in the text.\",\"To calculate the total duration required to fully train both the base models and the big models, we need to consider the number of steps taken for each type of model and the time taken for each step.\\\\n\\\\nFor the base models:\\\\n- Each training step took about 0.4 seconds.\\\\n- Total training steps for the base models: 100,000 steps\\\\n- Total training time for base models = 0.4 seconds/step * 100,000 steps = 40,000 seconds or approximately 11.11 hours.\\\\n\\\\nFor the big models:\\\\n- Step time was 1.0 seconds.\\\\n- Total training steps for the big models: 300,000 steps\\\\n- Total training time for big models = 1.0 seconds/step * 300,000 steps = 300,000 seconds or approximately 83.33 hours or 3.47 days.\\\\n\\\\nTherefore, the total duration required to fully train both the base models and the big models is approximately 11.11 hours for the base models and 83.33 hours for the big models, totaling about 94.44 hours or approximately 3.93 days.\\\\n\\\\nThe step time for the base models was 0.4 seconds, while the step time for the big models was 1.0 second. The step time for the big models is 2.5 times longer than the step time for the base models.\",\"In the context of evaluating English-to-German translation performance, it is significant to alter the base model in various ways because it allows researchers to understand the importance of different components of the Transformer model. By varying the base model, researchers can measure the change in performance and identify which components have the most significant impact on translation quality. This variation aims to reveal insights into how each component contributes to the overall performance of the model and helps researchers optimize the model for better translation results. By studying the variations in the base model, researchers can gain a deeper understanding of the Transformer architecture and make informed decisions on how to improve translation quality while minimizing training costs.\",\"The individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"When dealing with large values of dk in attention mechanisms, it becomes necessary to scale the dot product by 1/\\\\u221adk to prevent the dot products from growing too large in magnitude. This is important because when the dot products become too large, it can push the softmax function into regions where it has extremely small gradients. Scaling the dot products helps to prevent this issue by keeping the values within a reasonable range, which in turn helps to maintain more stable gradients during the optimization process. This scaling factor ensures that the softmax function operates effectively and prevents it from encountering numerical stability issues that could affect the training process of the model.\",\"The Transformer model achieved an F1 score of 92.7 under the semi-supervised setting for English constituency parsing. This score is higher than the F1 scores of other models listed in the context, with the closest scores being 92.1 achieved by models such as Zhu et al. (2013) and Vinyals &amp; Kaiser el al. (2014) under the semi-supervised setting. Additionally, the Transformer model outperformed models trained only on the WSJ dataset, which had F1 scores ranging from 88.3 to 91.7.\",\"The primary benefit of using the Transformer model over other models like RNNs or convolutional layers for translation tasks is that the Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. This design allows the Transformer model to be more parallelizable, meaning it can process input data more efficiently in parallel, resulting in significantly less time required for training. This increased parallelization and elimination of recurrence and convolutions also contribute to the superior quality of the Transformer model compared to other models. The experimental results mentioned in the context show that the Transformer model achieves better translation quality while requiring significantly less time to train, making it a more efficient and effective choice for translation tasks.\",\"In the context of transduction models, the fundamental mechanism that the Transformer model uniquely utilizes is self-attention. The Transformer model computes representations of its input and output entirely based on self-attention, without relying on sequence-aligned RNNs or convolution methods.\",\"The individuals instrumental in both the design and implementation phases of tensor2tensor are Lukasz and Aidan. They were affiliated with Google Brain and Google Research during this work.\",\"To determine how long it took to train the base models and how many steps were involved in the process, we can refer to the information provided in the context. \\\\n\\\\nFor the base models, each training step took about 0.4 seconds. The base models were trained for a total of 100,000 steps. Therefore, to calculate the total training time for the base models, we can multiply the time taken per step by the total number of steps:\\\\n\\\\n0.4 seconds/step * 100,000 steps = 40,000 seconds\\\\n\\\\nSince there are 60 seconds in a minute and 60 minutes in an hour, we can convert the total seconds to hours:\\\\n\\\\n40,000 seconds / (60 seconds * 60 minutes) = 11.11 hours\\\\n\\\\nTherefore, it took approximately 11.11 hours to train the base models, involving 100,000 steps in the process.\",\"When using multi-head attention in Transformers, self-attention layers exhibit faster computational complexity compared to recurrent layers for several reasons:\\\\n\\\\n1. Parallelization: Self-attention layers can process input sequences in parallel, as each token can attend to all other tokens simultaneously. This parallel processing allows for more efficient computation compared to recurrent layers, which process sequences sequentially.\\\\n\\\\n2. Reduced sequential operations: In self-attention layers, the number of operations required is O(n) for each token in the input sequence, regardless of the sequence length. On the other hand, recurrent layers require O(n) sequential operations for each token, resulting in a higher computational cost for larger input sequences.\\\\n\\\\n3. Scalability: Self-attention layers are more scalable when dealing with large input sequences because the computational cost remains constant regardless of the sequence length. In contrast, recurrent layers become increasingly computationally expensive as the sequence length grows.\\\\n\\\\n4. Long-range dependencies: Self-attention layers can capture long-range dependencies in the input sequence more effectively than recurrent layers, as each token can directly attend to any other token in the sequence. This ability to capture dependencies across long distances contributes to the efficiency of self-attention layers in handling large input sequences.\\\\n\\\\nOverall, the combination of parallel processing, reduced sequential operations, scalability, and the ability to capture long-range dependencies makes self-attention layers more computationally efficient than recurrent layers when dealing with large input sequences in Transformers.\",\"Reducing the attention key size dk in the Transformer architecture can impact the model quality by potentially limiting the model&#x27;s ability to capture complex relationships and dependencies in the input and output sequences. A smaller attention key size may result in a decrease in the model&#x27;s capacity to effectively attend to relevant information across the sequences, leading to lower performance in tasks that require capturing long-range dependencies.\\\\n\\\\nSpecific conditions under which this effect may be observed include tasks that involve processing longer sequences, where the reduced attention key size may hinder the model&#x27;s ability to effectively capture and utilize information from distant parts of the sequence. Additionally, tasks that require precise modeling of complex relationships or dependencies across the sequences may also be impacted by a smaller attention key size, as it may limit the model&#x27;s capacity to accurately represent and learn from the input data.\",\"Based on the provided context, the &#x27;big&#x27; Transformer model variation achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"The specific task employed to assess whether the Transformer model can generalize to other tasks was English constituency parsing.\",\"Based on the provided context, the BLEU score achieved by the Transformer (big) model for English-to-French translation is 41.0. However, there is no specific mention of the BLEU score achieved by the Transformer (big) model for English-to-German translation in the given text. Therefore, without additional information, it is not possible to specify the BLEU score for English-to-German translation.\",\"To determine the BLEU score achieved by the big model on the WMT 2014 English-to-French translation task, we need to refer to the given information. The big model achieved a BLEU score of 41.0, outperforming all previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. \\\\n\\\\nGiven that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, we can conclude that the BLEU score achieved by the big model on the WMT 2014 English-to-French translation task is 41.0.\",\"One potential side benefit of using self-attention in models is the ability to effectively capture and learn dependencies between distant positions in a sequence. This can lead to improved performance in tasks that require understanding relationships and context across a large span of the input sequence. Additionally, self-attention can help in tasks such as reading comprehension, abstractive summarization, and textual entailment by allowing the model to focus on relevant parts of the input sequence and create more accurate representations.\",\"Based on the provided context, the big transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"In the provided context, the decoder in the model is composed of N=6 identical layers. Each layer in the decoder has three sub-layers - two for self-attention within the decoder layer and one for multi-head attention over the output of the encoder stack. \\\\n\\\\nTherefore, in each decoder layer, there are two parallel attention layers for self-attention and one parallel attention layer for multi-head attention over the output of the encoder stack. Since there are N=6 identical layers in the decoder, the total number of parallel attention layers or heads implemented in this model would be 6 * (2 + 1) = 18.\",\"Masking in the decoder&#x27;s self-attention layers serves the purpose of preventing positions from attending to subsequent positions. This is important because in the context of the decoder, it ensures that the predictions for position i can only depend on the known outputs at positions less than i. By applying masking in the self-attention sub-layer of the decoder stack, the model is able to focus only on the relevant information and avoid incorporating future information that should not be available during the prediction process. This helps improve the accuracy and effectiveness of the decoder in generating the output sequence based on the input sequence.\",\"The model achieved a new state of the art in machine translation tasks, specifically in the WMT 2014 English-to-German and English-to-French translation tasks.\",\"Sure! Let&#x27;s break down the main differences between dot-product attention and additive attention based on the context provided.\\\\n\\\\n1. Dot-product attention:\\\\n- In dot-product attention, the matrix of outputs is computed using the formula:\\\\nAttention(Q, K, V) = softmax(QKT / \\\\u221adk) V\\\\n- The key difference in dot-product attention is the scaling factor of 1/\\\\u221adk, where dk represents the dimensionality of the keys and queries.\\\\n- Dot-product attention is faster and more space-efficient in practice because it can be implemented using highly optimized matrix multiplication code.\\\\n\\\\n2. Additive attention:\\\\n- Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\\\\n- While dot-product attention is faster and more space-efficient, additive attention can outperform dot-product attention without scaling for larger values of dk.\\\\n- For small values of dk, both mechanisms perform similarly, but additive attention is more effective for larger values of dk.\\\\n\\\\nIn summary, the main difference between dot-product attention and additive attention lies in the way the compatibility function is computed and the scaling factor used in the matrix of outputs. Dot-product attention is faster and more space-efficient, while additive attention can be more effective for larger values of dk.\",\"Sure! Based on the provided context, the Transformer model outperforms RNN sequence-to-sequence models and the Berkeley-Parser in translating English into German and French. The results in Table 4 show that the Transformer model performs surprisingly well, even without task-specific tuning, yielding better results than all previously reported models except for the Recurrent Neural Network Grammar. Additionally, the Transformer model can be trained significantly faster than architectures based on recurrent or convolutional layers, leading to achieving a new state of the art in translation tasks such as WMT 2014 English-to-German and WMT 2014 English-to-French. This suggests that the Transformer model is a promising and efficient approach for translation tasks compared to traditional RNN sequence-to-sequence models and the Berkeley-Parser.\",\"Based on the provided context, the dimensionality of the input and output in the Feedforward Neural Network (FFN) described in the document is not explicitly mentioned. However, it is stated that the network architecture includes self-attention layers and recurrent layers, which have different characteristics in terms of computational complexity and parallelization. To determine the exact dimensionality of the input and output in the FFN, you may need to refer to the specific details and equations provided in the research paper regarding the structure and implementation of the neural network.\",\"Based on the provided context, the vocabulary size in the semi-supervised setting is 32K tokens.\",\"To answer this question, we need to break down the reasons why self-attention layers are considered faster than recurrent layers in terms of computational complexity. \\\\n\\\\n1. Self-attention layers operate on all input elements simultaneously, whereas recurrent layers process inputs sequentially. This means that self-attention layers can process multiple elements in parallel, leading to faster computation.\\\\n\\\\n2. The computational complexity of self-attention layers is O(n^2) due to the dot product calculations between all input elements. However, this can still be more efficient than the O(n) sequential operations required by recurrent layers, especially for long sequences.\\\\n\\\\n3. Additionally, self-attention layers have been optimized with techniques like scaled dot-product attention and efficient implementations like the Transformer model, which further improve their computational efficiency.\\\\n\\\\nIn conclusion, self-attention layers are considered faster than recurrent layers in terms of computational complexity because they can process inputs in parallel, leading to potentially quicker computations, especially for longer sequences.\",\"The big transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Self-attention, also known as intra-attention, is a mechanism that relates different positions within a single sequence to compute a representation of that sequence. This means that each position in the sequence can attend to other positions to obtain a better understanding of the context.\\\\n\\\\nIn tasks such as reading comprehension, self-attention helps the model focus on relevant parts of the text to better answer questions about it. By attending to different positions in the text, the model can capture relationships and dependencies between words or phrases, leading to more accurate comprehension.\\\\n\\\\nSimilarly, in tasks like abstractive summarization, textual entailment, and learning task-independent sentence representations, self-attention allows the model to consider the entire input sequence simultaneously and capture long-range dependencies effectively. This leads to improved performance in generating concise summaries, determining textual entailment relationships, and creating meaningful sentence representations.\\\\n\\\\nOverall, self-attention has been successfully used in various natural language processing tasks where capturing relationships between different parts of a sequence is crucial for achieving high performance.\",\"To answer the question about what Figure 3 illustrates about the attention mechanism in the encoder, we need to consider the context provided. \\\\n\\\\nIn the context, it mentions that Figure 3 shows an example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. It further explains that many of the attention heads attend to a distant dependency of the verb &#x27;making&#x27;, completing the phrase &#x27;making...more difficult&#x27;. The figure also shows attentions only for the word &#x27;making&#x27;, with different colors representing different heads.\\\\n\\\\nBased on this information, we can infer that Figure 3 illustrates how the attention mechanism in the encoder focuses on distant dependencies within the text. In this specific example, the attention heads are looking at the verb &#x27;making&#x27; and its relationship to the phrase &#x27;more difficult&#x27;. This visualization helps researchers understand how the model processes and assigns importance to different parts of the input text, which can be valuable for analyzing and interpreting the model&#x27;s decisions and predictions related to voter registration laws and electoral processes.\",\"The purpose of the third sub-layer in the decoder is to perform multi-head attention over the output of the encoder stack. This allows the decoder to gather information from the encoder&#x27;s outputs and incorporate it into its own decoding process.\\\\n\\\\nTo ensure that predictions for position i depend only on known outputs at positions less than i, the decoder modifies the self-attention sub-layer by implementing masking. This masking prevents positions from attending to subsequent positions, meaning that the model can only consider information from positions that have already been decoded. Additionally, the output embeddings are offset by one position, further enforcing that predictions for position i can only be based on information from positions less than i. This mechanism helps the decoder maintain the correct order of information flow during the decoding process.\",\"The limits of language modeling were explored by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu in a 2016 preprint titled \\\\\"Exploring the limits of language modeling\\\\\" (arXiv preprint arXiv:1602.02410).\\\\n\\\\nThe work by Nal Kalchbrenner and colleagues related to neural machine translation is titled \\\\\"Neural machine translation in linear time\\\\\" (arXiv preprint arXiv:1610.10099v2, 2017).\",\"The authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\\\\n\\\\nThe title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions.&#x27;\",\"Separable convolutions decrease the computational complexity in neural networks considerably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2), where k is the factor of increased complexity compared to recurrent layers. Even with k = n, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer. \\\\n\\\\nA potential benefit of using self-attention in models is that it could yield more interpretable models. By inspecting attention distributions from the models, it is possible to see that individual attention heads learn to perform different tasks, and many exhibit behavior related to the syntactic and semantic structure of the sentences. This can lead to more interpretable and explainable models, which can be beneficial in various applications.\",\"The main point expressed about the law in the context is that while the law will never be perfect, its application should be just. This implies that there may be flaws in the law itself, but it is important for it to be applied fairly and justly.\\\\n\\\\nFigure 4 illustrates that in layer 5 of 6, there are two attention heads apparently involved in anaphora resolution. The top part of Figure 4 shows the full attentions for head 5, while the bottom part shows isolated attentions from just the word &#x27;its&#x27; for attention heads 5 and 6. It is noted that the attentions are very sharp for this word, indicating a high level of focus and importance placed on resolving anaphora involving the word &#x27;its&#x27;. This suggests that in layer 5, there is a particular emphasis on resolving references and maintaining coherence in the text.\",\"In the provided context, it is stated that self-attention layers have a computational complexity that is faster than recurrent layers. This is because self-attention layers are able to perform parallel operations, while recurrent layers require sequential operations. Recurrent layers require O(n) sequential operations, where n represents the length of the sequence being processed. This means that for every input in the sequence, a recurrent layer needs to perform operations in a sequential manner, which can lead to higher computational complexity compared to self-attention layers that can operate in parallel.\",\"Google grants permission to reproduce the tables and figures in the paper &#x27;Attention Is All You Need&#x27; solely for use in journalistic or scholarly works, provided proper attribution is provided.\\\\n\\\\nThe main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms. This architecture dispenses with recurrence and convolutions, which are commonly used in traditional sequence transduction models.\",\"The researchers chose the sinusoidal version of positional embeddings because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\\\n\\\\nThe three desiderata considered for using self-attention are:\\\\n1. Total computational complexity per layer\\\\n2. Amount of computation that can be parallelized\\\\n3. Minimum number of sequential operations required for parallelization.\",\"The main architectural feature of the Transformer model proposed in the work is the use of an attention mechanism to draw global dependencies between input and output, eschewing recurrence. This allows the Transformer to rely entirely on attention for capturing relationships between different parts of the input sequence.\\\\n\\\\nIn terms of parallelization, the Transformer model allows for significantly more parallelization compared to other models like Extended Neural GPU, ByteNet, and ConvS2S. These other models use convolutional neural networks as basic building blocks and compute hidden representations in parallel for all input and output positions. However, in these models, the number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions, either linearly for ConvS2S or logarithmically for ByteNet. On the other hand, the Transformer model does not have this limitation and can achieve higher levels of parallelization, leading to faster training times and potentially better performance in tasks such as translation.\",\"The purpose of multi-head attention in the model described is to allow the model to jointly attend to information from different representation subspaces at different positions. This helps the model to capture more complex relationships and dependencies in the input data. \\\\n\\\\nIn this specific model, 8 parallel attention layers or heads are employed. Each head performs its own attention mechanism on the input data, and the results are concatenated and projected to produce the final output values. This allows for more comprehensive attention to be applied to the input data, enhancing the model&#x27;s ability to learn and make predictions.\",\"The main method for achieving the attention mechanism following long-distance dependencies in the encoder self-attention is through the use of attention heads. These attention heads attend to distant dependencies in the input data, allowing the model to focus on specific parts of the input sequence that are relevant for the task at hand. In the provided context, the attention heads in layer 5 of 6 are shown attending to the verb &#x27;making&#x27;, completing the phrase &#x27;making...more difficult&#x27;. This method helps the model capture important relationships and dependencies in the input data, improving its performance on various tasks.\",\"\\\\\"It\\\\\" in this context refers to the application of the law being just. The text is discussing how although the law may never be perfect, its application should be fair and just.\",\"In the provided context, \\\\\"it\\\\\" refers to the application of the Law. The statement emphasizes that while the Law may never be perfect, its application should be just. This concept of ensuring just application of the Law has been discussed in the context of anaphora resolution in attention heads in layer 5 of the model being analyzed.\",\"In the provided context, a vocabulary of 16K tokens was utilized for the WSJ only setting, and a vocabulary of 32K tokens was utilized for the semi-supervised setting.\",\"Based on the context provided, \\\\\"they\\\\\" refers to the attention heads in the encoder self-attention at layer 5 of 6. These attention heads exhibit behavior that seems related to the structure of the sentence and have clearly learned to perform different tasks.\",\"Based on the provided context, \\\\\"they\\\\\" refer to the attention heads in the encoder self-attention at layer 5 of 6. These attention heads exhibit behavior related to the structure of the sentence and have clearly learned to perform different tasks.\",\"The context does not provide specific information about who \\\\\"they\\\\\" are referring to. It seems to be a general statement about the behavior of attention heads in the encoder self-attention at layer 5 of 6. \\\\\"They\\\\\" could potentially refer to the attention heads or the tasks they are performing. Further context or clarification would be needed to determine the exact identity of \\\\\"they\\\\\" in this case.\",\"Based on the context provided, \\\\\"it\\\\\" refers to the application of the Law being just. The speaker believes that while the Law may never be perfect, its application should be fair and just, which is what they feel is lacking.\",\"Based on the provided context, it is not explicitly mentioned what specific scores are being referred to. However, it can be inferred that the scores could potentially be related to the performance evaluation metrics of the translation model being discussed. These scores could include metrics such as BLEU score, accuracy, precision, recall, F1 score, etc. To accurately determine what scores are being referred to, further information or clarification may be needed.\",\"The option was selected because it provides a detailed explanation of the attention mechanism in the context of encoder self-attention in layer 5. It describes how the attention heads focus on a distant dependency related to the verb &#x27;making&#x27;, specifically completing the phrase &#x27;making...more difficult&#x27;. The explanation also mentions that different colors represent different heads, making it easier to visualize and understand the attention process. Overall, this option was chosen to highlight the complexity and effectiveness of the attention mechanism in capturing long-distance dependencies in the text.\"]],[\"reference_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Many of the attention heads exhibit behavior that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The paper proposes a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Using positional embedding instead of sinusoids results in a BLEU score of 4.92 compared to 25.7.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The attention heads in layer 5 of 6 appear to be involved in anaphora resolution.\",\"The base models were trained for a total of 100,000 steps or 12 hours. The big models were trained for 300,000 steps (3.5 days).\",\"To evaluate the importance of different components of the Transformer, measuring the change in performance on English-to-German translation.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"The dot products are scaled by 1/\\\\u221adk to counteract the effect of large values of dk, which cause the dot products to grow large in magnitude and push the softmax function into regions where it has extremely small gradients.\",\"The Transformer achieved an F1 score of 92.7 in the semi-supervised setting.\",\"The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\",\"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\",\"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor.\",\"The base models were trained for a total of 100,000 steps or 12 hours.\",\"Self-attention layers are faster than recurrent layers because they require O(1) sequential operations, whereas a recurrent layer requires O(n) sequential operations.\",\"Reducing the attention key size dk hurts model quality.\",\"The BLEU score for the &#x27;big&#x27; Transformer model variation is 26.4.\",\"The task used to evaluate if the Transformer can generalize to other tasks was English constituency parsing.\",\"The Transformer (big) model achieves BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The big model achieved a BLEU score of 41.0.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The purpose of masking in the decoder&#x27;s self-attention layers is to prevent leftward information flow in the decoder to preserve the auto-regressive property by masking out (setting to \\\\u2212\\\\u221e) all values in the input of the softmax which correspond to illegal connections.\",\"The model achieved a new state of the art in the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"Dot-product attention includes a scaling factor of 1/\\\\u221adk, whereas additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\",\"The Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences, and it performs surprisingly well compared to RNN sequence-to-sequence models.\",\"The dimensionality of input and output is dmodel = 512.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"Self-attention layers are considered faster than recurrent layers in terms of computational complexity because they require O(1) executed operations, whereas a recurrent layer requires O(n) sequential operations.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"Figure 3 illustrates the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6, where many of the attention heads attend to a distant dependency of the verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019.\",\"The third sub-layer in the decoder performs multi-head attention over the output of the encoder stack, and the decoder ensures that predictions for position i depend only on known outputs at positions less than i by modifying the self-attention sub-layer to prevent positions from attending to subsequent positions, and the output embeddings are offset by one position.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu explored the limits of language modeling in a 2016 arXiv preprint, and the title of the work by Nal Kalchbrenner and colleagues is &#x27;Neural machine translation in linear time.&#x27;\",\"The authors of the paper &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, and the title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions.&#x27;\",\"Separable convolutions decrease the complexity considerably to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2), even though with k = n, the complexity is equal to the combination of a self-attention layer and a point-wise feed-forward layer; a side benefit of self-attention is that it could yield more interpretable models, as individual attention heads learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences.\",\"The main point expressed is that the law will never be perfect, but its application should be just, which is considered to be missing; Figure 4 illustrates two attention heads in layer 5 of 6, involved in anaphora resolution, showing full attentions for head 5 and isolated attentions from the word &#x27;its&#x27; for attention heads 5 and 6, with very sharp attentions noted for this word.\",\"Self-attention layers are faster than recurrent layers in terms of computational complexity when the sequence is considered, and recurrent layers require O(n) sequential operations.\",\"Google grants permission to reproduce the tables and figures in the paper solely for use in journalistic or scholarly works, provided proper attribution is given, and the main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The researchers chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training, and the three desiderata considered for using self-attention are the total computational complexity per layer, the amount of computation that can be parallelized, and the minimum number of sequential operations required.\",\"The main architectural feature of the Transformer model is that it eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization compared to models like Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks and have operations that grow with the distance between input or output positions.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, and in this work, 8 parallel attention layers or heads are employed.\",\"The Transformer model relies entirely on an attention mechanism to draw global dependencies between input and output.\",\"Structured attention networks.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"The standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\",\"The three factors mentioned are: the total computational complexity per layer, the amount of computation that can be parallelized (measured by the minimum number of sequential operations required), and the path length between long-range dependencies in the network.\",\"The Transformer uses multi-head attention in three different ways: 1) In &#x27;encoder-decoder attention&#x27; layers, where queries come from the previous decoder layer and the memory keys and values come from the output of the encoder. 2) In encoder self-attention layers, where all keys, values, and queries come from the output of the previous layer in the encoder. 3) In decoder self-attention layers, allowing each position in the decoder to attend to all positions in the decoder up to and including that position.\",\"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\",\"Neural machine translation by jointly learning to align and translate.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The sinusoidal version was chosen because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"]],[\"id\",[1,51,39,29,0,36,16,14,45,34,26,50,28,32,45,15,37,38,8,3,28,24,34,33,35,29,31,26,30,16,18,39,14,38,19,36,24,30,7,49,11,43,41,26,50,24,0,22,6,16,6,43,7,27,23,17,47,40,29,22]],[\"content\",[\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\"]],[\"color\",[\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1098\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1099\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1094\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1095\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1096\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1112\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1103\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1104\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1105\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1113\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1114\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1109\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1110\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1111\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1066\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1081\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1082\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1083\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1084\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1089\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1090\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1057\",\"attributes\":{\"renderers\":[{\"id\":\"p1097\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1076\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1077\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1078\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1079\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1071\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1072\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1073\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1074\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1075\",\"attributes\":{\"axis\":{\"id\":\"p1071\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1080\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1076\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1100\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1101\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1097\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1102\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1097\"}],\"index\":0}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1118\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1115\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1116\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1117\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"MsVdQNX8hkCz0L0/P0X4Pw==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JQ4OQWx710AVgvJANvYVQQ==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"fefdd209-931f-4658-8df5-4f17462e6e5e\",\"roots\":{\"p1123\":\"fe9208b0-274b-4a7e-a629-16a598ccb5ce\"},\"root_ids\":[\"p1123\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"fe9208b0-274b-4a7e-a629-16a598ccb5ce\" data-root-id=\"p1123\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x178015edf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.7\n",
       "conversational               0.0\n",
       "distracting element          0.5\n",
       "double                       1.0\n",
       "simple                       0.6\n",
       "situational                  0.8"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "client = OpenAI(api_key=perplexity_api_key, base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\n",
    "\"\"\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention the name of a Vice Chancellor. The references listed are related to research papers on neural machine translation and parsing, and do not include information about university administrators or their titles. Therefore, it is not possible to answer the question based on the given context.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the name of Vice Chancellor ?\"\n",
    "retrieved_results = vectorstore.similarity_search(query)  \n",
    "context = retrieved_results[0].page_content\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant trained to answer based on provided context.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_template.format(context=context, input=query)},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-sonar-huge-128k-online\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(query, history=None):\n",
    "    retrieved_results = vectorstore.similarity_search(query)\n",
    "    context = retrieved_results[0].page_content\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant trained to answer based on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(context=context, input=query)},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-sonar-huge-128k-online\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 60/60 [12:22<00:00, 12.37s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 60/60 [01:03<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import evaluate\n",
    "\n",
    "report = evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"b1181e12-d34b-489d-8690-20b88936014d\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b1181e12-d34b-489d-8690-20b88936014d\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b1181e12-d34b-489d-8690-20b88936014d\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        82.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=82.0 class=\" progress-green \">82.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        70.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=70.0 class=\" progress-green \">70.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        53.33%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=53.333333333333336 class=\" progress-orange \">53.33%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        75.46%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=75.46296296296296 class=\" progress-green \">75.46%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">70%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">Improve the RAG system by focusing on enhancing the Rewriter and Retriever components, as evidenced by the low scores in conversational and distracting questions. Additionally, aligning the knowledge base to ensure more consistent performance across all topics is crucial.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"d32de13d-1d6f-42b5-a430-b7f26f202122\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1437\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1439\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1446\",\"attributes\":{\"factors\":[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1447\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1448\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1444\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1466\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1434\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1435\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1436\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"t23btm3bUUC0l9BeQntPQAAAAAAAwFJAAAAAAADgVUAAAAAAAIBRQA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]],[\"colors\",[\"#fdc473\",\"#a50026\",\"#fefbb9\",\"#006837\",\"#fba55c\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1467\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1468\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1463\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1464\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1465\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1475\",\"attributes\":{\"data_source\":{\"id\":\"p1434\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1476\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1477\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1472\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1473\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1474\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1485\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1479\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1480\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1481\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1486\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1487\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1482\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1483\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1484\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1445\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1459\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1454\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1455\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1456\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1457\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1449\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1450\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1451\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1452\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1453\",\"attributes\":{\"axis\":{\"id\":\"p1449\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1458\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1454\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1478\",\"attributes\":{\"location\":70.0,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1488\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1489\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1485\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"d32de13d-1d6f-42b5-a430-b7f26f202122\",\"roots\":{\"p1437\":\"b46c627e-75cd-4080-94a4-04944fd1272f\"},\"root_ids\":[\"p1437\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"b46c627e-75cd-4080-94a4-04944fd1272f\" data-root-id=\"p1437\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"452e0beb-aae7-4777-b28e-67f350c5cd93\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1368\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1298\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1249\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1258\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1259\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1260\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1261\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1256\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1289\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1246\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1247\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1248\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Others\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"length n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npa...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"and interpretable tree annotation. In Proceedings of the 21st International Conference on\\\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\\\\u2013440. ACL, July\\\\n2006.\\\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859, 2016.\\\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\\\n[32] Noam Shazeer, Azalia Mirhoseini,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\\\nAdvances in Neural Information Processing Systems, 2015.\\\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144, 2016.\\\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. D...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#ff7f0e\",\"#999\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#999\",\"#999\",\"#999\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#999\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#aec7e8\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1290\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1291\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1286\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1287\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1288\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1257\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1272\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1273\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1274\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1275\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1276\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1281\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1282\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1267\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1268\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1269\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1270\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1262\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1263\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1264\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1265\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1266\",\"attributes\":{\"axis\":{\"id\":\"p1262\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1271\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1267\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1292\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1293\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Attention Mechanisms in Neural Networks\"},\"renderers\":[{\"id\":\"p1289\"}],\"index\":12}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1294\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1289\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1295\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Self-Attention Mechanism\"},\"renderers\":[{\"id\":\"p1289\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1296\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Learning\"},\"renderers\":[{\"id\":\"p1289\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1297\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Translation\"},\"renderers\":[{\"id\":\"p1289\"}],\"index\":27}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1367\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1303\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1312\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1313\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1314\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1315\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1310\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1342\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1299\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1300\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1301\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xypyQKQPbT+/loBAXjtkQCckPUCjf7FA4MEFQNNf6z+IZrBAvrk1QCUcpz51hmg/5917QDGfTUCIZrBA1m8RQBGRnUBEzo1AdDQ/QKKnl0Dn3XtAInq1P765NUBPGY1ASbmjQF47ZEBI/IZAJRynPtjqeEDgwQVAyjv4P7+WgEDTX+s/RM6NQDBwAECjf7FAInq1P9jqeEDtYw9AKsy1PrZ/5T90BYlAA9JgQCUcpz51hmg/Inq1PyckPUDbqKg/NP9KQODBBUA0/0pAdAWJQO1jD0Ca3ZhA1uNZP9UPBUDxxIRAhJCKQF47ZEDbqKg/\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LBDxQKMsF0FTqgZBt2HHQOjDDEHpHeNAhvEOQS9oIUFtlhJBGJzhQJRwAkH/Jw5BspvIQEZsz0BtlhJBg4UcQb9T60A0evxAJwQEQQD3CUGym8hA2VkGQRic4UBOib5ANQ7+QLdhx0D88N5AlHACQY1G20CG8Q5BYRTkQFOqBkEvaCFBNHr8QETHyEDpHeNA2VkGQY1G20AJPAFBnucTQVh47kCVbhRBcB4cQZRwAkH/Jw5B2VkGQejDDEGPK/ZAZRv5QIbxDkFlG/lAlW4UQQk8AUHYW9dAdV8CQYVhB0GBBStB4hEiQbdhx0CPK/ZA\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Learning\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Translation\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\"]],[\"correctness\",[true,true,true,true,true,true,true,true,true,false,true,true,true,false,true,true,true,true,true,false,true,true,false,false,true,false,true,false,true,false,true,true,true,true,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false]],[\"questions\",[\"What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\",\"What does the attention head behavior in the encoder self-attention at layer 5 of 6 indicate?\",\"What translation tasks did the model achieve a new state of the art on?\",\"What BLEU scores does the Transformer (big) model achieve for English-to-German and English-to-French translations?\",\"What new network architecture is proposed in the paper &#x27;Attention Is All You Need&#x27;?\",\"What is the size of the vocabulary used in the semi-supervised setting?\",\"How many parallel attention layers or heads are employed in the described model?\",\"What are the two most commonly used attention functions mentioned in the context?\",\"Who are the authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27;?\",\"What is the impact of using positional embedding instead of sinusoids according to Table 3?\",\"Under what circumstances might using self-attention in models provide an additional advantage, specifically in terms of model interpretability, and how does this relate to the tasks performed by individual attention heads?\",\"In the context of anaphora resolution, what specific function do the attention heads in the fifth of six layers appear to perform, particularly when analyzing the word &#x27;its&#x27;?\",\"Considering the hardware and hyperparameters used, what was the total duration required to fully train both the base models and the big models, and how does the step time differ between them?\",\"In the context of evaluating English-to-German translation performance, why is it significant to alter the base model in various ways, and what specific insights or outcomes does this variation aim to reveal in the study?\",\"Identify the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27;, ensuring to include all authors as listed in the provided context.\",\"In the context of attention mechanisms, particularly when dealing with large values of dk, why is it necessary to scale the dot product by 1/\\\\u221adk, considering the impact on the softmax function and gradient behavior?\",\"Under the semi-supervised setting for English constituency parsing, what F1 score did the Transformer model achieve, and how does it compare to the scores of other models listed in the same context?\",\"Considering the context, what is the primary benefit of using the Transformer model over other models like RNNs or convolutional layers specifically for translation tasks, and how does it impact training speed and performance?\",\"In the context of transduction models, what is the fundamental mechanism that the Transformer model uniquely utilizes, avoiding traditional sequence-aligned RNNs or convolution methods?\",\"Identify the individuals who were instrumental in both the design and implementation phases of tensor2tensor, and specify any affiliations they had during this work.\",\"Considering the auto-regressive nature of the model&#x27;s decoder, how long did it take to train the base models, and how many steps were involved in this process?\",\"Considering the use of multi-head attention in Transformers, why do self-attention layers exhibit faster computational complexity compared to recurrent layers, especially when dealing with large input sequences?\",\"In the context of the Transformer architecture that relies solely on attention mechanisms, how does reducing the attention key size dk impact the model quality, and are there any specific conditions under which this effect is observed?\",\"Given the constraint of using the Section 22 development set for hyperparameter tuning, what is the BLEU score achieved by the &#x27;big&#x27; Transformer model variation on the English-to-German translation task?\",\"Under the condition that the evaluation involves a comparison with neural machine translation architectures, what specific task was employed to assess whether the Transformer model can generalize to other tasks?\",\"Under the condition that proper attribution is provided, could you specify the BLEU scores achieved by the Transformer (big) model for both English-to-German and English-to-French translations?\",\"Under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, what BLEU score did it achieve on the WMT 2014 English-to-French translation task?\",\"Considering the ability of self-attention mechanisms to follow long-distance dependencies, as demonstrated in attention visualizations, what is a potential side benefit of using self-attention in models?\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, what is the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task?\",\"Considering the model&#x27;s decoder configuration with its unique sub-layer structure, how many parallel attention layers or heads are implemented in this model?\",\"Hi, I&#x27;m studying for my computer science exam and I&#x27;m trying to understand the role of masking in the decoder&#x27;s self-attention layers. Could you explain what its purpose is?\",\"As an AI researcher working on attention-based models for translating text to audio, I&#x27;m curious to know what tasks the model achieved a new state of the art in?\",\"Hi, I&#x27;m going through my lecture notes on neural networks and I&#x27;m a bit confused. Could you explain the main difference between dot-product attention and additive attention? I need to understand this for my upcoming exam.\",\"Hi, I&#x27;m diving into the latest research on neural network models for my graduate studies. Can you explain how the Transformer model performs compared to RNN sequence-to-sequence models and the Berkeley-Parser, especially in the context of translating English into German and French?\",\"Hi, I&#x27;m trying to understand a neural network architecture from a research paper I&#x27;m studying late at night. Could you tell me what the dimensionality of the input and output is in the FFN described in the document?\",\"Hello, as I&#x27;m working on improving sentence parsing accuracy in machine translation systems, could you tell me what the size of the vocabulary is in the semi-supervised setting?\",\"Hi, I&#x27;m reviewing my notes for an upcoming exam and I&#x27;m a bit confused. Why are self-attention layers considered faster than recurrent layers in terms of computational complexity?\",\"Hi, I&#x27;m working on my thesis about advancements in natural language processing, and I&#x27;m analyzing different machine translation models. Could you tell me what BLEU score the big transformer model achieved on the WMT 2014 English-to-German translation task?\",\"Hi, I&#x27;m a computer science student trying to grasp how self-attention mechanisms enhance tasks such as reading comprehension and learning sentence representations. Can you explain what self-attention is and in which tasks it has been used successfully?\",\"Hi, I&#x27;m working on a research paper about changes in voter registration laws and how they impact electoral processes. Could you explain what Figure 3 illustrates about the attention mechanism in the encoder?\",\"What is the purpose of the third sub-layer in the decoder and how does the decoder ensure that predictions for position i depend only on known outputs at positions less than i?\",\"Who explored the limits of language modeling in a 2016 preprint and what is the title of the work by Nal Kalchbrenner and colleagues related to neural machine translation?\",\"Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; and what is the title of the paper authored by Francois Chollet in 2016?\",\"How do separable convolutions affect computational complexity in neural networks and what is a potential benefit of using self-attention in models?\",\"What is the main point expressed about the law in the context and what does Figure 4 illustrate regarding attention heads in layer 5?\",\"How does the computational complexity of self-attention layers compare to recurrent layers and what kind of operations do recurrent layers require?\",\"What permission does Google grant regarding the reproduction of tables and figures in the paper &#x27;Attention Is All You Need&#x27;, and what is the main innovation proposed in the paper?\",\"Why did the researchers choose the sinusoidal version of positional embeddings and what are the three desiderata considered for using self-attention?\",\"What is the main architectural feature of the Transformer model proposed in the work and how does it compare to other models like Extended Neural GPU, ByteNet, and ConvS2S in terms of parallelization?\",\"What is the purpose of multi-head attention in the model described and how many parallel attention layers or heads are employed?\",\"What is the main method for achieving this?\",\"What is it?\",\"What is it and where has it been applied?\",\"Which one was utilized?\",\"What are they?\",\"What are they?\",\"Who are they?\",\"What is it?\",\"What are those scores?\",\"Why was that option selected?\"]],[\"agent_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, which is an improvement of over 2 BLEU compared to the existing best results, including ensembles[3].\",\"The attention head behavior in the encoder self-attention at layer 5 of 6 indicates that different attention heads have learned to perform different tasks, and their behavior seems related to the structure of the sentence. This suggests that the model is capable of capturing various aspects of the input sequence and focusing on specific parts of it when computing the representation of each word.\\\\n\\\\nIn the context provided, it is shown that different heads from the encoder self-attention at layer 5 exhibit distinct behaviors that are related to the sentence structure. This implies that the attention mechanism is effectively allowing the model to weigh the importance of different words in the sentence and to capture long-range dependencies and relationships within the input sequence[1][3].\\\\n\\\\nFor example, in the given context, the repetition of the sentence \\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\\\\\" with different attention heads suggests that each head is focusing on different aspects of the sentence, such as syntactic dependencies or semantic relationships. This behavior is indicative of the model&#x27;s ability to perform complex tasks like understanding sentence structure and context, which is crucial for natural language processing tasks.\",\"The Transformer model achieved a new state-of-the-art on two machine translation tasks:\\\\n\\\\n1. **WMT 2014 English-to-German translation task**: It achieved a BLEU score of 28.4, improving over the existing best results, including ensembles, by over 2 BLEU.\\\\n\\\\n2. **WMT 2014 English-to-French translation task**: It established a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs[3].\",\"Based on the provided context and additional information from the search results, the Transformer (big) model achieves the following BLEU scores:\\\\n\\\\n1. **English-to-French Translation**: The Transformer (big) model achieves a BLEU score of 41.0 for English-to-French translation, as mentioned in the provided context. However, it&#x27;s noted in [1] that the model actually establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs.\\\\n\\\\n2. **English-to-German Translation**: The BLEU score for English-to-German translation is not directly mentioned in the provided context but is found in [1]. According to [1], the Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\\\\n\\\\nTherefore, the Transformer (big) model achieves BLEU scores of 41.8 for English-to-French and 28.4 for English-to-German translations.\",\"The new network architecture proposed in the paper &#x27;Attention Is All You Need&#x27; is the Transformer model, which is based solely on attention mechanisms, dispensing with recurrence and convolutions. This architecture relies on self-attention mechanisms to process and generate sequences of data efficiently, capturing long-range dependencies and contextual relationships without the need for recurrent or convolutional neural networks[1][2][4].\",\"The size of the vocabulary used in the semi-supervised setting is 32K tokens[1][context].\",\"According to the provided context, the model employs **8 parallel attention layers or heads**. This is specified in the text as \\\\\"In this work we employ h = 8 parallel attention layers, or heads.\\\\\"[1][3].\",\"The two most commonly used attention functions mentioned in the context are:\\\\n\\\\n1. **Additive Attention**: This computes the compatibility function using a feed-forward network with a single hidden layer. It calculates attention scores by applying a feed-forward neural network to the concatenated query and key vectors[2].\\\\n\\\\n2. **Dot-Product (Multiplicative) Attention**: This computes attention scores using the dot product between the query and key vectors. Specifically, the Scaled Dot-Product Attention calculates the dot products of the query with all keys, divides each by the square root of the dimension of the query and key vectors, and applies a softmax function to obtain the weights on the values[2][4].\",\"The authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"According to the context provided, specifically looking at Table 3 row (E), the impact of using positional embedding instead of sinusoids is as follows:\\\\n\\\\n- The BLEU score is 4.92, which is slightly lower compared to the best setting mentioned earlier in the context.\\\\n- The perplexity is 25.7.\\\\n\\\\nHowever, without the specific details of the best setting&#x27;s BLEU score and perplexity for direct comparison within the provided context, it&#x27;s challenging to assess the full impact. Nonetheless, the mention of a 0.9 BLEU score drop for single-head attention suggests that the 4.92 BLEU score for positional embedding instead of sinusoids might also indicate a performance drop, albeit the exact comparison to the baseline isn&#x27;t fully detailed in the given snippet.\\\\n\\\\nFor a more comprehensive understanding, it would be necessary to compare these results directly with the baseline model&#x27;s performance metrics, which are not fully provided in the context snippet. \\\\n\\\\nHowever, based on the broader context from other sources like [4], it&#x27;s known that learned positional embeddings (like those possibly referred to in row (E)) can be more expressive but may not generalize as well to longer sequences not seen during training, compared to sinusoidal embeddings. This could imply that the choice between sinusoidal and learned positional embeddings depends on the specific task and dataset characteristics.\",\"Using self-attention in models can provide an additional advantage in terms of model interpretability under several circumstances, particularly when the model needs to capture complex dependencies and relationships within input sequences. Here are some key points:\\\\n\\\\n1. **Capturing Long-Range Dependencies**: Self-attention allows models to capture long-range dependencies within sequences, which is crucial for tasks such as natural language processing (NLP) and computer vision. By attending to different parts of the input sequence, self-attention helps models understand the context and assign appropriate weights to each element based on its relevance[1][3].\\\\n\\\\n2. **Interpretability**: The use of self-attention can enhance model interpretability by providing insights into how the model weighs the importance of different parts of the input sequence. This is particularly useful in NLP tasks where understanding the context and relationships between words is essential. For instance, in tasks like question answering and text summarization, self-attention can help identify which words or phrases are most relevant to the task at hand[3][5].\\\\n\\\\n3. **Individual Attention Heads**: In multi-head attention, each attention head can focus on different aspects of the input sequence, which can improve interpretability by showing how different parts of the sequence contribute to the overall understanding. For example, one attention head might focus on syntactic dependencies, while another might focus on semantic relationships[2][4].\\\\n\\\\n4. **Parallel Computation**: Self-attention can be computed in parallel for each element in the sequence, making it computationally efficient and scalable for large datasets. This parallelism can also contribute to better interpretability by allowing for the examination of how different parts of the sequence are processed simultaneously[1][4].\\\\n\\\\n5. **Task-Specific Insights**: In tasks like reading comprehension and textual entailment, self-attention can provide insights into how the model understands the relationships between different parts of the text. By analyzing the attention weights, researchers can gain a better understanding of how the model makes predictions and which parts of the text are most influential[3][4].\\\\n\\\\nIn summary, using self-attention in models can enhance interpretability by providing insights into how the model captures complex dependencies and relationships within input sequences. The use of individual attention heads in multi-head attention can further improve interpretability by showing how different parts of the sequence contribute to the overall understanding. This is particularly beneficial in NLP tasks where understanding the context and relationships between words is crucial.\",\"In the context of anaphora resolution, the attention heads in the fifth of six layers appear to perform a crucial function in identifying the antecedent of an anaphor. Specifically, when analyzing the word &#x27;its&#x27;, these attention heads exhibit sharp attentions that help in pinpointing the correct antecedent.\\\\n\\\\nAs illustrated in Figure 4, the top part shows the full attentions for head 5, while the bottom part isolates the attentions from just the word &#x27;its&#x27; for attention heads 5 and 6. The sharp attentions for the word &#x27;its&#x27; suggest that these attention heads are particularly focused on resolving the anaphora by linking &#x27;its&#x27; to its appropriate antecedent, which in this case is &#x27;The Law&#x27;.\\\\n\\\\nThis indicates that these attention heads are involved in a detailed analysis of the context to accurately determine the antecedent-anaphor relationship, which is a key aspect of anaphora resolution[5].\",\"To answer the question, let&#x27;s break down the information provided:\\\\n\\\\n1. **Base Models:**\\\\n   - **Training Time per Step:** 0.4 seconds\\\\n   - **Total Training Steps:** 100,000 steps\\\\n   - **Total Training Time:** 100,000 steps * 0.4 seconds/step = 40,000 seconds\\\\n   - **Total Training Time in Hours:** 40,000 seconds / 3600 seconds/hour = 12 hours\\\\n\\\\n2. **Big Models:**\\\\n   - **Training Time per Step:** 1.0 second\\\\n   - **Total Training Steps:** 300,000 steps\\\\n   - **Total Training Time:** 300,000 steps * 1.0 second/step = 300,000 seconds\\\\n   - **Total Training Time in Days:** 300,000 seconds / 86400 seconds/day = 3.47 days (approximately 3.5 days as mentioned)\\\\n\\\\n**Comparison:**\\\\n- **Step Time Difference:** The big models take 1.0 second per step, which is 2.5 times longer than the 0.4 seconds per step for the base models.\\\\n- **Total Training Duration:**\\\\n  - **Base Models:** 12 hours\\\\n  - **Big Models:** 3.5 days\\\\n\\\\nThus, the total duration required to fully train the base models was 12 hours, and for the big models, it was 3.5 days. The step time for the big models was 2.5 times longer than for the base models.\",\"In the context of evaluating English-to-German translation performance, altering the base model in various ways is significant because it allows researchers to understand the impact of different components on the overall translation quality. This variation aims to reveal several specific insights:\\\\n\\\\n1. **Component Importance**: By modifying different parts of the base model, researchers can identify which components are most crucial for achieving high-quality translations. This helps in understanding how each component contributes to the model&#x27;s performance.\\\\n\\\\n2. **Optimization Strategies**: Varying the model components can also help in identifying optimal configurations that balance performance and computational resources. This is particularly important for low-resource language pairs where data is scarce and computational efficiency is critical.\\\\n\\\\n3. **Generalizability**: Testing different model variations helps in assessing the generalizability of the findings. If certain modifications consistently improve performance across different settings, it suggests that these modifications are robust and can be applied to other translation tasks.\\\\n\\\\n4. **Comparison with Other Architectures**: By comparing the performance of various model variations with other architectures from the literature, researchers can evaluate the relative strengths and weaknesses of different approaches. This comparison provides a broader understanding of how different models perform under similar conditions.\\\\n\\\\nIn summary, altering the base model in various ways is a methodological approach to systematically investigate the effects of different components on translation performance. This helps in identifying optimal configurations, understanding the importance of each component, and comparing the performance of different models, ultimately leading to improved translation quality and efficiency.\",\"To identify the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27;, we need to look at the provided context for the relevant information.\\\\n\\\\nThe context mentions the paper in reference , which lists the authors as follows:\\\\n- Romain Paulus\\\\n- Caiming Xiong\\\\n- Richard Socher\\\\n\\\\nTherefore, the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"In the context of attention mechanisms, particularly when dealing with large values of \\\\\\\\(d_k\\\\\\\\), scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{d_k}\\\\\\\\) is necessary to counteract the effect of large dot products on the softmax function and its gradients.\\\\n\\\\n1. **Large Dot Products**: When \\\\\\\\(d_k\\\\\\\\) is large, the dot products of the query and key vectors (\\\\\\\\(q \\\\\\\\cdot k\\\\\\\\)) tend to grow large in magnitude. This is because the dot product of two vectors with independent random components (mean 0 and variance 1) results in a value with mean 0 and variance \\\\\\\\(d_k\\\\\\\\)[3]. As \\\\\\\\(d_k\\\\\\\\) increases, the variance of the dot product increases, leading to larger magnitudes.\\\\n\\\\n2. **Impact on Softmax Function**: The softmax function is sensitive to the scale of its input. When the input values (in this case, the dot products) are large, the softmax function pushes the output values to the extremes (very close to 0 or 1). This results in very small gradients for most of the input values, making it difficult for the model to learn and update the weights effectively[3].\\\\n\\\\n3. **Scaling the Dot Product**: To mitigate this issue, scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{d_k}\\\\\\\\) helps to reduce the magnitude of the dot products. This scaling factor normalizes the variance of the dot product to 1, regardless of the size of \\\\\\\\(d_k\\\\\\\\). This normalization ensures that the softmax function operates in a more stable region, where gradients are more evenly distributed and not extremely small. This facilitates better learning and updating of the model weights.\\\\n\\\\nIn summary, scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{d_k}\\\\\\\\) is necessary to prevent the softmax function from being pushed into regions with extremely small gradients due to large dot products, thereby improving the model&#x27;s ability to learn and update effectively.\",\"Under the semi-supervised setting for English constituency parsing, the Transformer model achieved an F1 score of 92.7. This score is competitive with other state-of-the-art models listed in the same context:\\\\n\\\\n- **McClosky et al. (2006) ** and **Vinyals &amp; Kaiser el al. (2014) ** both achieved an F1 score of 92.1, which is slightly lower than the Transformer model.\\\\n- **Luong et al. (2015) ** achieved an F1 score of 93.0, which is slightly higher than the Transformer model.\\\\n- **Dyer et al. (2016) ** achieved an F1 score of 93.3, which is higher than the Transformer model.\\\\n\\\\nThus, while the Transformer model performs well, it does not achieve the highest F1 score among the models listed in the semi-supervised setting. However, it is worth noting that the Transformer model&#x27;s performance is impressive given the lack of task-specific tuning mentioned in the context.\",\"The primary benefit of using the Transformer model over other models like RNNs or convolutional layers for translation tasks is its ability to handle dependencies between input and output sequences more effectively through the exclusive use of attention mechanisms. This approach dispenses with recurrence and convolutions entirely, leading to several key advantages:\\\\n\\\\n1. **Superior Quality**: The Transformer model demonstrates superior quality in machine translation tasks. For example, it achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over the existing best results by over 2 BLEU. Similarly, it establishes a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task[3].\\\\n\\\\n2. **Parallelizability**: The Transformer model is more parallelizable compared to traditional models. This is because it can process input sequences in parallel, rather than sequentially, which is a limitation of RNNs. This parallelization significantly reduces training time[3][4].\\\\n\\\\n3. **Training Speed**: The Transformer model requires significantly less time to train. For instance, it achieves state-of-the-art results on the WMT 2014 English-to-French translation task after training for just 3.5 days on eight GPUs, which is a small fraction of the training costs of the best models from the literature[3].\\\\n\\\\n4. **Generalizability**: The Transformer model generalizes well to other tasks beyond machine translation. It has been successfully applied to English constituency parsing, both with large and limited training data, demonstrating its versatility and effectiveness across different NLP tasks[3].\\\\n\\\\nIn summary, the Transformer model offers superior quality, better parallelizability, faster training speeds, and greater generalizability compared to traditional models like RNNs or convolutional layers, making it a preferred choice for translation tasks and other NLP applications.\",\"The fundamental mechanism that the Transformer model uniquely utilizes in the context of transduction models is **self-attention**. Unlike traditional sequence-aligned RNNs or convolution methods, the Transformer relies entirely on self-attention to compute representations of its input and output. This mechanism allows the model to weigh the importance of different parts of the input sequence differently, capturing dependencies regardless of their distance within the sequence, and enables parallel processing of the input data, making it highly efficient and effective for tasks involving sequential data[1][4][5].\",\"Based on the provided context, the individuals instrumental in both the design and implementation phases of tensor2tensor were:\\\\n\\\\n1. **Lukasz Kaiser** - He was affiliated with **Google Brain** during this work.\\\\n2. **Aidan** (full name not specified in the context) - The affiliation is not explicitly mentioned for Aidan in the provided context, but given the context and the mention of Google Brain and Google Research, it is reasonable to infer that Aidan was also affiliated with one of these entities, likely **Google Brain** or **Google Research**, during this work.\\\\n\\\\nThese individuals spent considerable time designing various parts of and implementing tensor2tensor, which significantly improved results and accelerated research.\",\"Based on the provided context, the training process for the base models involved 100,000 steps and took approximately 12 hours to complete. The auto-regressive nature of the model&#x27;s decoder is not directly related to the time and steps involved in training the base models, but rather it is a characteristic of how the model generates output. The training time and steps are determined by the computational resources (in this case, 8 NVIDIA P100 GPUs) and the specific hyperparameters used, including the optimizer settings and learning rate schedule. \\\\n\\\\nTo summarize:\\\\n- **Training Time for Base Models:** 12 hours\\\\n- **Number of Steps for Base Models:** 100,000 steps\",\"To understand why self-attention layers exhibit faster computational complexity compared to recurrent layers, especially when dealing with large input sequences, let&#x27;s break down the key points:\\\\n\\\\n1. **Computational Complexity of Recurrent Layers**: Recurrent Neural Networks (RNNs) process sequences sequentially, with each step depending on the previous one. This sequential nature means that RNNs require O(n) sequential operations for a sequence of length n, where each operation involves computing the hidden state based on the current input and the previous hidden state[2][4].\\\\n\\\\n2. **Computational Complexity of Self-Attention Layers**: Self-attention mechanisms, particularly in the context of Transformers and multi-head attention, compute attention weights by considering all positions in the input sequence simultaneously. The complexity of self-attention is primarily dominated by the computation of the attention scores and the subsequent weighted sum, which is O(n^2 * d_model) for a sequence of length n and a model dimensionality of d_model. However, this complexity is parallelizable, unlike the sequential operations in RNNs[1][3].\\\\n\\\\n3. **Parallelization Advantage**: The key advantage of self-attention layers is their ability to be parallelized. Since the computation of attention scores between different parts of the input sequence can be done independently, self-attention layers can leverage parallel computing architectures more effectively than RNNs. This parallelization significantly reduces the actual computational time for self-attention layers when dealing with large input sequences[1][5].\\\\n\\\\n4. **Scalability with Sequence Length**: For large input sequences, the O(n^2 * d_model) complexity of self-attention can be more efficient in practice than the O(n) sequential operations of RNNs because the parallelization benefits of self-attention outweigh the increased complexity. This is particularly true when the sequence length n is large, and the computational resources can handle the parallel operations efficiently.\\\\n\\\\nIn summary, self-attention layers exhibit faster computational complexity compared to recurrent layers for large input sequences because they can be parallelized, leveraging modern computing architectures to process all parts of the input sequence simultaneously. This parallelization advantage outweighs the higher theoretical complexity of self-attention, making it more efficient in practice for large sequences.\",\"The provided context does not directly address how reducing the attention key size \\\\\\\\(d_k\\\\\\\\) impacts the model quality in the Transformer architecture. However, understanding the role of \\\\\\\\(d_k\\\\\\\\) in the attention mechanism can provide insights into its potential effects.\\\\n\\\\nIn the Transformer architecture, the attention mechanism, specifically the scaled dot-product attention, is defined as:\\\\n\\\\\\\\[ \\\\\\\\text{Attention}(Q, K, V) = \\\\\\\\text{softmax}\\\\\\\\left(\\\\\\\\frac{QK^T}{\\\\\\\\sqrt{d_k}}\\\\\\\\right)V \\\\\\\\]\\\\nwhere \\\\\\\\(Q\\\\\\\\), \\\\\\\\(K\\\\\\\\), and \\\\\\\\(V\\\\\\\\) are the query, key, and value matrices, respectively, and \\\\\\\\(d_k\\\\\\\\) is the dimensionality of the key[2][4][5].\\\\n\\\\nReducing \\\\\\\\(d_k\\\\\\\\) would have several implications:\\\\n\\\\n1. **Computational Efficiency**: Smaller \\\\\\\\(d_k\\\\\\\\) reduces the computational cost of the attention mechanism, as the dot product \\\\\\\\(QK^T\\\\\\\\) and the subsequent scaling by \\\\\\\\(\\\\\\\\sqrt{d_k}\\\\\\\\) become less computationally intensive. This can be beneficial for large models and datasets.\\\\n\\\\n2. **Model Quality**: The impact on model quality is more nuanced. A smaller \\\\\\\\(d_k\\\\\\\\) might reduce the capacity of the model to capture complex dependencies between input elements. The attention mechanism relies on the dot product to measure similarity between queries and keys, and reducing the dimensionality could lead to less informative representations, potentially harming the model&#x27;s ability to learn long-range dependencies.\\\\n\\\\n3. **Specific Conditions**: The effect of reducing \\\\\\\\(d_k\\\\\\\\) on model quality might be more pronounced in tasks that require capturing long-range dependencies or in datasets with complex contextual relationships. For simpler tasks or datasets with shorter sequences, the impact might be less significant.\\\\n\\\\nIn summary, while the provided context does not explicitly discuss the effects of reducing \\\\\\\\(d_k\\\\\\\\) on model quality, understanding the role of \\\\\\\\(d_k\\\\\\\\) in the attention mechanism suggests that reducing it could lead to a trade-off between computational efficiency and model capacity to capture complex dependencies. The specific conditions under which this effect is observed would depend on the complexity of the task and the characteristics of the dataset.\",\"The BLEU score achieved by the &#x27;big&#x27; Transformer model variation on the English-to-German translation task is 28.4[1][context provided]. This score was obtained after training the model with specific configurations and hyperparameters, as detailed in the context, and it represents a new state-of-the-art at the time of the report. \\\\n\\\\nNote: The context provided does not explicitly mention using the Section 22 development set for hyperparameter tuning, but it does mention achieving a BLEU score of 28.4 on the WMT 2014 English-to-German translation task with the &#x27;big&#x27; Transformer model. The mention of a specific development set is not present in the provided context but is assumed based on the query. For clarity, the WMT 2014 dataset is referenced in the context, which includes various translation tasks and their respective datasets[2][5].\",\"To assess whether the Transformer model can generalize to other tasks beyond machine translation, the specific task employed was English constituency parsing. This task was chosen to evaluate the model&#x27;s ability to perform well on different tasks, both with large and limited training data[3][context].\",\"Based on the provided context and additional information from the search results:\\\\n\\\\n1. **English-to-French Translation**: The Transformer (big) model achieves a BLEU score of 41.0 on the WMT 2014 English-to-French translation task[1].\\\\n\\\\n2. **English-to-German Translation**: The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task[1].\\\\n\\\\nNote that the specific details about the \\\\\"big\\\\\" model variant for English-to-German are not explicitly mentioned in the provided context, but the general Transformer model&#x27;s performance is given as 28.4 BLEU. The distinction between \\\\\"base\\\\\" and \\\\\"big\\\\\" models is made clear for English-to-French but not specified for English-to-German in the provided context. However, the general Transformer model&#x27;s performance for English-to-German is provided in the broader context of the search results.\",\"Based on the provided context, the big model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task, with a training cost that is less than 1/4 of the previous state-of-the-art model, which implies it is under the specified condition of less than 2.5 \\\\u00b7 10^19 FLOPs[1]. However, the exact FLOPs count is not specified in the given context, but it is mentioned that the training cost is significantly reduced compared to the previous state-of-the-art model. \\\\n\\\\nTherefore, under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, it achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task.\",\"A potential side benefit of using self-attention in models is the ability to capture long-range dependencies efficiently. Unlike traditional models that struggle with learning dependencies between distant positions due to the linear or logarithmic increase in the number of operations required, self-attention reduces this to a constant number of operations. This is particularly beneficial in tasks such as natural language processing (NLP) and computer vision, where understanding the relationships between distant elements in a sequence is crucial[1][4].\\\\n\\\\nMoreover, self-attention allows models to focus on relevant information by selectively attending to different parts of the input sequence, which enhances contextual understanding and the ability to handle complex patterns. This capability is essential for achieving state-of-the-art results in various NLP tasks, including machine translation, text summarization, and question answering[1][3].\\\\n\\\\nAdditionally, self-attention mechanisms can be computed in parallel for each element in the sequence, making them computationally efficient and scalable for large datasets[1]. This parallel computation aspect is a significant advantage over traditional recurrent neural networks (RNNs), which process sequences sequentially and can be slower for long sequences.\\\\n\\\\nOverall, the use of self-attention in models not only improves their ability to capture long-range dependencies but also enhances their efficiency and scalability, making them more suitable for a wide range of applications in NLP and beyond.\",\"Based on the provided context, the big transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task. This information is directly stated in the context:\\\\n\\\\n\\\\\"On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.\\\\\"\\\\n\\\\nThis answer does not depend on any changes to the model&#x27;s parameters from the English-to-German base translation model, as it directly reports the performance of the big transformer model on the specified task.\",\"The provided context does not explicitly state the number of parallel attention layers or heads implemented in the model&#x27;s decoder configuration. However, based on the general architecture of Transformers and the description of multi-head attention in other sources[2][4], we can infer that the model likely uses multiple heads in its multi-head attention mechanism.\\\\n\\\\nIn the standard Transformer architecture, the multi-head attention mechanism typically consists of 8 heads[2][4]. Each head attends to the input sequence independently, and their outputs are concatenated and linearly transformed. However, the specific number of heads in the model described in the provided context is not mentioned.\\\\n\\\\nGiven the context&#x27;s focus on the decoder&#x27;s sub-layer structure and the use of multi-head attention over the output of the encoder stack, it is reasonable to assume that the model follows the conventional Transformer architecture, which often employs 8 heads. However, without explicit information, this remains an educated guess rather than a definitive answer. \\\\n\\\\nFor clarity and based on typical Transformer configurations:\\\\n- **Multi-head attention** in standard Transformer models usually involves **8 heads**.\\\\n- The context does not specify the number of heads but describes using multi-head attention, suggesting it aligns with standard practices.\\\\n\\\\nThus, while the context does not provide a direct answer, the conventional use of 8 heads in multi-head attention mechanisms offers a plausible inference.\",\"The masking in the decoder&#x27;s self-attention layers is a critical component that ensures the model generates output sequences in a way that respects the sequential nature of the data. Here&#x27;s a detailed explanation:\\\\n\\\\n1. **Preventing Future Information Leverage**: In sequence-to-sequence tasks, such as translation or text generation, the model should predict the next token based only on the previous tokens, not on future tokens. The masking mechanism in the decoder&#x27;s self-attention layers prevents the model from attending to subsequent positions in the output sequence. This means that when predicting a token at position \\\\\\\\(i\\\\\\\\), the model can only use information from tokens at positions less than \\\\\\\\(i\\\\\\\\), ensuring that the prediction does not depend on future tokens that have not been generated yet.\\\\n\\\\n2. **Offset by One Position**: The output embeddings are offset by one position, which aligns with the masking mechanism. This offset ensures that the predictions for position \\\\\\\\(i\\\\\\\\) are based on the known outputs at positions less than \\\\\\\\(i\\\\\\\\), reinforcing the sequential generation process.\\\\n\\\\n3. **Sequential Dependency**: By combining masking with the offset, the decoder maintains the sequential dependency that is crucial for tasks like translation or text generation. This setup allows the model to generate output sequences one token at a time, using only the information from previously generated tokens.\\\\n\\\\nIn summary, the masking in the decoder&#x27;s self-attention layers is essential for ensuring that the model adheres to the sequential nature of the output data, preventing it from using future information that would not be available in real-time sequence generation tasks[4].\",\"Based on the provided context, the Transformer model achieved a new state-of-the-art in two machine translation tasks:\\\\n\\\\n1. **English-to-German Translation Task**: The model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\\\\n\\\\n2. **English-to-French Translation Task**: The model established a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task after training for 3.5 days on eight GPUs.\\\\n\\\\nAdditionally, the Transformer model was successfully applied to English constituency parsing, both with large and limited training data, demonstrating its generalizability to other tasks[3].\",\"The main difference between dot-product attention and additive attention lies in how they compute the attention weights:\\\\n\\\\n1. **Dot-Product Attention**:\\\\n   - **Calculation**: The attention weights are calculated by taking the dot product of the query matrix \\\\\\\\(Q\\\\\\\\) and the key matrix \\\\\\\\(K\\\\\\\\), then dividing by the square root of the dimension of the key matrix \\\\\\\\(d_k\\\\\\\\). This is followed by applying a softmax function to obtain the weights. The formula is:\\\\n     \\\\\\\\[\\\\n     \\\\\\\\text{Attention}(Q, K, V) = \\\\\\\\text{softmax}\\\\\\\\left(\\\\\\\\frac{QK^T}{\\\\\\\\sqrt{d_k}}\\\\\\\\right)V\\\\n     \\\\\\\\]\\\\n   - **Purpose**: This scaling helps keep the dot product manageable and ensures numerical stability. It&#x27;s widely used in transformer architectures because it can be efficiently implemented using optimized matrix multiplication[1][3].\\\\n\\\\n2. **Additive Attention**:\\\\n   - **Calculation**: Instead of using a dot product, additive attention computes the similarity between the query and key vectors by passing them through a feed-forward neural network. This network produces a scalar value that represents the attention weight. The process involves concatenating the query and key vectors, passing them through a fully connected layer, and then applying a non-linear activation function (like tanh) to produce the attention weights[1][4].\\\\n   - **Advantage**: Additive attention allows the model to learn a non-linear similarity measure between the query and key vectors, which can be beneficial in certain scenarios, especially when the dot product similarity measure is not appropriate[1].\\\\n\\\\nIn summary, dot-product attention uses a scaled dot product to compute attention weights, while additive attention uses a feed-forward neural network to compute these weights. This difference in computation method is the key distinction between the two types of attention mechanisms.\",\"The Transformer model has demonstrated superior performance compared to RNN sequence-to-sequence models and the Berkeley-Parser in various translation tasks. Here are the key points:\\\\n\\\\n1. **Performance Comparison**:\\\\n   - The Transformer model outperforms RNN sequence-to-sequence models and the Berkeley-Parser, even when trained only on a limited dataset such as the WSJ training set of 40K sentences[1][4].\\\\n   - Specifically, in the context of translating English into German and French, the Transformer achieves a new state of the art on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks[1].\\\\n\\\\n2. **Training Speed**:\\\\n   - The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. This is because the Transformer uses multi-headed self-attention, which allows for parallel processing of sequences, unlike RNNs which process data sequentially[1][5].\\\\n\\\\n3. **Key Advantages**:\\\\n   - The Transformer&#x27;s self-attention mechanism enables it to capture relationships between distant words in a sequence, which is crucial for understanding context in NLP tasks[1][4][5].\\\\n   - Unlike RNNs, which struggle with long-term dependencies due to the vanishing gradient problem, the Transformer can handle long sequences more efficiently[1][5].\\\\n\\\\n4. **State-of-the-Art Results**:\\\\n   - The Transformer has achieved state-of-the-art results in various NLP benchmarks, including language translation, text classification, and summarization[1][4][5].\\\\n\\\\nIn summary, the Transformer model offers significant advantages over RNN sequence-to-sequence models and the Berkeley-Parser in terms of performance, training speed, and handling long sequences, making it a preferred choice for many NLP tasks, including translation tasks like English-to-German and English-to-French.\",\"The provided context does not specify the dimensionality of the input and output for the Feed-Forward Network (FFN) explicitly. However, based on the general description of neural network layers and their computational complexities, we can infer some general principles but not the specific dimensions for the FFN in question.\\\\n\\\\nTo understand the dimensionality of the input and output for the FFN in your research paper, you would need to look at the specific details provided in the paper itself, such as the architecture diagram or the text describing the FFN&#x27;s configuration. Typically, FFNs are described with input dimensions (e.g., the number of features or the embedding size) and output dimensions (e.g., the number of classes for classification tasks or the output embedding size).\\\\n\\\\nIf you can provide more context or details from the paper, I might be able to help you better. However, based on the provided information, it&#x27;s not possible to determine the exact dimensionality of the input and output for the FFN. \\\\n\\\\nFor general understanding:\\\\n- **Input Dimensionality**: This refers to the size of the input vector that the FFN accepts. For example, if you&#x27;re processing sequences of words and each word is represented by an embedding vector of size 512, then the input dimensionality for the FFN would be 512.\\\\n- **Output Dimensionality**: This is the size of the output vector produced by the FFN. For classification tasks, this might be the number of classes. For other tasks, it could be another embedding size, depending on the application.\\\\n\\\\nPlease refer to the specific sections of your research paper that describe the FFN architecture for these details.\",\"In the semi-supervised setting, the vocabulary size used is 32K tokens[1][context provided]. This is in contrast to the WSJ only setting, which uses a vocabulary of 16K tokens.\",\"To understand why self-attention layers are considered faster than recurrent layers in terms of computational complexity, let&#x27;s break down the key points:\\\\n\\\\n1. **Sequential Operations**: Recurrent layers, such as those in RNNs, require \\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\) sequential operations because each step depends on the previous one. This sequential dependency makes parallel processing challenging[2][4].\\\\n\\\\n2. **Computational Complexity**: The computational complexity of recurrent layers is \\\\\\\\(\\\\\\\\mathcal{O}(nd^2)\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the sequence length and \\\\\\\\(d\\\\\\\\) is the dimension of the input[1][3].\\\\n\\\\n3. **Self-Attention Layers**: Self-attention layers, on the other hand, have a computational complexity of \\\\\\\\(\\\\\\\\mathcal{O}(n^2d)\\\\\\\\), but they can be parallelized with \\\\\\\\(\\\\\\\\mathcal{O}(1)\\\\\\\\) sequential operations. This means that despite the quadratic complexity with respect to the sequence length, the ability to parallelize operations makes self-attention layers faster for many tasks, especially when dealing with shorter sequences[1][3].\\\\n\\\\n4. **Parallelization Advantage**: The key advantage of self-attention layers is their ability to parallelize computations, which significantly accelerates training and inference times compared to the sequential nature of recurrent layers[1][5].\\\\n\\\\nIn summary, while self-attention layers have a higher computational complexity in terms of \\\\\\\\(n\\\\\\\\) and \\\\\\\\(d\\\\\\\\), their ability to parallelize operations (\\\\\\\\(\\\\\\\\mathcal{O}(1)\\\\\\\\) sequential operations) makes them faster than recurrent layers (\\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\) sequential operations) for many applications, particularly in natural language processing tasks.\",\"According to the provided context, the Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, which is an improvement of over 2 BLEU compared to the existing best results, including ensembles[3].\",\"Self-attention, also known as intra-attention, is a mechanism used in machine learning and natural language processing (NLP) to relate different positions of a single sequence in order to compute a representation of that sequence. It allows a model to attend to different parts of the input sequence and weigh their importance relative to each other, capturing dependencies between different elements in the sequence.\\\\n\\\\nHere\\\\u2019s a step-by-step breakdown of how self-attention works:\\\\n\\\\n1. **Transformation into Vectors**: The input sequence is transformed into three vectors: query, key, and value. These vectors are obtained through linear transformations of the input[1][4].\\\\n\\\\n2. **Attention Calculation**: The attention mechanism calculates a weighted sum of the values based on the similarity between the query and key vectors. This process allows the model to focus on relevant information and capture long-range dependencies[1][4].\\\\n\\\\n3. **Feed-Forward Network**: The resulting weighted sum, along with the original input, is then passed through a feed-forward neural network to produce the final output[1][2].\\\\n\\\\nSelf-attention has been used successfully in various NLP tasks, including:\\\\n\\\\n- **Reading Comprehension**: It helps models understand complex texts by focusing on relevant parts of the input sequence.\\\\n- **Abstractive Summarization**: Self-attention aids in summarizing long texts by identifying key sentences and their relationships.\\\\n- **Textual Entailment**: It assists in determining whether one sentence implies another by analyzing the dependencies between different parts of the sentences.\\\\n- **Learning Task-Independent Sentence Representations**: Self-attention enables models to learn generalizable representations of sentences that can be used across different tasks[4][3].\\\\n\\\\nAdditionally, self-attention is a key component of the Transformer model, which has achieved state-of-the-art results in various NLP tasks, including machine translation and question answering[4][3].\\\\n\\\\nIn summary, self-attention is a powerful technique that enhances the ability of models to capture complex patterns and dependencies in sequential data, making it instrumental in various NLP tasks.\",\"Figure 3 illustrates how the attention mechanism in the encoder self-attention layer (specifically layer 5 of 6) handles long-distance dependencies in natural language processing tasks. Here&#x27;s a detailed explanation:\\\\n\\\\n1. **Contextual Understanding**: The figure shows an example sentence discussing changes in voter registration laws, specifically how American governments have made the registration or voting process more difficult since 2009.\\\\n\\\\n2. **Attention Mechanism**: The attention mechanism is a critical component of transformer models that allows the model to focus on different parts of the input sequence simultaneously and weigh their importance. In this case, the figure highlights how the attention mechanism captures long-distance dependencies.\\\\n\\\\n3. **Long-Distance Dependency**: The specific example provided focuses on the verb \\\\\"making\\\\\" and how it relates to the phrase \\\\\"making...more difficult.\\\\\" This is a long-distance dependency because the words \\\\\"making\\\\\" and \\\\\"more difficult\\\\\" are not adjacent in the sentence but are crucial for understanding the sentence&#x27;s meaning.\\\\n\\\\n4. **Attention Heads**: The figure shows that many of the attention heads (represented by different colors) attend to this distant dependency. This means that the model is able to recognize and emphasize the relationship between \\\\\"making\\\\\" and \\\\\"more difficult\\\\\" across the sentence, despite the intervening words.\\\\n\\\\n5. **Encoder Self-Attention**: The encoder self-attention layer is where this process occurs. It allows the model to compute attention scores that determine the relevance of each token (word) to every other token in the sequence, enabling the model to capture long-range dependencies effectively.\\\\n\\\\nIn summary, Figure 3 demonstrates how the attention mechanism in the encoder self-attention layer of a transformer model effectively captures long-distance dependencies in natural language processing tasks, such as understanding the relationship between \\\\\"making\\\\\" and \\\\\"more difficult\\\\\" in a sentence about voter registration laws[3].\",\"The third sub-layer in the decoder performs multi-head attention over the output of the encoder stack. This sub-layer is crucial for allowing the decoder to focus on different parts of the input sequence (encoded by the encoder) when generating each output token.\\\\n\\\\nTo ensure that predictions for position \\\\\\\\(i\\\\\\\\) depend only on known outputs at positions less than \\\\\\\\(i\\\\\\\\), the decoder employs two key mechanisms:\\\\n\\\\n1. **Masking in the Self-Attention Sub-Layer**: The self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions. This means that when the decoder is generating the output for position \\\\\\\\(i\\\\\\\\), it cannot look at the outputs for positions \\\\\\\\(i+1\\\\\\\\), \\\\\\\\(i+2\\\\\\\\), etc. This masking ensures that the decoder can only use information from previous positions to make predictions.\\\\n\\\\n2. **Offsetting Output Embeddings by One Position**: The output embeddings are offset by one position. This means that the prediction for position \\\\\\\\(i\\\\\\\\) is made based on the known outputs up to position \\\\\\\\(i-1\\\\\\\\). This offset, combined with the masking, ensures that the decoder can only rely on previously generated outputs to make new predictions, thus maintaining the sequential dependency in the output sequence.\\\\n\\\\nThese mechanisms are critical for tasks like machine translation, where the decoder needs to generate output tokens sequentially, with each token potentially depending on the context provided by the previous tokens[4].\",\"To answer this question, we need to look at the provided context and identify the relevant information.\\\\n\\\\n1. **Who explored the limits of language modeling in a 2016 preprint?**\\\\n   - The individuals who explored the limits of language modeling in a 2016 preprint are Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. The preprint is titled \\\\\"Exploring the limits of language modeling\\\\\" and is available as arXiv:1602.02410.\\\\n\\\\n2. **What is the title of the work by Nal Kalchbrenner and colleagues related to neural machine translation?**\\\\n   - The work by Nal Kalchbrenner and colleagues is titled \\\\\"Neural machine translation in linear time\\\\\" and is available as arXiv preprint arXiv:1610.10099v2, published in 2017.\\\\n\\\\nTherefore, the detailed answer to the question is:\\\\n- **Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu** explored the limits of language modeling in a 2016 preprint titled \\\\\"Exploring the limits of language modeling.\\\\\"\\\\n- **Nal Kalchbrenner and colleagues** published a work related to neural machine translation titled \\\\\"Neural machine translation in linear time\\\\\" in 2017.\",\"To answer the question, we need to look at the provided context which includes references to various research papers.\\\\n\\\\n1. **Authors of &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27;:**\\\\n   - The authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. This information is found in reference [5].\\\\n\\\\n2. **Title of the paper authored by Francois Chollet in 2016:**\\\\n   - The title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions&#x27;. This information is found in reference .\\\\n\\\\nTherefore, the detailed answer is:\\\\n- **Authors of &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27;:** Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.\\\\n- **Title of the paper authored by Francois Chollet in 2016:** &#x27;Xception: Deep learning with depthwise separable convolutions&#x27;.\",\"Separable convolutions significantly decrease the computational complexity in neural networks. The complexity is reduced to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d^2), which is a considerable improvement over standard convolutional layers. This reduction makes separable convolutions more efficient, even when k = n, bringing their complexity in line with the combined complexity of a self-attention layer and a point-wise feed-forward layer.\\\\n\\\\nA potential benefit of using self-attention in models is that it can yield more interpretable models. Self-attention mechanisms allow for the inspection of attention distributions, which can provide insights into how the model processes information. For example, individual attention heads can learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences, making the model&#x27;s decision-making process more transparent and understandable[1][5].\",\"The main point expressed about the law in the context is that while the law itself may never be perfect, its application should be just. This is highlighted in the repeated statement: \\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\\\\\"\\\\n\\\\nFigure 4 illustrates the role of attention heads in layer 5 of a neural network model, specifically in the context of anaphora resolution. Anaphora resolution is the process of identifying the antecedent (the noun or phrase to which an anaphor refers) for pronouns or other anaphoric expressions in a text. In this case, the figure shows:\\\\n\\\\n1. **Full attentions for head 5**: This part of the figure displays the complete attention patterns for one of the attention heads in layer 5. Attention in neural networks is a mechanism that allows the model to focus on specific parts of the input data when processing each word or token. The full attentions show how the model distributes its attention across the entire input sequence.\\\\n\\\\n2. **Isolated attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5 and 6**: This part specifically highlights how the attention heads 5 and 6 focus on the word &#x27;its&#x27; and its potential antecedents. The sharp attentions indicate that the model is effectively identifying &#x27;The Law&#x27; as the antecedent for &#x27;its&#x27;, demonstrating the model&#x27;s ability to resolve anaphora.\\\\n\\\\nIn summary, the context emphasizes the importance of just application of the law, and Figure 4 demonstrates how attention mechanisms in a neural network can be used to resolve anaphora, specifically by focusing on the relationship between the pronoun &#x27;its&#x27; and its antecedent &#x27;The Law&#x27;.\",\"To answer this question, let&#x27;s break down the information provided and analyze it step by step:\\\\n\\\\n1. **Computational Complexity Comparison**:\\\\n   - The context states that self-attention layers are faster than recurrent layers when the sequence length is considered. This implies that the computational complexity of self-attention is less than that of recurrent layers under certain conditions.\\\\n\\\\n2. **Operations Required by Recurrent Layers**:\\\\n   - Recurrent layers require O(n) sequential operations. This means that the number of operations in recurrent layers grows linearly with the sequence length (n).\\\\n\\\\n3. **Self-Attention Layers**:\\\\n   - Although not explicitly stated in the provided context, self-attention layers generally have a computational complexity of O(n^2) due to the need to compute attention weights between all pairs of tokens in the input sequence[1][4]. However, the context suggests that despite this, self-attention can be faster than recurrent layers under specific conditions, likely due to parallelization capabilities.\\\\n\\\\n4. **Conclusion**:\\\\n   - Recurrent layers require O(n) sequential operations, making their computational complexity linear with respect to the sequence length.\\\\n   - Self-attention layers, while having a higher theoretical complexity of O(n^2), can be faster in practice than recurrent layers for certain sequence lengths, likely due to the ability to parallelize operations in self-attention mechanisms.\\\\n\\\\nIn summary, while recurrent layers have a linear computational complexity with respect to sequence length (O(n)), self-attention layers have a quadratic complexity (O(n^2)) but can be faster in practice for certain scenarios due to parallelization.\",\"Google grants permission to reproduce the tables and figures in the paper \\\\\"Attention Is All You Need\\\\\" solely for use in journalistic or scholarly works, provided proper attribution is given.\\\\n\\\\nThe main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms. This architecture dispenses with recurrence and convolutions, unlike traditional sequence transduction models that rely on complex recurrent or convolutional neural networks with an encoder and a decoder connected through an attention mechanism. The Transformer model focuses on using self-attention mechanisms to process and generate sequences of data efficiently, capturing long-range dependencies and contextual relationships without the need for sequential processing.\",\"The researchers chose the sinusoidal version of positional embeddings because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. This is in contrast to learned positional embeddings, which, despite producing nearly identical results in their experiments, do not inherently offer this extrapolation capability.\\\\n\\\\nThe three desiderata considered for using self-attention are:\\\\n\\\\n1. **Total computational complexity per layer**: This refers to the overall computational cost associated with processing a layer. Self-attention mechanisms are considered in the context of their computational efficiency compared to recurrent and convolutional layers.\\\\n\\\\n2. **Amount of computation that can be parallelized**: This is measured by the minimum number of sequential operations required. Self-attention allows for more parallelization compared to recurrent layers, which process sequences sequentially and thus have more sequential operations.\\\\n\\\\n3. **Minimum number of sequential operations required**: This directly relates to the parallelization aspect, emphasizing the importance of minimizing sequential dependencies to take full advantage of parallel computing capabilities. Self-attention mechanisms can process all positions in the input sequence simultaneously and weigh their importance, which aligns with this desideratum.\",\"The main architectural feature of the Transformer model proposed in the work is its reliance entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence and convolutional neural networks[3][4].\\\\n\\\\nCompared to other models like Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks as their basic building blocks, the Transformer model offers significantly more parallelization. The Transformer can process input and output positions in parallel without the need for sequential computation, which is a limitation in models like ConvS2S and ByteNet. In those models, the number of operations required to relate signals from two arbitrary input or output positions grows with the distance between positions (linearly for ConvS2S and logarithmically for ByteNet). This limitation is overcome in the Transformer model, allowing it to achieve state-of-the-art translation quality with much less training time, such as reaching new state-of-the-art results after being trained for as little as twelve hours on eight P100 GPUs[3].\",\"The purpose of multi-head attention in the model described is to allow the model to jointly attend to information from different representation subspaces at different positions. This is achieved by running multiple attention mechanisms in parallel, each focusing on different aspects of the input data. This approach enables the model to capture a richer set of relationships and nuances in the data compared to using a single attention head.\\\\n\\\\nIn the model described, 8 parallel attention layers or heads are employed. Each head operates on a lower-dimensional representation of the input data (dk = dv = dmodel/h = 64), which helps maintain a similar total computational cost compared to using a single-head attention with full dimensionality[3][context]. This setup allows the model to learn different patterns and dependencies in the data, leading to more comprehensive and accurate representations.\",\"The main method for achieving the ability to focus on relevant parts of the input sequence, as described in the context, is through the use of the **attention mechanism**. Specifically, this involves computing pairwise similarities between all tokens in the input sequence, which allows the model to dynamically adjust the importance of certain words based on the relevance of the current context. This is particularly useful for capturing long-range dependencies and resolving ambiguities in language.\\\\n\\\\nIn the example provided in the context, the attention mechanism is illustrated in Figure 3, where it shows how different attention heads in the encoder self-attention layer focus on distant dependencies, such as the verb &#x27;making&#x27; and its completion &#x27;more difficult&#x27;. This demonstrates how the attention mechanism enables the model to understand complex sentences by dynamically focusing on relevant parts of the input sequence[1][3][5].\",\"The context provided does not directly answer the question \\\\\"What is it?\\\\\" as it seems to be a snippet from a technical document discussing neural network layers and attention mechanisms, specifically in the context of anaphora resolution. However, based on the information given and the topic of anaphora resolution, we can infer that \\\\\"it\\\\\" likely refers to \\\\\"the Law\\\\\" mentioned in the input sequences. This inference is based on the typical function of anaphora resolution, which involves identifying the antecedent (in this case, \\\\\"the Law\\\\\") that a pronoun or anaphor (\\\\\"its\\\\\") refers to.\\\\n\\\\nIn the context of anaphora resolution as discussed in the provided sources[1][2], the process involves identifying the correct antecedent for an anaphor (like \\\\\"its\\\\\") by considering various constraints and preferences, such as syntactic agreement, semantic consistency, and discourse structure. Given the sentence structure and the use of \\\\\"its\\\\\" in the provided input sequences, \\\\\"the Law\\\\\" is the most logical antecedent.\\\\n\\\\nTherefore, based on the context and the principles of anaphora resolution, \\\\\"it\\\\\" refers to \\\\\"the Law.\\\\\"\",\"Based on the provided context, \\\\\"it\\\\\" refers to the attention mechanism in neural networks, specifically in the context of anaphora resolution. Anaphora resolution is a task in Natural Language Processing (NLP) that involves identifying the antecedent (the noun or phrase to which a pronoun refers) in a given text.\\\\n\\\\nThe context provided shows a figure illustrating how attention heads in a neural network layer are involved in anaphora resolution. The figure highlights the sharp attentions for the word &#x27;its&#x27;, indicating how the network focuses on specific parts of the input to resolve anaphora.\\\\n\\\\nAnaphora resolution has been applied in various NLP tasks, including but not limited to:\\\\n\\\\n1. **Question Answering**: As discussed in [4], anaphora resolution is crucial for improving the performance of question answering systems by accurately identifying the references of pronouns and other referring expressions.\\\\n\\\\n2. **Code-Mixed Social Media Text**: Research such as [5] explores the application of anaphora resolution in code-mixed social media data, specifically for the Hindi language, using deep learning-based encoder-decoder models with attention.\\\\n\\\\n3. **General NLP Tasks**: Anaphora resolution is a fundamental component of natural language understanding and has been applied in various NLP tasks to enhance the comprehension of discursive connections and logical relationships within texts[1][2].\\\\n\\\\nIn summary, \\\\\"it\\\\\" refers to the attention mechanism used in anaphora resolution, and this technique has been applied in various NLP tasks including question answering, code-mixed social media text analysis, and general NLP tasks to improve language understanding.\",\"Based on the provided context, the Penn Treebank with about 40K training sentences was utilized, along with additional semi-supervised settings that included the larger high-confidence and BerkeleyParser corpora with approximately 17M sentences. \\\\n\\\\n1. **Penn Treebank**: This was the primary dataset used, containing about 40K training sentences.\\\\n2. **Semi-supervised setting**: This involved using larger corpora, including the high-confidence and BerkeleyParser corpora, which together contained approximately 17M sentences. \\\\n\\\\nTwo different vocabularies were used:\\\\n- **WSJ only setting**: A vocabulary of 16K tokens.\\\\n- **Semi-supervised setting**: A vocabulary of 32K tokens. \\\\n\\\\nThis indicates that both the Penn Treebank and the larger semi-supervised corpora were utilized in the experiments.\",\"Based on the provided context, the question \\\\\"What are they?\\\\\" seems to refer to the examples of attention heads in the encoder self-attention at layer 5 of 6, as mentioned in Figure 5. These attention heads are mechanisms within the Transformer architecture that allow the model to focus on different parts of the input sequence when processing it.\\\\n\\\\nSpecifically, in the context of the Transformer model, attention heads are components of the multi-head attention mechanism. This mechanism splits the input sequence into multiple representations (Query, Key, and Value) and processes these representations in parallel through different attention heads. Each head computes attention scores by comparing the Query and Key vectors, which helps the model to weigh the importance of different parts of the input sequence[1][2].\\\\n\\\\nIn the given example, the attention heads at layer 5 of 6 in the encoder self-attention have learned to perform different tasks, indicating that they are capturing different aspects of the input sequence&#x27;s structure. This allows the model to encode richer interpretations of the sequence by considering multiple relationships and nuances[1]. \\\\n\\\\nThus, \\\\\"they\\\\\" are examples of attention heads that have learned to focus on different structural aspects of the input sentence.\",\"The context provided seems to be discussing the behavior of attention heads in a transformer model, specifically in the encoder self-attention at layer 5 of 6. The figure mentioned (Figure 5) illustrates how different attention heads learn to perform different tasks, focusing on various aspects of the sentence structure.\\\\n\\\\nGiven this context, the question \\\\\"What are they?\\\\\" likely refers to the attention heads and their specific behaviors or tasks. However, without a direct reference to what \\\\\"they\\\\\" are in the provided text, we can infer based on the discussion about attention heads.\\\\n\\\\n1. **Attention Heads**: In the context of transformer models, attention heads are components of the multi-head attention mechanism. This mechanism allows the model to focus on different parts of the input sequence simultaneously and weigh their importance. Each attention head can learn to attend to different aspects of the input data, such as syntactic or semantic features[1][2].\\\\n\\\\n2. **Specific Behaviors or Tasks**: The attention heads in the encoder self-attention learn to perform tasks that are related to the structure of the sentence. For example, some heads might focus on the subject-verb relationship, while others might attend to the modifier-noun relationship or the overall sentence context[1][4].\\\\n\\\\nIn summary, \\\\\"they\\\\\" likely refer to the specific behaviors or tasks that different attention heads learn to perform in the context of processing input sequences in a transformer model.\",\"The context provided does not specify who \\\\\"they\\\\\" are. The text appears to be discussing the concept of law and its application, and then shifts to describing the behavior of attention heads in a neural network model, specifically in the context of the Transformer architecture. There is no clear reference to any specific individuals or groups that \\\\\"they\\\\\" could refer to. \\\\n\\\\nGiven the information, it seems that \\\\\"they\\\\\" might be a pronoun used in a general sense or could be referring to the attention heads themselves, but without more context, it&#x27;s impossible to provide a definitive answer. \\\\n\\\\nHowever, considering the broader discussion about the Transformer model and its attention mechanisms in the provided search results[1][3][5], it&#x27;s clear that the focus is on the model&#x27;s components and their functionalities rather than on specific individuals or entities.\",\"To answer the question \\\\\"What is it?\\\\\" based on the provided context, we need to understand the anaphora resolution process. Anaphora resolution is the process of identifying the antecedent (the noun or phrase to which a pronoun refers) for a given anaphor (the pronoun itself)[1][2].\\\\n\\\\nIn the provided context, the sentence is: \\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\\\\\"\\\\n\\\\nHere, \\\\\"its\\\\\" is an anaphor. To resolve it, we need to identify the antecedent to which \\\\\"its\\\\\" refers. In this sentence, \\\\\"its\\\\\" clearly refers to \\\\\"The Law\\\\\" because \\\\\"its\\\\\" is a possessive pronoun indicating that the application belongs to or is associated with \\\\\"The Law\\\\\".\\\\n\\\\nTherefore, the answer to \\\\\"What is it?\\\\\" in this context is \\\\\"The Law\\\\\". \\\\n\\\\nThis resolution process aligns with the principles outlined in the literature on anaphora resolution, which emphasize the importance of syntactic, semantic, and pragmatic information in determining the correct antecedent for an anaphor[1][2]. The attention mechanisms in neural networks, as discussed in the context of transformer architectures, also play a crucial role in focusing on specific parts of the input data to make such determinations[2][5].\",\"The context provided does not directly mention specific scores but seems to be discussing parameters and settings for a machine translation model. However, based on the broader context of the search results provided, it appears you might be asking about BLEU scores, which are a measure of the quality of machine translations compared to human translations. Here&#x27;s a general explanation of BLEU scores:\\\\n\\\\n- **BLEU Scores**: BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-translated text. It measures the similarity between the machine-translated text and a set of high-quality reference translations. The score ranges from 0 to 1, with higher scores indicating better translations. Often, BLEU scores are expressed on a scale of 1 to 100 for easier communication[1][2][4].\\\\n\\\\n- **Interpretation of BLEU Scores**:\\\\n  - **30-40**: Understandable to good translations.\\\\n  - **40-50**: High-quality translations.\\\\n  - **50-60**: Very high-quality, adequate, and fluent translations.\\\\n  - **&gt;60**: Quality often better than human[4].\\\\n\\\\nHowever, the specific scores you are asking about are not mentioned in the provided context. If you are looking for the results of a specific experiment or model evaluation, please provide more details or clarify the question.\",\"The option was selected to illustrate how the self-attention mechanism in transformer models effectively handles long-range dependencies in natural language processing tasks. Specifically, the example provided in the context demonstrates how the attention mechanism in the encoder self-attention layer 5 captures the distant dependency between the verb \\\\\"making\\\\\" and the phrase \\\\\"more difficult,\\\\\" which are separated by several tokens.\\\\n\\\\n1. **Contextual Understanding**: The context provided includes a sentence discussing how a majority of American governments have passed new laws since 2009 that make the registration or voting process more difficult. This sentence is used to illustrate the attention mechanism&#x27;s ability to follow long-distance dependencies.\\\\n\\\\n2. **Attention Mechanism**: The self-attention mechanism allows the model to compute dependencies between any two tokens in the sequence, regardless of their distance. This is crucial for understanding the relationship between \\\\\"making\\\\\" and \\\\\"more difficult\\\\\" in the given sentence.\\\\n\\\\n3. **Visualization**: Figure 3 in the context visualizes how different attention heads in the self-attention mechanism focus on the word \\\\\"making\\\\\" and its dependency on \\\\\"more difficult.\\\\\" This visualization helps in understanding how the model captures long-range dependencies.\\\\n\\\\n4. **Relevance to NLP**: The example highlights the importance of the self-attention mechanism in natural language processing tasks, particularly in handling complex sentences with distant dependencies. This is a key feature that distinguishes transformer models from traditional RNNs and LSTMs, which often struggle with long-range dependencies due to their sequential processing and vanishing gradient problems[2].\\\\n\\\\nIn summary, the option was selected to demonstrate the effectiveness of the self-attention mechanism in capturing long-range dependencies, which is a critical aspect of natural language processing tasks.\"]],[\"reference_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Many of the attention heads exhibit behavior that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The paper proposes a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Using positional embedding instead of sinusoids results in a BLEU score of 4.92 compared to 25.7.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The attention heads in layer 5 of 6 appear to be involved in anaphora resolution.\",\"The base models were trained for a total of 100,000 steps or 12 hours. The big models were trained for 300,000 steps (3.5 days).\",\"To evaluate the importance of different components of the Transformer, measuring the change in performance on English-to-German translation.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"The dot products are scaled by 1/\\\\u221adk to counteract the effect of large values of dk, which cause the dot products to grow large in magnitude and push the softmax function into regions where it has extremely small gradients.\",\"The Transformer achieved an F1 score of 92.7 in the semi-supervised setting.\",\"The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\",\"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\",\"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor.\",\"The base models were trained for a total of 100,000 steps or 12 hours.\",\"Self-attention layers are faster than recurrent layers because they require O(1) sequential operations, whereas a recurrent layer requires O(n) sequential operations.\",\"Reducing the attention key size dk hurts model quality.\",\"The BLEU score for the &#x27;big&#x27; Transformer model variation is 26.4.\",\"The task used to evaluate if the Transformer can generalize to other tasks was English constituency parsing.\",\"The Transformer (big) model achieves BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The big model achieved a BLEU score of 41.0.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The purpose of masking in the decoder&#x27;s self-attention layers is to prevent leftward information flow in the decoder to preserve the auto-regressive property by masking out (setting to \\\\u2212\\\\u221e) all values in the input of the softmax which correspond to illegal connections.\",\"The model achieved a new state of the art in the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"Dot-product attention includes a scaling factor of 1/\\\\u221adk, whereas additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\",\"The Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences, and it performs surprisingly well compared to RNN sequence-to-sequence models.\",\"The dimensionality of input and output is dmodel = 512.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"Self-attention layers are considered faster than recurrent layers in terms of computational complexity because they require O(1) executed operations, whereas a recurrent layer requires O(n) sequential operations.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"Figure 3 illustrates the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6, where many of the attention heads attend to a distant dependency of the verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019.\",\"The third sub-layer in the decoder performs multi-head attention over the output of the encoder stack, and the decoder ensures that predictions for position i depend only on known outputs at positions less than i by modifying the self-attention sub-layer to prevent positions from attending to subsequent positions, and the output embeddings are offset by one position.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu explored the limits of language modeling in a 2016 arXiv preprint, and the title of the work by Nal Kalchbrenner and colleagues is &#x27;Neural machine translation in linear time.&#x27;\",\"The authors of the paper &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, and the title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions.&#x27;\",\"Separable convolutions decrease the complexity considerably to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2), even though with k = n, the complexity is equal to the combination of a self-attention layer and a point-wise feed-forward layer; a side benefit of self-attention is that it could yield more interpretable models, as individual attention heads learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences.\",\"The main point expressed is that the law will never be perfect, but its application should be just, which is considered to be missing; Figure 4 illustrates two attention heads in layer 5 of 6, involved in anaphora resolution, showing full attentions for head 5 and isolated attentions from the word &#x27;its&#x27; for attention heads 5 and 6, with very sharp attentions noted for this word.\",\"Self-attention layers are faster than recurrent layers in terms of computational complexity when the sequence is considered, and recurrent layers require O(n) sequential operations.\",\"Google grants permission to reproduce the tables and figures in the paper solely for use in journalistic or scholarly works, provided proper attribution is given, and the main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The researchers chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training, and the three desiderata considered for using self-attention are the total computational complexity per layer, the amount of computation that can be parallelized, and the minimum number of sequential operations required.\",\"The main architectural feature of the Transformer model is that it eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization compared to models like Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks and have operations that grow with the distance between input or output positions.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, and in this work, 8 parallel attention layers or heads are employed.\",\"The Transformer model relies entirely on an attention mechanism to draw global dependencies between input and output.\",\"Structured attention networks.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"The standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\",\"The three factors mentioned are: the total computational complexity per layer, the amount of computation that can be parallelized (measured by the minimum number of sequential operations required), and the path length between long-range dependencies in the network.\",\"The Transformer uses multi-head attention in three different ways: 1) In &#x27;encoder-decoder attention&#x27; layers, where queries come from the previous decoder layer and the memory keys and values come from the output of the encoder. 2) In encoder self-attention layers, where all keys, values, and queries come from the output of the previous layer in the encoder. 3) In decoder self-attention layers, allowing each position in the decoder to attend to all positions in the decoder up to and including that position.\",\"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\",\"Neural machine translation by jointly learning to align and translate.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The sinusoidal version was chosen because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"]],[\"id\",[1,51,39,29,0,36,16,14,45,34,26,50,28,32,45,15,37,38,8,3,28,24,34,33,35,29,31,26,30,16,18,39,14,38,19,36,24,30,7,49,11,43,41,26,50,24,0,22,6,16,6,43,7,27,23,17,47,40,29,22]],[\"content\",[\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\"]],[\"color\",[\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1343\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1344\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1339\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1340\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1341\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1357\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1348\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1349\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1350\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1358\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1359\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1354\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1355\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1356\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1311\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1326\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1327\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1328\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1329\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1334\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1335\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1302\",\"attributes\":{\"renderers\":[{\"id\":\"p1342\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1321\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1322\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1323\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1324\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1316\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1317\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1318\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1319\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1320\",\"attributes\":{\"axis\":{\"id\":\"p1316\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1325\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1321\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1345\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1346\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1342\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1347\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1342\"}],\"index\":0}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1363\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1360\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1361\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1362\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"MsVdQNX8hkCz0L0/P0X4Pw==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JQ4OQWx710AVgvJANvYVQQ==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"452e0beb-aae7-4777-b28e-67f350c5cd93\",\"roots\":{\"p1368\":\"e19a2b94-994c-4713-8859-b62772dac290\"},\"root_ids\":[\"p1368\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"e19a2b94-994c-4713-8859-b62772dac290\" data-root-id=\"p1368\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x17816890ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.8\n",
       "conversational               0.1\n",
       "distracting element          0.5\n",
       "double                       1.0\n",
       "simple                       0.9\n",
       "situational                  0.9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 60/60 [07:15<00:00,  7.26s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 60/60 [00:52<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "def answer_fn(query, history=None):\n",
    "    retrieved_results = vectorstore.similarity_search(query)\n",
    "    context = retrieved_results[0].page_content\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant trained to answer based on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(context=context, input=query)},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-sonar-large-128k-online\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "from giskard.rag import evaluate\n",
    "report = evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ba27febb-b0be-408d-b69e-221ddc450e98\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"ba27febb-b0be-408d-b69e-221ddc450e98\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ba27febb-b0be-408d-b69e-221ddc450e98\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        86.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=86.00000000000001 class=\" progress-green \">86.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        80.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=80.0 class=\" progress-green \">80.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        60.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=60.0 class=\" progress-orange \">60.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        79.17%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=79.16666666666666 class=\" progress-green \">79.17%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">73%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">Focus on improving the conversational question handling by enhancing the rewriter component, as it shows the lowest score of 0.1, which is significantly impacting overall performance. Additionally, consider improving the knowledge base coverage on &#39;Others&#39; and &#39;Transformer Model in Machine Translation&#39; topics to achieve more balanced topic performance.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"dde62b45-c292-4649-b97a-2090c1b3248d\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1682\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1684\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1691\",\"attributes\":{\"factors\":[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1692\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1693\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1689\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1711\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1679\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1680\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1681\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"27Zt27ZtVUCqqqqqqqpQQAAAAAAAwFJAAAAAAADgVUAAAAAAAIBRQA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]],[\"colors\",[\"#158f4b\",\"#a50026\",\"#fee08b\",\"#006837\",\"#e75236\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1712\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1713\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1708\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1709\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1710\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1720\",\"attributes\":{\"data_source\":{\"id\":\"p1679\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1721\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1722\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1717\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1718\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1719\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1730\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1724\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1725\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1726\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1731\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1732\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1727\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1728\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1729\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1690\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1704\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1699\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1700\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1701\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1702\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1694\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1695\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1696\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1697\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1698\",\"attributes\":{\"axis\":{\"id\":\"p1694\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1703\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1699\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1723\",\"attributes\":{\"location\":73.33333333333333,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1733\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1734\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1730\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"dde62b45-c292-4649-b97a-2090c1b3248d\",\"roots\":{\"p1682\":\"da5c14fc-8a62-4594-8658-66c546392e9a\"},\"root_ids\":[\"p1682\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"da5c14fc-8a62-4594-8658-66c546392e9a\" data-root-id=\"p1682\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"81f471ba-3d94-44ef-951e-04b3fb96fcdd\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1613\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1543\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1494\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1503\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1504\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1505\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1506\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1501\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1534\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1491\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1492\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1493\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Others\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"length n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npa...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"and interpretable tree annotation. In Proceedings of the 21st International Conference on\\\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\\\\u2013440. ACL, July\\\\n2006.\\\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859, 2016.\\\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\\\n[32] Noam Shazeer, Azalia Mirhoseini,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\\\nAdvances in Neural Information Processing Systems, 2015.\\\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144, 2016.\\\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. D...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#ff7f0e\",\"#999\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#999\",\"#999\",\"#999\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#999\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#aec7e8\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1535\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1536\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1531\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1532\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1533\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1502\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1517\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1518\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1519\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1520\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1521\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1526\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1527\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1512\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1513\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1514\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1515\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1507\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1508\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1509\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1510\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1511\",\"attributes\":{\"axis\":{\"id\":\"p1507\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1516\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1512\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1537\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1538\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Attention Mechanisms in Neural Networks\"},\"renderers\":[{\"id\":\"p1534\"}],\"index\":12}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1539\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1534\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1540\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Self-Attention Mechanism\"},\"renderers\":[{\"id\":\"p1534\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1541\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Learning\"},\"renderers\":[{\"id\":\"p1534\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1542\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Translation\"},\"renderers\":[{\"id\":\"p1534\"}],\"index\":27}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1612\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1548\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1557\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1558\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1559\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1560\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1555\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1587\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1544\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1545\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1546\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xypyQKQPbT+/loBAXjtkQCckPUCjf7FA4MEFQNNf6z+IZrBAvrk1QCUcpz51hmg/5917QDGfTUCIZrBA1m8RQBGRnUBEzo1AdDQ/QKKnl0Dn3XtAInq1P765NUBPGY1ASbmjQF47ZEBI/IZAJRynPtjqeEDgwQVAyjv4P7+WgEDTX+s/RM6NQDBwAECjf7FAInq1P9jqeEDtYw9AKsy1PrZ/5T90BYlAA9JgQCUcpz51hmg/Inq1PyckPUDbqKg/NP9KQODBBUA0/0pAdAWJQO1jD0Ca3ZhA1uNZP9UPBUDxxIRAhJCKQF47ZEDbqKg/\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LBDxQKMsF0FTqgZBt2HHQOjDDEHpHeNAhvEOQS9oIUFtlhJBGJzhQJRwAkH/Jw5BspvIQEZsz0BtlhJBg4UcQb9T60A0evxAJwQEQQD3CUGym8hA2VkGQRic4UBOib5ANQ7+QLdhx0D88N5AlHACQY1G20CG8Q5BYRTkQFOqBkEvaCFBNHr8QETHyEDpHeNA2VkGQY1G20AJPAFBnucTQVh47kCVbhRBcB4cQZRwAkH/Jw5B2VkGQejDDEGPK/ZAZRv5QIbxDkFlG/lAlW4UQQk8AUHYW9dAdV8CQYVhB0GBBStB4hEiQbdhx0CPK/ZA\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Learning\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Translation\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\"]],[\"correctness\",[true,true,true,true,true,true,true,true,true,false,true,true,true,false,true,true,true,true,true,false,true,true,true,false,true,false,true,false,true,true,true,true,true,true,false,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,false,false,false,false,false,false,false,false,false]],[\"questions\",[\"What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\",\"What does the attention head behavior in the encoder self-attention at layer 5 of 6 indicate?\",\"What translation tasks did the model achieve a new state of the art on?\",\"What BLEU scores does the Transformer (big) model achieve for English-to-German and English-to-French translations?\",\"What new network architecture is proposed in the paper &#x27;Attention Is All You Need&#x27;?\",\"What is the size of the vocabulary used in the semi-supervised setting?\",\"How many parallel attention layers or heads are employed in the described model?\",\"What are the two most commonly used attention functions mentioned in the context?\",\"Who are the authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27;?\",\"What is the impact of using positional embedding instead of sinusoids according to Table 3?\",\"Under what circumstances might using self-attention in models provide an additional advantage, specifically in terms of model interpretability, and how does this relate to the tasks performed by individual attention heads?\",\"In the context of anaphora resolution, what specific function do the attention heads in the fifth of six layers appear to perform, particularly when analyzing the word &#x27;its&#x27;?\",\"Considering the hardware and hyperparameters used, what was the total duration required to fully train both the base models and the big models, and how does the step time differ between them?\",\"In the context of evaluating English-to-German translation performance, why is it significant to alter the base model in various ways, and what specific insights or outcomes does this variation aim to reveal in the study?\",\"Identify the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27;, ensuring to include all authors as listed in the provided context.\",\"In the context of attention mechanisms, particularly when dealing with large values of dk, why is it necessary to scale the dot product by 1/\\\\u221adk, considering the impact on the softmax function and gradient behavior?\",\"Under the semi-supervised setting for English constituency parsing, what F1 score did the Transformer model achieve, and how does it compare to the scores of other models listed in the same context?\",\"Considering the context, what is the primary benefit of using the Transformer model over other models like RNNs or convolutional layers specifically for translation tasks, and how does it impact training speed and performance?\",\"In the context of transduction models, what is the fundamental mechanism that the Transformer model uniquely utilizes, avoiding traditional sequence-aligned RNNs or convolution methods?\",\"Identify the individuals who were instrumental in both the design and implementation phases of tensor2tensor, and specify any affiliations they had during this work.\",\"Considering the auto-regressive nature of the model&#x27;s decoder, how long did it take to train the base models, and how many steps were involved in this process?\",\"Considering the use of multi-head attention in Transformers, why do self-attention layers exhibit faster computational complexity compared to recurrent layers, especially when dealing with large input sequences?\",\"In the context of the Transformer architecture that relies solely on attention mechanisms, how does reducing the attention key size dk impact the model quality, and are there any specific conditions under which this effect is observed?\",\"Given the constraint of using the Section 22 development set for hyperparameter tuning, what is the BLEU score achieved by the &#x27;big&#x27; Transformer model variation on the English-to-German translation task?\",\"Under the condition that the evaluation involves a comparison with neural machine translation architectures, what specific task was employed to assess whether the Transformer model can generalize to other tasks?\",\"Under the condition that proper attribution is provided, could you specify the BLEU scores achieved by the Transformer (big) model for both English-to-German and English-to-French translations?\",\"Under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, what BLEU score did it achieve on the WMT 2014 English-to-French translation task?\",\"Considering the ability of self-attention mechanisms to follow long-distance dependencies, as demonstrated in attention visualizations, what is a potential side benefit of using self-attention in models?\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, what is the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task?\",\"Considering the model&#x27;s decoder configuration with its unique sub-layer structure, how many parallel attention layers or heads are implemented in this model?\",\"Hi, I&#x27;m studying for my computer science exam and I&#x27;m trying to understand the role of masking in the decoder&#x27;s self-attention layers. Could you explain what its purpose is?\",\"As an AI researcher working on attention-based models for translating text to audio, I&#x27;m curious to know what tasks the model achieved a new state of the art in?\",\"Hi, I&#x27;m going through my lecture notes on neural networks and I&#x27;m a bit confused. Could you explain the main difference between dot-product attention and additive attention? I need to understand this for my upcoming exam.\",\"Hi, I&#x27;m diving into the latest research on neural network models for my graduate studies. Can you explain how the Transformer model performs compared to RNN sequence-to-sequence models and the Berkeley-Parser, especially in the context of translating English into German and French?\",\"Hi, I&#x27;m trying to understand a neural network architecture from a research paper I&#x27;m studying late at night. Could you tell me what the dimensionality of the input and output is in the FFN described in the document?\",\"Hello, as I&#x27;m working on improving sentence parsing accuracy in machine translation systems, could you tell me what the size of the vocabulary is in the semi-supervised setting?\",\"Hi, I&#x27;m reviewing my notes for an upcoming exam and I&#x27;m a bit confused. Why are self-attention layers considered faster than recurrent layers in terms of computational complexity?\",\"Hi, I&#x27;m working on my thesis about advancements in natural language processing, and I&#x27;m analyzing different machine translation models. Could you tell me what BLEU score the big transformer model achieved on the WMT 2014 English-to-German translation task?\",\"Hi, I&#x27;m a computer science student trying to grasp how self-attention mechanisms enhance tasks such as reading comprehension and learning sentence representations. Can you explain what self-attention is and in which tasks it has been used successfully?\",\"Hi, I&#x27;m working on a research paper about changes in voter registration laws and how they impact electoral processes. Could you explain what Figure 3 illustrates about the attention mechanism in the encoder?\",\"What is the purpose of the third sub-layer in the decoder and how does the decoder ensure that predictions for position i depend only on known outputs at positions less than i?\",\"Who explored the limits of language modeling in a 2016 preprint and what is the title of the work by Nal Kalchbrenner and colleagues related to neural machine translation?\",\"Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; and what is the title of the paper authored by Francois Chollet in 2016?\",\"How do separable convolutions affect computational complexity in neural networks and what is a potential benefit of using self-attention in models?\",\"What is the main point expressed about the law in the context and what does Figure 4 illustrate regarding attention heads in layer 5?\",\"How does the computational complexity of self-attention layers compare to recurrent layers and what kind of operations do recurrent layers require?\",\"What permission does Google grant regarding the reproduction of tables and figures in the paper &#x27;Attention Is All You Need&#x27;, and what is the main innovation proposed in the paper?\",\"Why did the researchers choose the sinusoidal version of positional embeddings and what are the three desiderata considered for using self-attention?\",\"What is the main architectural feature of the Transformer model proposed in the work and how does it compare to other models like Extended Neural GPU, ByteNet, and ConvS2S in terms of parallelization?\",\"What is the purpose of multi-head attention in the model described and how many parallel attention layers or heads are employed?\",\"What is the main method for achieving this?\",\"What is it?\",\"What is it and where has it been applied?\",\"Which one was utilized?\",\"What are they?\",\"What are they?\",\"Who are they?\",\"What is it?\",\"What are those scores?\",\"Why was that option selected?\"]],[\"agent_answer\",[\"The Transformer model achieved a BLEU score of **28.4** on the WMT 2014 English-to-German translation task. This score represents an improvement over the existing best results, including ensembles, by more than 2 BLEU points[3].\",\"The behavior of the attention heads in the encoder self-attention at layer 5 of 6 in a Transformer model indicates several key aspects of how the model processes and understands the input sequence:\\\\n\\\\n## Different Tasks Learned by Heads\\\\nEach attention head in the multi-head attention mechanism learns to focus on different aspects or structures of the input sequence. This is evident because different heads exhibit behavior that seems related to the structure of the sentence, such as focusing on different grammatical elements, semantic relationships, or contextual dependencies within the sentence[1][3][5].\\\\n\\\\n## Specialization of Heads\\\\nThe fact that heads from the same layer (in this case, layer 5 of 6) can perform distinct tasks suggests that the model is able to specialize each head to capture various types of information. For example, one head might focus on subject-verb relationships, while another might focus on noun phrases or contextual dependencies between clauses.\\\\n\\\\n## Capture of Long-Range Dependencies\\\\nThe self-attention mechanism, particularly in the encoder, allows the model to capture long-range dependencies within the input sequence. This means that each word in the sequence can attend to any other word, regardless of their position, which is crucial for understanding complex sentences and relationships that span across the sequence[1][3].\\\\n\\\\n## Contextual Understanding\\\\nThe attention scores produced by these heads help in creating a contextual representation of each word in the input sequence. This representation incorporates the influence of other words in the sequence, enhancing the model&#x27;s ability to understand the sentence as a whole and make more informed decisions during the encoding process[1][3].\\\\n\\\\nIn summary, the behavior of attention heads at layer 5 of 6 indicates that the model is effectively utilizing multi-head attention to specialize in different aspects of the input sequence, capture long-range dependencies, and build a rich contextual understanding of the input.\",\"The Transformer model achieved new state-of-the-art results on the following machine translation tasks:\\\\n\\\\n## WMT 2014 English-to-German Translation Task\\\\n- The model achieved a BLEU score of 28.4, improving over the existing best results, including ensembles, by over 2 BLEU points[3].\\\\n\\\\n## WMT 2014 English-to-French Translation Task\\\\n- The model established a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs[3].\",\"Based on the provided context and additional information from the sources, here are the BLEU scores achieved by the Transformer (big) model for the specified translation tasks:\\\\n\\\\n## English-to-French Translation\\\\nThe Transformer (big) model achieves a BLEU score of 41.8 on the WMT 2014 English-to-French translation task, as mentioned in the context and further detailed in the source[1].\\\\n\\\\n## English-to-German Translation\\\\nFor the WMT 2014 English-to-German translation task, the Transformer (big) model achieves a BLEU score of 28.4, which is also noted in the source[1]. This score improves over the existing best results, including ensembles, by over 2 BLEU points.\",\"The paper \\\\\"Attention Is All You Need\\\\\" by Ashish Vaswani et al. proposes a new network architecture called the **Transformer**. This architecture is distinct because it is based solely on attention mechanisms, eliminating the need for recurrence and convolutions that are typically found in recurrent neural networks (RNNs) or convolutional neural networks.\\\\n\\\\nKey features of the Transformer architecture include:\\\\n\\\\n- An **encoder-decoder structure**: Both the encoder and decoder consist of stacks of identical layers that utilize self-attention mechanisms and feed-forward neural networks[2][4][5].\\\\n- **Multi-head self-attention mechanism**: This allows the model to weigh the importance of different parts of the input sequence, capturing dependencies regardless of their distance within the sequence[1][4][5].\\\\n- **Position-wise feed-forward neural networks**: These networks process the output of the self-attention mechanism to generate refined representations[1][4][5].\\\\n- **Positional encoding**: Additional values or vectors are added to the token embeddings to encode position information[1][4][5].\\\\n\\\\nThis architecture enables the model to process input data in parallel, making it highly efficient and effective for sequence-to-sequence tasks such as machine translation.\",\"In the semi-supervised setting described, the size of the vocabulary used is 32K tokens[1].\",\"In the described model, \\\\\\\\( h = 8 \\\\\\\\) parallel attention layers, or heads, are employed. This means the model uses 8 attention heads in its multi-head attention mechanism.\",\"In the context of attention mechanisms, the two most commonly used attention functions are:\\\\n\\\\n## Dot-Product Attention (or Multiplicative Attention)\\\\nThis method computes attention scores by taking the dot product of the query and key vectors, then scaling the result by the square root of the dimension of the key vectors, and finally applying a softmax function to obtain the weights. This approach is efficient and can be implemented using optimized matrix multiplication code[2][4].\\\\n\\\\n## Additive Attention\\\\nThis method computes the compatibility function using a feed-forward neural network with a single hidden layer applied to the concatenated query and key vectors. While it has similar theoretical complexity to dot-product attention, it is generally slower and less space-efficient in practice compared to dot-product attention[2][3].\",\"The authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Based on the provided context, the impact of using positional embedding instead of sinusoids, as mentioned in row (E) of Table 3, is not explicitly detailed in the given text snippets. However, here are some general insights and potential implications that can be inferred from the broader context of positional embeddings:\\\\n\\\\n## General Impact of Positional Embeddings\\\\n- Positional embeddings, whether sinusoidal or other types, are crucial for Transformers to understand the sequence order of tokens since the model itself does not inherently capture this information[1][2][5].\\\\n\\\\n## Specific Context of Table 3\\\\n- The context provided does not give specific metrics or results for row (E) that directly compare the performance of using positional embeddings instead of sinusoids. It only mentions that in row (E), the positional embedding method was replaced, but it does not specify the type of positional embedding used or the exact impact on performance.\\\\n\\\\n## Inference\\\\n- Generally, replacing sinusoidal positional embeddings with other types of positional embeddings (such as learned positional embeddings or Rotary Positional Embedding) can have various effects:\\\\n  - **Learned Positional Embeddings**: These can be more expressive but introduce more trainable parameters, potentially increasing the model size and computational cost. They may perform better for specific tasks but lack extrapolation capabilities beyond the training sequence length[5].\\\\n  - **Rotary Positional Embedding (RoPE)**: This method uses a rotation matrix to encode positions and can handle longer sequences and complex positional relationships better than sinusoidal embeddings[2].\\\\n\\\\nWithout specific metrics or results from Table 3, it is not possible to determine the exact impact of using positional embedding instead of sinusoids in this particular case. However, the general understanding is that different positional embedding methods can have varying effects on model performance, extrapolation capabilities, and computational efficiency.\",\"Using self-attention in models can provide several advantages, including some benefits related to model interpretability, although interpretability is still a challenging aspect of these models. Here are some key points to consider:\\\\n\\\\n## Capturing Long-Range Dependencies\\\\nSelf-attention allows models to capture relationships between distant elements in a sequence, which is crucial for tasks like reading comprehension, abstractive summarization, and textual entailment. This capability to handle long-range dependencies can make the model&#x27;s behavior more interpretable because it explicitly weights the importance of different parts of the input sequence[1][4][5].\\\\n\\\\n## Contextual Understanding\\\\nBy attending to different parts of the input sequence, self-attention helps the model understand the context and assign appropriate weights to each element based on its relevance. This contextual understanding can provide insights into why the model is making certain predictions, as it highlights which parts of the input are most influential[1][3][5].\\\\n\\\\n## Multi-Head Attention\\\\nIn models like the Transformer, self-attention is often implemented using multi-head attention. Each attention head can focus on different aspects of the input sequence, allowing the model to capture a variety of relationships. Analyzing the attention weights from different heads can provide insights into what the model is focusing on and why, which can improve interpretability. For example, some heads might focus on syntactic relationships, while others might focus on semantic relationships[2][4].\\\\n\\\\n## Parallel Computation\\\\nWhile the parallel computation aspect of self-attention primarily benefits computational efficiency, it also means that the model can process and weigh all parts of the input sequence simultaneously. This can make it easier to analyze the attention weights and understand how the model is integrating information from different parts of the sequence[1][4].\\\\n\\\\n## Limitations and Challenges\\\\nDespite these advantages, self-attention models still have limitations in terms of interpretability. The complexity of the attention mechanisms and the large number of parameters involved can make it difficult to fully understand the model&#x27;s decision-making process. Additionally, the attention weights themselves may not always provide clear insights, especially in cases where the model is vulnerable to noise or where the input length is limited[3].\\\\n\\\\nIn summary, the use of self-attention can enhance model interpretability by:\\\\n\\\\n- Capturing long-range dependencies explicitly.\\\\n- Providing contextual understanding through weighted attention.\\\\n- Allowing analysis of different attention heads to understand various aspects of the input sequence.\\\\n\\\\nHowever, interpreting self-attention models remains challenging due to their complexity and the need for further research to improve their interpretability.\",\"In the context of anaphora resolution, the attention heads in the fifth of six layers appear to play a crucial role in identifying and resolving anaphoric references. Here are the key points:\\\\n\\\\n## Attention Mechanism\\\\nThe attention mechanism in neural networks, particularly in transformer-based models, helps the model focus on specific parts of the input that are relevant to the task at hand. In anaphora resolution, this is critical for identifying the antecedent of an anaphor.\\\\n\\\\n## Specific Function of Attention Heads\\\\n- When analyzing the word &#x27;its&#x27;, the attention heads in layer 5 seem to be involved in sharpening the focus on this word. The figure described shows that the attentions are very sharp for this word, indicating that the model is highlighting the importance of &#x27;its&#x27; in the context.\\\\n- This sharp attention suggests that the model is using the word &#x27;its&#x27; to establish a link between the anaphor and its antecedent. For example, in the sentence \\\\\"The Law will never be perfect, but its application should be just,\\\\\" the attention heads help in resolving &#x27;its&#x27; to &#x27;The Law&#x27;, which is the antecedent.\\\\n\\\\n## Anaphora Resolution\\\\n- The attention mechanism here aids in coreference resolution by identifying the correct antecedent for the anaphor &#x27;its&#x27;. This is essential for understanding the sentence correctly and maintaining coherence in the text.\\\\n\\\\nIn summary, the attention heads in the fifth layer are specifically involved in identifying and resolving anaphoric references by focusing sharply on words like &#x27;its&#x27; and linking them to their appropriate antecedents[5].\",\"To determine the total duration required to fully train both the base models and the big models, and to understand the difference in step time between them, we can analyze the provided information step by step.\\\\n\\\\n### Base Models\\\\n- **Step Time**: Each training step took about 0.4 seconds.\\\\n- **Total Steps**: The base models were trained for a total of 100,000 steps.\\\\n- **Total Training Time**: To calculate the total training time, we multiply the step time by the total number of steps.\\\\n  \\\\\\\\[\\\\n  0.4 \\\\\\\\text{ seconds/step} \\\\\\\\times 100,000 \\\\\\\\text{ steps} = 40,000 \\\\\\\\text{ seconds}\\\\n  \\\\\\\\]\\\\n  Converting seconds to hours:\\\\n  \\\\\\\\[\\\\n  40,000 \\\\\\\\text{ seconds} \\\\\\\\div 3600 \\\\\\\\text{ seconds/hour} = 11.11 \\\\\\\\text{ hours}\\\\n  \\\\\\\\]\\\\n  However, the context mentions that the training took about 12 hours, which might include additional overhead or other factors not accounted for in the step time alone.\\\\n\\\\n### Big Models\\\\n- **Step Time**: Each training step took about 1.0 second.\\\\n- **Total Steps**: The big models were trained for a total of 300,000 steps.\\\\n- **Total Training Time**: To calculate the total training time, we multiply the step time by the total number of steps.\\\\n  \\\\\\\\[\\\\n  1.0 \\\\\\\\text{ second/step} \\\\\\\\times 300,000 \\\\\\\\text{ steps} = 300,000 \\\\\\\\text{ seconds}\\\\n  \\\\\\\\]\\\\n  Converting seconds to hours:\\\\n  \\\\\\\\[\\\\n  300,000 \\\\\\\\text{ seconds} \\\\\\\\div 3600 \\\\\\\\text{ seconds/hour} = 83.33 \\\\\\\\text{ hours}\\\\n  \\\\\\\\]\\\\n  However, the context mentions that the training took about 3.5 days, which is approximately 84 hours. This slight discrepancy might be due to rounding or additional overhead.\\\\n\\\\n### Summary\\\\n- **Base Models**: Trained for approximately 12 hours.\\\\n- **Big Models**: Trained for approximately 3.5 days (or 84 hours).\\\\n\\\\n### Step Time Difference\\\\n- The step time for the base models was 0.4 seconds per step.\\\\n- The step time for the big models was 1.0 second per step.\\\\n- Therefore, the step time for the big models was 2.5 times longer than that of the base models.\\\\n\\\\nIn summary, the total duration to fully train both the base models and the big models was approximately 12 hours for the base models and 3.5 days (or 84 hours) for the big models. The step time for the big models was significantly longer, at 1.0 second per step compared to 0.4 seconds per step for the base models.\",\"In the context of evaluating English-to-German translation performance using the Transformer model, altering the base model in various ways is significant for several reasons:\\\\n\\\\n## Understanding Component Importance\\\\nBy varying different components of the base Transformer model, researchers can determine the importance and impact of each component on the overall translation performance. This includes modifications such as:\\\\n- Changing the number of layers: This helps in understanding how the depth of the model affects translation quality.\\\\n- Adjusting the number of attention heads: This reveals the optimal number of heads necessary for effective attention mechanisms.\\\\n- Testing various regularization techniques: This evaluates how different regularization methods (e.g., dropout, weight decay) influence the model&#x27;s performance and generalization.\\\\n- Evaluating different subword models: For example, using SentencePiece models with unigram or BPE (Byte Pair Encoding) approaches to see which subword model leads to better translation performance[1][3].\\\\n\\\\n## Performance Optimization\\\\nVariations in the model architecture aim to optimize the performance of the Transformer model. By identifying which components or configurations lead to better results, researchers can fine-tune the model to achieve higher translation quality. For instance, the study on English-Irish translation showed that the correct choice of subword model was the biggest driver of translation performance, leading to significant improvements in BLEU scores and other metrics[1][3].\\\\n\\\\n## Efficiency and Resource Utilization\\\\nEvaluating different model variations also helps in assessing the training costs and efficiency. This includes estimating the number of floating-point operations required to train the model, which is crucial for understanding the computational resources needed. By comparing the performance and training costs of different model variations, researchers can identify the most efficient configurations that balance performance and resource utilization[5].\\\\n\\\\n## Generalizability and Robustness\\\\nTesting various components and configurations helps in determining the robustness and generalizability of the model. This ensures that the model is not overly specialized to a particular setup but can perform well across different conditions and datasets. For example, comparing the model&#x27;s performance on different corpora (e.g., generic vs. in-domain corpora) can provide insights into its adaptability[1][3].\\\\n\\\\nIn summary, varying the base Transformer model in different ways allows researchers to understand the importance of each component, optimize performance, evaluate efficiency, and assess the model&#x27;s generalizability and robustness. These insights are crucial for developing and refining effective machine translation systems.\",\"The research paper entitled \\\\\"A deep reinforced model for abstractive summarization\\\\\" was written by the following authors, as listed in the provided context:\\\\n\\\\n- Romain Paulus\\\\n- Caiming Xiong\\\\n- Richard Socher.\",\"In the context of attention mechanisms, particularly those using dot-product attention, scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\) is crucial when dealing with large values of \\\\\\\\(dk\\\\\\\\), where \\\\\\\\(dk\\\\\\\\) represents the dimensionality of the keys (and queries).\\\\n\\\\nHere are the key reasons for this scaling:\\\\n\\\\n## Dot Product Magnitude\\\\nWhen computing the dot product of two vectors \\\\\\\\(q\\\\\\\\) and \\\\\\\\(k\\\\\\\\), each with dimension \\\\\\\\(dk\\\\\\\\), the magnitude of the dot product can become very large. This is because the dot product \\\\\\\\(q \\\\\\\\cdot k = \\\\\\\\sum_{i=1}^{dk} q_i k_i\\\\\\\\) has a variance that scales with \\\\\\\\(dk\\\\\\\\). Specifically, if the components of \\\\\\\\(q\\\\\\\\) and \\\\\\\\(k\\\\\\\\) are independent random variables with mean 0 and variance 1, the variance of the dot product is \\\\\\\\(dk\\\\\\\\)[3].\\\\n\\\\n## Impact on Softmax Function\\\\nThe softmax function is used to normalize the attention scores, which are typically the results of these dot products. However, when the dot products are large, they can push the softmax function into regions where the gradients become extremely small. This is because the softmax function has an exponential form, and large input values can lead to very small gradients due to the saturation of the exponential function. This can hinder the training process, as small gradients can lead to vanishing gradient problems during backpropagation[3].\\\\n\\\\n## Scaling to Mitigate Issues\\\\nTo counteract the issue of large dot product magnitudes and their impact on the softmax function, the dot products are scaled by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\). This scaling has several beneficial effects:\\\\n\\\\n- **Reduces Magnitude**: Scaling by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\) reduces the magnitude of the dot products, preventing them from becoming too large.\\\\n- **Stabilizes Gradients**: By reducing the magnitude, the softmax function operates in a region where the gradients are more stable and not excessively small, which helps in maintaining healthy gradient flow during training.\\\\n- **Improves Numerical Stability**: This scaling ensures that the attention scores are more numerically stable, which is crucial for reliable and efficient training of the model.\\\\n\\\\nIn summary, scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\) is necessary to prevent large dot product magnitudes from causing issues with the softmax function and gradient behavior, thereby ensuring more stable and effective training of attention-based models.\",\"In the context provided, under the semi-supervised setting for English constituency parsing on Section 23 of the Wall Street Journal (WSJ), here are the relevant details:\\\\n\\\\n- The Transformer model (with 4 layers) achieved an F1 score of **92.7**[Context].\\\\n\\\\nComparing this to other models listed in the same context:\\\\n\\\\n- **Zhu et al. (2013)** and **Vinyals &amp; Kaiser et al. (2014)** both achieved an F1 score of **91.3** in the semi-supervised setting[Context].\\\\n- **Huang &amp; Harper (2009)** and **McClosky et al. (2006)** also achieved an F1 score of **92.1** in the semi-supervised setting[Context].\\\\n\\\\nTherefore, the Transformer model with 4 layers outperforms the other semi-supervised models listed, achieving the highest F1 score of **92.7** in this setting.\",\"The primary benefits of using the Transformer model over other models like Recurrent Neural Networks (RNNs) or convolutional neural networks for translation tasks can be summarized as follows:\\\\n\\\\n## Attention Mechanism\\\\nThe Transformer model is based solely on attention mechanisms, which allows it to focus on different parts of the input sequence simultaneously. This approach eliminates the need for recurrence and convolutions entirely, making the model simpler and more efficient[3][4].\\\\n\\\\n## Parallelization\\\\nOne of the key advantages of the Transformer model is its ability to process input sequences in parallel, rather than sequentially. This parallelization capability significantly reduces the training time compared to RNNs, which process sequences word-by-word in order. The Transformer can train on entire texts all at once, making the training process much faster[3][4].\\\\n\\\\n## Handling Long-Range Dependencies\\\\nThe self-attention mechanism in Transformer models allows them to handle long-range dependencies in input sequences more effectively than RNNs. This is because self-attention can model relationships between all words in the sequence simultaneously, which is particularly beneficial for tasks like machine translation[1][3].\\\\n\\\\n## Performance\\\\nExperiments have shown that Transformer models achieve superior quality in translation tasks. For instance, on the WMT 2014 English-to-German and English-to-French translation tasks, the Transformer model achieved state-of-the-art BLEU scores, improving over existing best results by over 2 BLEU points in the case of English-to-German translation. This indicates a substantial improvement in translation quality[3].\\\\n\\\\n## Training Efficiency\\\\nThe Transformer model requires significantly less time to train compared to traditional models. For example, the model achieved a new single-model state-of-the-art BLEU score on the WMT 2014 English-to-French translation task after training for just 3.5 days on eight GPUs, which is a small fraction of the training costs of the best models from the literature[3].\\\\n\\\\nIn summary, the Transformer model offers the following primary benefits:\\\\n- **Efficient Parallelization**: Allows for faster training times by processing input sequences in parallel.\\\\n- **Improved Handling of Long-Range Dependencies**: Uses self-attention mechanisms to better capture relationships within the input sequence.\\\\n- **Superior Performance**: Achieves state-of-the-art results in translation tasks with improved BLEU scores.\\\\n- **Reduced Training Time**: Requires significantly less time to train compared to RNNs and convolutional neural networks.\",\"In the context of transduction models, the Transformer model uniquely utilizes the **self-attention mechanism** to compute representations of its input and output. This mechanism allows the model to process sequences in parallel, capturing dependencies between different parts of the input sequence regardless of their distance, without relying on traditional sequence-aligned Recurrent Neural Networks (RNNs) or convolution methods.\\\\n\\\\nHere are the key points:\\\\n\\\\n- **Self-Attention**: This mechanism enables the model to weigh the importance of different parts of the input sequence when processing each element, allowing it to capture long-range dependencies efficiently[2][5].\\\\n- **Parallel Processing**: Unlike RNNs, which process sequences sequentially, the self-attention mechanism in Transformers allows for parallel processing of the entire sequence, making it more computationally efficient[2][5].\\\\n- **Encoder-Decoder Structure**: The Transformer model uses an encoder-decoder architecture, where the encoder maps the input sequence to a sequence of continuous representations, and the decoder generates the output sequence based on these representations. Both the encoder and decoder rely on self-attention mechanisms[1][4].\",\"The individuals who were instrumental in both the design and implementation phases of the tensor2tensor library are:\\\\n\\\\n## Lukasz Kaiser\\\\nLukasz Kaiser was a key figure in designing and implementing various parts of tensor2tensor. During this work, he was affiliated with Google Brain[2][5].\\\\n\\\\n## Aidan Gomez\\\\nAidan Gomez also played a crucial role in designing and implementing tensor2tensor, working closely with Lukasz Kaiser. Like Lukasz, Aidan was affiliated with Google Brain during this period[2][4].\\\\n\\\\nBoth Lukasz and Aidan spent significant time and effort in replacing the earlier codebase, which greatly improved results and accelerated the research[1][2].\",\"Based on the provided context, here are the details for training the base models:\\\\n\\\\n## Training Time\\\\nThe base models were trained for a total of **100,000 steps**.\\\\n\\\\n## Time Per Step\\\\nEach training step took about **0.4 seconds**.\\\\n\\\\n## Total Training Time\\\\nTo calculate the total training time, you can multiply the number of steps by the time per step:\\\\n\\\\\\\\[ 100,000 \\\\\\\\text{ steps} \\\\\\\\times 0.4 \\\\\\\\text{ seconds/step} = 40,000 \\\\\\\\text{ seconds} \\\\\\\\]\\\\n\\\\n## Conversion to Hours\\\\nTo convert the total training time from seconds to hours:\\\\n\\\\\\\\[ 40,000 \\\\\\\\text{ seconds} \\\\\\\\div 3600 \\\\\\\\text{ seconds/hour} = 11.11 \\\\\\\\text{ hours} \\\\\\\\]\\\\nHowever, the context mentions that the training took approximately **12 hours**, which might include additional overhead or other factors not accounted for in the simple calculation.\\\\n\\\\nIn summary, the base models were trained for **100,000 steps**, and this process took approximately **12 hours**.\",\"To understand why self-attention layers, particularly those using multi-head attention in Transformers, exhibit faster computational complexity compared to recurrent layers when dealing with large input sequences, let&#x27;s break down the key points:\\\\n\\\\n## Computational Complexity of Recurrent Layers\\\\n\\\\nRecurrent Neural Networks (RNNs) process sequences sequentially, where each time step depends on the previous one. This sequential processing leads to a computational complexity that is linear with respect to the sequence length \\\\\\\\(N\\\\\\\\), but it is inherently sequential. The hidden state at each time step \\\\\\\\(t\\\\\\\\) depends on the previous state \\\\\\\\(h_{t-1}\\\\\\\\) and the current input \\\\\\\\(x_t\\\\\\\\), which requires \\\\\\\\(O(N)\\\\\\\\) sequential operations. However, because these operations are sequential and not parallelizable in the same way as self-attention, the overall computational time can be significant for long sequences[2][4].\\\\n\\\\n## Computational Complexity of Self-Attention Layers\\\\n\\\\nSelf-attention layers, including multi-head attention, compute attention scores and weighted outputs in a parallelizable manner. Here are the key points:\\\\n\\\\n- **Parallel Computation**: Self-attention mechanisms can process the entire sequence simultaneously, which allows for parallel computation. This is in contrast to the sequential processing of RNNs[1][3].\\\\n\\\\n- **Complexity Analysis**: The computational complexity of multi-head attention is \\\\\\\\(O(N^2 \\\\\\\\cdot d_{model} + N \\\\\\\\cdot d_{model}^2)\\\\\\\\), where \\\\\\\\(N\\\\\\\\) is the sequence length and \\\\\\\\(d_{model}\\\\\\\\) is the model dimension. Although this complexity appears quadratic with respect to \\\\\\\\(N\\\\\\\\), it can be computed in parallel, which significantly reduces the actual computational time compared to sequential RNNs[1][5].\\\\n\\\\n- **Scalability**: While the quadratic term \\\\\\\\(N^2 \\\\\\\\cdot d_{model}\\\\\\\\) might seem daunting, modern computing architectures, such as GPUs and TPUs, are highly optimized for matrix operations. This makes the parallel computation of self-attention much faster in practice than the sequential computation of RNNs.\\\\n\\\\n## Key Differences\\\\n\\\\n- **Parallelization**: Self-attention layers can be fully parallelized across the sequence length, which is not possible with RNNs. This parallelization is a significant advantage when dealing with large input sequences.\\\\n- **Matrix Operations**: Self-attention involves matrix multiplications and dot products, which are highly optimized in modern deep learning frameworks. These operations can be performed much faster than the sequential operations required by RNNs.\\\\n- **Memory and Computational Resources**: Although self-attention has a higher computational complexity in terms of \\\\\\\\(N^2\\\\\\\\), the ability to parallelize these computations often makes it faster in practice. Additionally, techniques like head pruning and other optimizations can mitigate the increased computational complexity during inference[3][5].\\\\n\\\\nIn summary, while the computational complexity of self-attention layers appears higher due to the quadratic term, the ability to perform these computations in parallel and the optimization of matrix operations in modern hardware make self-attention layers generally faster than recurrent layers when dealing with large input sequences.\",\"In the context of the Transformer architecture, which relies entirely on attention mechanisms to draw global dependencies between input and output, reducing the attention key size \\\\\\\\(d_k\\\\\\\\) can have several implications on the model quality.\\\\n\\\\n## Impact on Model Quality\\\\n\\\\n### Dimensionality and Representation Capacity\\\\nReducing \\\\\\\\(d_k\\\\\\\\) decreases the dimensionality of the key vectors in the self-attention mechanism. This can limit the model&#x27;s ability to capture complex and nuanced relationships between different parts of the input sequence. Lower dimensional key vectors may not be able to represent the input data as effectively, potentially leading to a reduction in the model&#x27;s performance and its ability to generalize well[2][4].\\\\n\\\\n### Computational Efficiency\\\\nWhile reducing \\\\\\\\(d_k\\\\\\\\) can lead to lower computational costs and memory requirements, as it decreases the number of parameters and the amount of data that needs to be processed, this efficiency gain comes at the cost of potentially reduced model performance. The attention mechanism&#x27;s ability to capture long-range dependencies and parallelize computations efficiently might be compromised if the key size is too small[3][5].\\\\n\\\\n## Specific Conditions\\\\n\\\\n### Sequence Length and Complexity\\\\nThe impact of reducing \\\\\\\\(d_k\\\\\\\\) is more pronounced for longer and more complex sequences. For shorter sequences or simpler tasks, the model might still perform reasonably well with a smaller \\\\\\\\(d_k\\\\\\\\). However, for tasks involving long sequences or complex dependencies, a smaller \\\\\\\\(d_k\\\\\\\\) could significantly degrade the model&#x27;s performance because it would struggle to capture the necessary contextual information[2][5].\\\\n\\\\n### Multi-Headed Attention\\\\nIn the Transformer architecture, multi-headed attention is used to project the input embeddings into different representation subspaces. Reducing \\\\\\\\(d_k\\\\\\\\) in this context means each attention head has fewer dimensions to work with. While the model can still benefit from multiple attention heads, the overall capacity of each head to capture diverse aspects of the input is reduced. This can lead to a less robust representation of the input data[2][4].\\\\n\\\\n### Training and Generalization\\\\nReducing \\\\\\\\(d_k\\\\\\\\) can also affect the model&#x27;s ability to generalize well to unseen data. With fewer dimensions, the model may overfit to the training data more easily, as it has fewer parameters to learn and generalize from. This can result in poor performance on test datasets or in real-world applications[3].\\\\n\\\\nIn summary, reducing the attention key size \\\\\\\\(d_k\\\\\\\\) in the Transformer architecture can lead to reduced model performance, particularly for complex and long sequences, due to the limited representation capacity and potential for overfitting. While it may offer some computational efficiency gains, these come at the cost of model quality and generalization capabilities.\",\"Based on the provided context, the &#x27;big&#x27; Transformer model achieved a significant improvement in the BLEU score for the English-to-German translation task on the WMT 2014 dataset. Specifically, it is mentioned that the &#x27;big&#x27; Transformer model established a new state-of-the-art BLEU score of **28.4** on this task[1]. This performance was achieved without explicitly mentioning the use of the Section 22 development set for hyperparameter tuning, but it indicates the model&#x27;s performance on the WMT 2014 English-to-German translation task.\",\"To assess whether the Transformer model can generalize to other tasks beyond machine translation, the authors of the paper \\\\\"Attention Is All You Need\\\\\" applied the Transformer model to the task of **English constituency parsing**.\\\\n\\\\nThis task involves parsing the syntactic structure of English sentences, which is different from machine translation but still a sequence processing task. The model was successfully applied to this task both with large and limited training data, demonstrating its generalizability beyond machine translation[3].\",\"Based on the provided context and additional information from the sources, here are the BLEU scores achieved by the Transformer (big) model for the specified translation tasks:\\\\n\\\\n## English-to-French Translation\\\\nThe Transformer (big) model achieved a BLEU score of **41.0** on the WMT 2014 English-to-French translation task[Context].\\\\n\\\\n## English-to-German Translation\\\\nFor the WMT 2014 English-to-German translation task, the Transformer model (though not explicitly stated as &#x27;big&#x27; in this context, but referring to the same model architecture) achieved a BLEU score of **28.4**[1].\",\"Based on the provided context, the big model, which is the Transformer model, achieved a BLEU score of **41.0** on the WMT 2014 English-to-French translation task. This performance was achieved with less than 1/4 the training cost of the previous state-of-the-art model, although the specific FLOP count is not mentioned in the context, the BLEU score of 41.0 is the relevant metric here[1].\",\"A potential side benefit of using self-attention mechanisms in models, particularly in the context of capturing long-distance dependencies, is their ability to enhance **contextual understanding** and **parallel computation efficiency**.\\\\n\\\\n## Contextual Understanding\\\\nSelf-attention allows the model to weigh the importance of different parts of the input sequence relative to each other. This mechanism enables the model to identify and focus on relevant information, even if it is distant within the sequence. This is crucial for tasks such as natural language processing, where understanding the context and relationships between words or phrases is essential for tasks like machine translation, text summarization, and question answering[1][3][5].\\\\n\\\\n## Parallel Computation Efficiency\\\\nUnlike some traditional models that rely on sequential processing (e.g., RNNs), self-attention can be computed in parallel for each element in the sequence. This makes it computationally efficient and scalable for large datasets, as the number of operations required to relate signals from two arbitrary input or output positions remains constant, rather than increasing with the distance between positions[1][2][5].\\\\n\\\\n## Handling Long-Range Dependencies\\\\nThe ability to capture long-range dependencies without the computational cost of sequential processing is a significant advantage. This is particularly important in tasks where understanding the relationships between distant elements in a sequence is critical, such as in reading comprehension, abstractive summarization, and textual entailment[1][2][3].\\\\n\\\\nIn summary, the use of self-attention mechanisms not only enhances the model&#x27;s ability to capture long-distance dependencies but also improves contextual understanding and computational efficiency.\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, the big transformer model achieved a BLEU score of **28.4** on the WMT 2014 English-to-German translation task[5].\",\"In the context of the Transformer model described, the decoder and encoder layers both utilize a multi-head attention mechanism, but the specific number of attention heads is not explicitly mentioned in the provided snippet.\\\\n\\\\nHowever, based on the general architecture of Transformers, each multi-head attention mechanism typically consists of multiple parallel attention heads. The standard implementation often uses 8 or 16 attention heads, but this can vary depending on the specific model configuration.\\\\n\\\\nFor the Transformer model described in the original paper by Vaswani et al., which is a common reference point, each multi-head attention layer typically uses 8 attention heads[4].\\\\n\\\\nTherefore, in the decoder of this model, each layer that implements multi-head attention would likely have **8 parallel attention heads**, assuming it follows the standard configuration. However, without explicit mention of the number of heads in the provided context, this is an inference based on common practices in Transformer architectures.\",\"In the context of Transformer models, particularly in the decoder&#x27;s self-attention layers, masking plays a crucial role in ensuring that the model generates output sequentially and does not have access to future information that it should not know.\\\\n\\\\nHere are the key points about the purpose of masking in the decoder&#x27;s self-attention layers:\\\\n\\\\n## Preventing Future Information Leakage\\\\nThe primary purpose of masking is to prevent the model from attending to positions that are subsequent to the current position. This is essential because, in many sequence-to-sequence tasks like machine translation or text generation, the model should only use the information available up to the current position to make predictions for that position[4].\\\\n\\\\n## Masking Mechanism\\\\nIn the decoder, the self-attention mechanism is modified to include a masking process. This masking ensures that the attention weights for positions that are ahead of the current position are set to negative infinity, effectively preventing the model from attending to those positions. This is often referred to as \\\\\"causal masking\\\\\" or \\\\\"autoregressive masking\\\\\"[4].\\\\n\\\\n## Offset Output Embeddings\\\\nAdditionally, the output embeddings in the decoder are offset by one position. This means that when generating the output for position \\\\\\\\(i\\\\\\\\), the model only has access to the outputs generated for positions less than \\\\\\\\(i\\\\\\\\). This, combined with the masking, ensures that the predictions for position \\\\\\\\(i\\\\\\\\) can only depend on the known outputs at positions less than \\\\\\\\(i\\\\\\\\)[4].\\\\n\\\\n## Ensuring Sequential Generation\\\\nThis mechanism is vital for tasks where the output needs to be generated sequentially, such as in language translation or text generation. It ensures that the model generates output in a way that mimics human-like sequential processing, where each word or token is generated based on the context of the previous words or tokens, without peeking into the future.\\\\n\\\\nIn summary, the masking in the decoder&#x27;s self-attention layers is essential for maintaining the autoregressive nature of the model, ensuring that it generates output sequentially and does not leverage future information that it should not have access to.\",\"The Transformer model, as described in the context, achieved new state-of-the-art results in several specific tasks:\\\\n\\\\n## Machine Translation\\\\n- On the WMT 2014 English-to-German translation task, the Transformer model achieved a BLEU score of 28.4, improving over the existing best results, including ensembles, by over 2 BLEU points[3].\\\\n- On the WMT 2014 English-to-French translation task, the Transformer model established a new single-model state-of-the-art BLEU score of 41.8[3].\\\\n\\\\n## English Constituency Parsing\\\\n- The Transformer model also generalized well to English constituency parsing, performing successfully with both large and limited training data[3].\\\\n\\\\nThese achievements highlight the model&#x27;s superior performance, parallelizability, and efficiency in training time compared to previous models.\",\"To understand the main differences between dot-product attention and additive attention, here are the key points to consider:\\\\n\\\\n## Dot-Product Attention\\\\n\\\\n- **Calculation Method**: Dot-product attention calculates the attention weights by taking the dot product of the query matrix \\\\\\\\(Q\\\\\\\\) and the key matrix \\\\\\\\(K\\\\\\\\), and then scaling the result by the square root of the dimension of the key matrix \\\\\\\\(d_k\\\\\\\\). This is expressed as:\\\\n  \\\\\\\\[\\\\n  \\\\\\\\text{Attention}(Q, K, V) = \\\\\\\\text{softmax}\\\\\\\\left(\\\\\\\\frac{QK^T}{\\\\\\\\sqrt{d_k}}\\\\\\\\right)V\\\\n  \\\\\\\\][1][3][4].\\\\n\\\\n- **Efficiency**: This method is highly efficient because it can be implemented using optimized matrix multiplication code, making it faster and more space-efficient in practice[1][3].\\\\n\\\\n- **Scaling Factor**: The scaling factor \\\\\\\\(\\\\\\\\frac{1}{\\\\\\\\sqrt{d_k}}\\\\\\\\) is crucial to prevent the dot products from growing too large, which helps in stabilizing the gradients during training[1][3].\\\\n\\\\n## Additive Attention\\\\n\\\\n- **Calculation Method**: Additive attention computes the attention weights using a feed-forward neural network with a single hidden layer. Here, the similarity between the query and key vectors is calculated by passing the concatenated query and key vectors through this network, which produces a scalar value used as the attention weight[1][3][4].\\\\n\\\\n- **Non-Linear Similarity**: Unlike dot-product attention, additive attention allows for a non-linear similarity measure between the query and key vectors, which can be beneficial in certain scenarios where a linear dot product is not sufficient[1].\\\\n\\\\n- **Performance**: While both mechanisms have similar theoretical complexity, additive attention can outperform dot-product attention without scaling for larger values of \\\\\\\\(d_k\\\\\\\\), as it can capture more complex relationships between the query and key vectors[3].\\\\n\\\\n## Summary\\\\n\\\\n- **Dot-Product Attention**:\\\\n  - Uses dot product and scaling.\\\\n  - Efficient due to optimized matrix operations.\\\\n  - Suitable for most transformer architectures.\\\\n\\\\n- **Additive Attention**:\\\\n  - Uses a feed-forward neural network.\\\\n  - Allows for non-linear similarity measures.\\\\n  - Can perform better for larger \\\\\\\\(d_k\\\\\\\\) values but is less widely used and more computationally extensive.\\\\n\\\\nUnderstanding these differences will help you distinguish between these two attention mechanisms in your exam.\",\"To understand how Transformer models compare to RNN sequence-to-sequence models and the Berkeley-Parser, especially in the context of translation tasks like English to German and French, here are some key points:\\\\n\\\\n## Performance and Efficiency\\\\n\\\\n### Transformer Models\\\\n- Transformer models outperform RNN sequence-to-sequence models in several aspects. They are the first transduction models to rely entirely on self-attention mechanisms, eliminating the need for recurrent layers[4][5].\\\\n- This self-attention mechanism allows Transformers to capture long-range dependencies between words in a sentence more effectively than RNNs, which often suffer from the vanishing gradient problem and are limited by their sequential processing nature[1][5].\\\\n- Transformers can process input sequences in parallel, which significantly reduces the training and inference time compared to RNNs. This parallel processing capability makes them much more efficient, especially for large datasets[5].\\\\n\\\\n### RNN Sequence-to-Sequence Models\\\\n- RNNs process input sequences sequentially, which can lead to slower processing times and difficulties in capturing long-range dependencies due to the vanishing gradient problem. This sequential processing limits their ability to handle large sequences efficiently[2][5].\\\\n\\\\n## Specific Results and Comparisons\\\\n\\\\n### Against RNN Sequence-to-Sequence Models\\\\n- In translation tasks such as English to German and French, Transformer models have achieved state-of-the-art results, outperforming previously reported models based on RNNs. This is evident from the performance on WMT 2014 English-to-German and English-to-French translation tasks, where Transformers set new state-of-the-art benchmarks[4][5].\\\\n\\\\n### Against the Berkeley-Parser\\\\n- Even when trained only on the WSJ training set of 40K sentences, the Transformer model outperforms the Berkeley-Parser. This indicates that the Transformer&#x27;s ability to learn from limited task-specific data is superior to traditional models like the Berkeley-Parser[Context provided].\\\\n\\\\n## Training Speed and Task-Specific Tuning\\\\n\\\\n### Training Speed\\\\n- Transformers can be trained significantly faster than architectures based on recurrent or convolutional layers. This is due to their ability to leverage parallel computing resources, which accelerates both training and inference processes[4][5].\\\\n\\\\n### Task-Specific Tuning\\\\n- Despite the lack of task-specific tuning, the Transformer model performs surprisingly well, often yielding better results than models that have been fine-tuned for specific tasks. This suggests that Transformers can generalize well across different tasks without extensive task-specific tuning[Context provided].\\\\n\\\\nIn summary, Transformer models excel in translation tasks by:\\\\n- Capturing long-range dependencies more effectively through self-attention mechanisms.\\\\n- Processing input sequences in parallel, which enhances efficiency and speed.\\\\n- Achieving state-of-the-art results in translation tasks such as English to German and French.\\\\n- Outperforming traditional models like the Berkeley-Parser even with limited task-specific training data.\\\\n\\\\nThese advantages make Transformer models a preferred choice over RNN sequence-to-sequence models and other traditional architectures for many natural language processing tasks.\",\"The question about the dimensionality of the input and output in a Feed-Forward Network (FFN) or any specific layer, such as those involving self-attention, can be addressed by looking at the general structure of these layers.\\\\n\\\\n### Self-Attention Layer\\\\nIn the context of self-attention, as described in the sources, each token in the input sequence is represented by a vector. Here are the key points:\\\\n- The input to a self-attention layer is typically a sequence of tokens, each represented by a vector of dimension \\\\\\\\(d\\\\\\\\). Therefore, the input can be viewed as an \\\\\\\\(n \\\\\\\\times d\\\\\\\\) matrix, where \\\\\\\\(n\\\\\\\\) is the number of tokens and \\\\\\\\(d\\\\\\\\) is the dimension of each token&#x27;s representation[2][5].\\\\n- The output of the self-attention layer also maintains this structure, resulting in an \\\\\\\\(n \\\\\\\\times d\\\\\\\\) matrix, as the self-attention mechanism transforms the input tokens while keeping the same number of tokens and dimensionality[5].\\\\n\\\\n### Feed-Forward Network (FFN)\\\\nFor a Feed-Forward Network (FFN) layer, often used within transformer architectures:\\\\n- The input to an FFN layer is usually the output from the previous layer, which could be an \\\\\\\\(n \\\\\\\\times d\\\\\\\\) matrix if it follows a self-attention layer.\\\\n- The FFN typically consists of two linear transformations with a ReLU activation function in between. The first linear transformation increases the dimensionality to a higher dimension (often denoted as \\\\\\\\(d_{ff}\\\\\\\\)), and the second linear transformation reduces it back to the original dimension \\\\\\\\(d\\\\\\\\)[3].\\\\n  - Therefore, the input and output dimensionality of the FFN layer would be \\\\\\\\(n \\\\\\\\times d\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the number of tokens and \\\\\\\\(d\\\\\\\\) is the dimension of each token&#x27;s representation.\\\\n\\\\nIn summary, for both self-attention and FFN layers within transformer architectures, the input and output dimensionality is typically \\\\\\\\(n \\\\\\\\times d\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the number of tokens in the sequence and \\\\\\\\(d\\\\\\\\) is the dimension of each token&#x27;s representation.\",\"In the semi-supervised setting described, the vocabulary size used is **32K tokens**. This is in contrast to the 16K tokens used in the setting where only the WSJ (Wall Street Journal) corpus was utilized[5].\",\"To understand why self-attention layers are considered faster than recurrent layers in terms of computational complexity and execution, here are the key points to consider:\\\\n\\\\n## Computational Complexity\\\\n\\\\n- **Recurrent Neural Networks (RNNs):**\\\\n  - In RNNs, the computational complexity is \\\\\\\\(\\\\\\\\mathcal{O}(nd^2)\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the sequence length and \\\\\\\\(d\\\\\\\\) is the dimension of the input vectors. This complexity arises because RNNs process the sequence sequentially, updating the hidden state at each time step. This sequential processing means that each step depends on the previous one, leading to \\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\) sequential operations[2][4].\\\\n\\\\n- **Self-Attention Layers:**\\\\n  - Self-attention layers have a computational complexity of \\\\\\\\(\\\\\\\\mathcal{O}(n^2d)\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the sequence length and \\\\\\\\(d\\\\\\\\) is the dimension of the input vectors. Although this seems higher than RNNs at first glance, the critical difference lies in how these operations can be parallelized[1][3].\\\\n\\\\n## Parallelization\\\\n\\\\n- **RNNs:**\\\\n  - RNNs are inherently sequential, meaning that each time step&#x27;s computation depends on the previous time step. This makes parallelizing the computations across the sequence challenging, resulting in \\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\) sequential operations[2][4].\\\\n\\\\n- **Self-Attention Layers:**\\\\n  - Self-attention layers allow for parallel computation across the entire sequence. Since each token can attend to every other token simultaneously, the operations can be performed in parallel. This results in \\\\\\\\(\\\\\\\\mathcal{O}(1)\\\\\\\\) sequential operations, significantly reducing the overall computation time despite the higher computational complexity[1][5].\\\\n\\\\n## Maximum Path Length\\\\n\\\\n- **RNNs:**\\\\n  - RNNs have a maximum path length of \\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\), meaning that information from the beginning of the sequence has to travel through \\\\\\\\(n\\\\\\\\) steps to reach the end. This can lead to issues like the vanishing gradient problem, especially for long sequences[2][4].\\\\n\\\\n- **Self-Attention Layers:**\\\\n  - Self-attention layers have a maximum path length of \\\\\\\\(\\\\\\\\mathcal{O}(1)\\\\\\\\), as each token can directly interact with any other token in the sequence. This makes it easier to capture long-range dependencies without the issues associated with sequential processing[1][5].\\\\n\\\\nIn summary, while self-attention layers have a higher computational complexity (\\\\\\\\(\\\\\\\\mathcal{O}(n^2d)\\\\\\\\)) compared to RNNs (\\\\\\\\(\\\\\\\\mathcal{O}(nd^2)\\\\\\\\)), their ability to be parallelized across the entire sequence makes them significantly faster in practice. This parallelization capability, combined with their shorter maximum path length, allows self-attention layers to handle long sequences more efficiently than RNNs.\",\"The Transformer model, as described in the context, achieved a BLEU score of **28.4** on the WMT 2014 English-to-German translation task. This score represents a significant improvement over the existing best results, including ensembles, by more than 2 BLEU points[3].\",\"## What is Self-Attention?\\\\n\\\\nSelf-attention, also known as intra-attention, is a mechanism in machine learning that allows a model to relate different positions of a single sequence to compute a representation of that sequence. Here\\\\u2019s a detailed breakdown of how it works and its applications:\\\\n\\\\n### Mechanism\\\\n\\\\nSelf-attention operates by transforming the input sequence into three vectors: **query**, **key**, and **value**. These vectors are obtained through linear transformations of the input.\\\\n\\\\n- **Query**: Represents the current focus or question the model has about a specific part of the sequence.\\\\n- **Key**: Acts as a label or reference point for each part of the sequence.\\\\n- **Value**: The actual content that is used to compute the weighted sum.\\\\n\\\\nThe attention mechanism calculates a weighted sum of the values based on the similarity between the query and key vectors. This process is often described as the model \\\\\"attending\\\\\" to different parts of the input sequence to understand their relative importance and relationships[1][3][5].\\\\n\\\\n### Benefits and Capabilities\\\\n\\\\n- **Long-range Dependencies**: Self-attention can capture relationships between distant elements in a sequence, which is crucial for understanding complex patterns and dependencies. Unlike some other models (like ConvS2S and ByteNet), self-attention reduces the number of operations required to relate signals from two arbitrary input or output positions to a constant, making it more efficient for long-range dependencies[2].\\\\n- **Contextual Understanding**: By attending to different parts of the input sequence, self-attention helps the model understand the context and assign appropriate weights to each element based on its relevance[1][3][5].\\\\n- **Parallel Computation**: Self-attention can be computed in parallel for each element in the sequence, making it computationally efficient and scalable for large datasets[1].\\\\n\\\\n### Successful Applications\\\\n\\\\nSelf-attention has been successfully applied in various tasks:\\\\n\\\\n- **Natural Language Processing (NLP)**:\\\\n  - Machine translation\\\\n  - Text summarization\\\\n  - Sentiment analysis\\\\n  - Question answering\\\\n  - Reading comprehension\\\\n  - Abstractive summarization\\\\n  - Textual entailment\\\\n  - Learning task-independent sentence representations[1][3][5].\\\\n\\\\n- **Computer Vision**:\\\\n  - Image classification\\\\n  - Object detection\\\\n  - Image captioning, where it helps capture long-range dependencies between image regions[1].\\\\n\\\\n- **Recommender Systems**:\\\\n  - Personalized recommendation systems by capturing user preferences and item relationships[1].\\\\n\\\\n### Key Models and Architectures\\\\n\\\\nSelf-attention is a key component of several influential models:\\\\n\\\\n- **Transformer Model**: Introduced in the paper \\\\\"Attention Is All You Need,\\\\\" this model revolutionized NLP tasks by using self-attention as its core mechanism[2][3][5].\\\\n- **BERT (Bidirectional Encoder Representations from Transformers)**: A pre-trained Transformer model that utilizes self-attention to capture contextual information in natural language[1].\\\\n\\\\nIn summary, self-attention is a powerful technique that enhances the ability of models to understand and process sequential data by capturing long-range dependencies, contextual relationships, and parallelizing computations, making it instrumental in a wide range of NLP and other machine learning tasks.\",\"Figure 3 in your context illustrates the operation of the self-attention mechanism in the encoder layer of a transformer model, particularly in how it handles long-range dependencies in a sequence of text. Here\\\\u2019s a detailed breakdown of what it shows:\\\\n\\\\n## Self-Attention Mechanism\\\\nThe self-attention mechanism is a key component of transformer models, introduced to address the limitations of traditional sequential models like RNNs and LSTMs in capturing long-range dependencies[1][5].\\\\n\\\\n## Encoder Self-Attention\\\\nIn the encoder of a transformer model, the self-attention mechanism allows each token in the input sequence to attend to all other tokens in the same sequence. This is done through multiple attention heads, each of which can focus on different aspects of the input.\\\\n\\\\n## Figure 3 Explanation\\\\n- **Attention Visualization**: Figure 3 visualizes the attention weights for the word \\\\\"making\\\\\" in the sentence \\\\\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.\\\\\"\\\\n- **Long-Distance Dependencies**: The figure highlights how different attention heads in layer 5 of the encoder attend to distant dependencies. Specifically, it shows that many of the attention heads focus on the verb \\\\\"making\\\\\" and its relationship to other parts of the sentence, such as \\\\\"more difficult,\\\\\" which completes the phrase \\\\\"making...more difficult.\\\\\"\\\\n- **Multiple Heads**: The use of different colors represents different attention heads. Each head can capture different types of dependencies and contextual relationships within the sentence. This allows the model to understand the sentence more comprehensively by considering multiple perspectives simultaneously.\\\\n- **Contextual Understanding**: By attending to distant tokens, the model can capture the contextual meaning of the word \\\\\"making\\\\\" in relation to the entire sentence. This is crucial for understanding the impact of voter registration laws, as the context in which these laws are mentioned is vital for accurate interpretation.\\\\n\\\\n## Key Points\\\\n- **Parallel Computation**: Unlike RNNs and LSTMs, the self-attention mechanism allows for parallel computation of attention scores across all tokens, which significantly speeds up the processing time[1].\\\\n- **Contextual Representation**: The mechanism enables the model to create contextual representations of each token by considering its relationship with all other tokens in the sequence, which is essential for tasks like understanding the nuances of legal or political text[1][5].\\\\n\\\\nIn summary, Figure 3 illustrates how the self-attention mechanism in the encoder layer of a transformer model effectively captures long-range dependencies and contextual relationships within a sentence, which is particularly useful for analyzing complex texts related to changes in voter registration laws and their impact on electoral processes.\",\"In the context of the Transformer architecture, the decoder component has several key features that distinguish it from the encoder.\\\\n\\\\n## Purpose of the Third Sub-Layer in the Decoder\\\\nThe third sub-layer in the decoder is designed to perform multi-head attention over the output of the encoder stack. This sub-layer is often referred to as the \\\\\"encoder-decoder attention\\\\\" or \\\\\"cross-attention\\\\\" mechanism. Its primary purpose is to allow the decoder to attend to the output of the encoder, which contains the contextual information from the input sequence. This enables the decoder to generate output based on the encoded representation of the input sequence[4][5].\\\\n\\\\n## Ensuring Predictions Depend Only on Known Outputs\\\\nTo ensure that the predictions for position \\\\\\\\(i\\\\\\\\) in the decoder depend only on the known outputs at positions less than \\\\\\\\(i\\\\\\\\), the Transformer architecture employs two key techniques:\\\\n\\\\n### Masking\\\\nThe self-attention sub-layer in the decoder stack is modified to include a masking mechanism. This masking prevents positions from attending to subsequent positions. Specifically, it masks out the future positions, so the attention mechanism can only consider the outputs from positions less than \\\\\\\\(i\\\\\\\\) when generating the output for position \\\\\\\\(i\\\\\\\\)[4].\\\\n\\\\n### Offset Output Embeddings\\\\nIn addition to masking, the output embeddings in the decoder are offset by one position. This means that when generating the output for position \\\\\\\\(i\\\\\\\\), the decoder only has access to the outputs from positions \\\\\\\\(0\\\\\\\\) to \\\\\\\\(i-1\\\\\\\\). This offset, combined with the masking, ensures that the predictions for position \\\\\\\\(i\\\\\\\\) can depend only on the known outputs at positions less than \\\\\\\\(i\\\\\\\\), preventing the model from looking ahead and maintaining the auto-regressive nature of the decoder[4].\",\"To answer the question, we need to identify the specific works mentioned in the context.\\\\n\\\\n## Limits of Language Modeling\\\\nThe work titled \\\\\"Exploring the limits of language modeling\\\\\" was authored by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. This was published as an arXiv preprint in 2016.\\\\n\\\\n## Neural Machine Translation by Nal Kalchbrenner and Colleagues\\\\nThe work related to neural machine translation by Nal Kalchbrenner and colleagues is titled \\\\\"Neural machine translation in linear time.\\\\\" This was also published as an arXiv preprint in 2017.\",\"To answer your question, here are the details from the provided context:\\\\n\\\\n## Authors of the Paper \\\\\"Learning phrase representations using rnn encoder-decoder for statistical machine translation\\\\\"\\\\nThe authors of this paper are:\\\\n- Kyunghyun Cho\\\\n- Bart van Merrienboer\\\\n- Caglar Gulcehre\\\\n- Fethi Bougares\\\\n- Holger Schwenk\\\\n- Yoshua Bengio[5].\\\\n\\\\n## Title of the Paper Authored by Francois Chollet in 2016\\\\nThe title of the paper authored by Francois Chollet in 2016 is:\\\\n- \\\\\"Xception: Deep learning with depthwise separable convolutions\\\\\".\",\"### Effect of Separable Convolutions on Computational Complexity\\\\n\\\\nSeparable convolutions significantly reduce the computational complexity compared to standard convolutional layers. Here\\\\u2019s a breakdown of how they impact the complexity:\\\\n\\\\n- **Standard Convolutional Layers**: These layers generally have a complexity of \\\\\\\\(O(k \\\\\\\\cdot n \\\\\\\\cdot d^2)\\\\\\\\), where \\\\\\\\(k\\\\\\\\) is the kernel size, \\\\\\\\(n\\\\\\\\) is the input length, and \\\\\\\\(d\\\\\\\\) is the number of channels or features.\\\\n- **Separable Convolutions**: Separable convolutions decompose the convolution operation into two separate operations: a depthwise convolution and a pointwise convolution. This decomposition reduces the complexity to \\\\\\\\(O(k \\\\\\\\cdot n \\\\\\\\cdot d + n \\\\\\\\cdot d^2)\\\\\\\\)[1][4].\\\\n\\\\nEven when \\\\\\\\(k = n\\\\\\\\), the complexity of separable convolutions is still lower than that of standard convolutions, making them more efficient.\\\\n\\\\n### Potential Benefits of Using Self-Attention in Models\\\\n\\\\nSelf-attention mechanisms, particularly in transformer models, offer several benefits:\\\\n\\\\n- **Interpretability**: Self-attention can yield more interpretable models. By inspecting the attention distributions, it is possible to see that individual attention heads learn to perform different tasks. Many of these heads exhibit behavior related to the syntactic and semantic structure of the sentences, providing insights into how the model processes and understands the input data[1].\\\\n\\\\n- **Handling Long-Range Dependencies**: Self-attention allows the model to consider the entire input sequence simultaneously, which is particularly useful for capturing long-range dependencies in data. This is in contrast to convolutional or recurrent layers, which may struggle with long-range dependencies due to their sequential or local processing nature[4].\\\\n\\\\n- **Parallelization**: Self-attention operations can be parallelized more easily than recurrent layers, which can lead to faster training times on modern hardware[4].\\\\n\\\\nHowever, it is important to note that standard self-attention mechanisms have a quadratic time and space complexity with respect to the input sequence length (\\\\\\\\(O(n^2)\\\\\\\\)), which can be computationally expensive for long sequences. Various methods, such as the Linformer, have been proposed to reduce this complexity to linear time and space (\\\\\\\\(O(n)\\\\\\\\))[5].\",\"### Main Point About the Law\\\\n\\\\nThe main point expressed about the law in the provided context is that while the law itself may never be perfect, its application should strive to be just. This is highlighted in the repeated phrase:\\\\n\\\\n\\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\\\\\"\\\\n\\\\nThis suggests a distinction between the inherent imperfections of the law and the importance of ensuring that its application is fair and just.\\\\n\\\\n### Figure 4: Attention Heads in Layer 5\\\\n\\\\nFigure 4, as described, illustrates the role of attention heads in layer 5 of a neural network model, specifically in the context of anaphora resolution.\\\\n\\\\n- **Attention Heads**: The figure shows two attention heads in layer 5, which are involved in anaphora resolution. Attention mechanisms in neural networks, such as transformers, help the model focus on specific parts of the input data that are relevant to the task at hand.\\\\n- **Full Attentions for Head 5**: The top part of the figure displays the full attention patterns for one of the attention heads in layer 5. This indicates how the model distributes its attention across the entire input sequence.\\\\n- **Isolated Attentions from the Word \\\\u2018its\\\\u2019**: The bottom part of the figure isolates the attention patterns specifically for the word \\\\u2018its\\\\u2019 from attention heads 5 and 6. It is noted that the attentions are very sharp for this word, meaning the model is strongly focusing on \\\\u2018its\\\\u2019 when resolving anaphoric references. This sharp focus suggests that the word \\\\u2018its\\\\u2019 is crucial for determining the antecedent in the anaphora resolution task[2][5].\",\"To compare the computational complexity of self-attention layers and recurrent layers, we need to analyze the nature and scale of the operations each type of layer involves.\\\\n\\\\n### Self-Attention Layers\\\\n\\\\nSelf-attention layers, which are a core component of transformer architectures, have a computational complexity that is quadratic in the length of the input sequence. Here are the key points:\\\\n\\\\n- The standard definition of self-attention requires \\\\\\\\(O(dn^2)\\\\\\\\) time to compute the output, where \\\\\\\\(n\\\\\\\\) is the length of the input sequence and \\\\\\\\(d\\\\\\\\) is the dimension of the tokens[1][4].\\\\n- This quadratic dependence on \\\\\\\\(n\\\\\\\\) arises because self-attention mechanisms involve computing attention scores between all pairs of tokens in the input sequence.\\\\n\\\\n### Recurrent Layers\\\\n\\\\nRecurrent Neural Networks (RNNs) and their variants (like LSTMs) have a different computational complexity profile:\\\\n\\\\n- RNNs process sequences sequentially, meaning each time step depends on the previous time step. This results in \\\\\\\\(O(n)\\\\\\\\) sequential operations, where \\\\\\\\(n\\\\\\\\) is the length of the input sequence[3].\\\\n- The key operations in RNNs involve updating the hidden state at each time step using the current input and the previous hidden state. This process is linear in the length of the sequence but involves repeated applications of the same set of weights across all time steps.\\\\n\\\\n### Comparison\\\\n\\\\n- **Self-Attention Layers**: These layers have a quadratic time complexity (\\\\\\\\(O(dn^2)\\\\\\\\)) due to the pairwise interactions between all tokens in the sequence.\\\\n- **Recurrent Layers**: These layers have a linear time complexity (\\\\\\\\(O(n)\\\\\\\\)) in terms of the number of sequential operations, but each operation involves matrix multiplications and other computations that are repeated \\\\\\\\(n\\\\\\\\) times.\\\\n\\\\n### Operations Required\\\\n\\\\n- **Recurrent Layers**: Require sequential operations where each time step&#x27;s computation depends on the output of the previous time step. This involves updating the hidden state using the current input and the previous hidden state, which can be expressed as \\\\\\\\(h_t = f(h_{t-1}, x_t)\\\\\\\\)[3].\\\\n\\\\nIn summary, while self-attention layers are faster in certain contexts due to their ability to parallelize computations (since the attention scores can be computed independently), they inherently have a higher computational complexity (\\\\\\\\(O(dn^2)\\\\\\\\)) compared to the linear complexity (\\\\\\\\(O(n)\\\\\\\\)) of recurrent layers. However, the parallelizable nature of self-attention can make it more efficient in practice when leveraging parallel computing resources like GPUs.\",\"Based on the provided context, here are the details to answer your question:\\\\n\\\\n## Permission for Reproduction\\\\nGoogle grants permission to reproduce the tables and figures in the paper \\\\\"Attention Is All You Need\\\\\" solely for use in journalistic or scholarly works, provided that proper attribution is given.\\\\n\\\\n## Main Innovation Proposed\\\\nThe main innovation proposed in the paper \\\\\"Attention Is All You Need\\\\\" is the introduction of a new network architecture called the Transformer, which is based solely on attention mechanisms. This architecture dispenses with the use of recurrence and convolutions, which were previously used in complex recurrent or convolutional neural networks. Instead, it relies entirely on self-attention mechanisms to handle sequence-to-sequence tasks, allowing for more parallelization and reducing training time[2][4].\",\"Based on the provided context, here are the reasons why the researchers chose the sinusoidal version of positional embeddings and the three desiderata considered for using self-attention:\\\\n\\\\n## Choice of Sinusoidal Positional Embeddings\\\\nThe researchers chose the sinusoidal version of positional embeddings because it allows the model to extrapolate to sequence lengths longer than those encountered during training. This is in contrast to learned positional embeddings, which, although producing nearly identical results, may not offer the same level of generalization to unseen sequence lengths[1][2].\\\\n\\\\n## Three Desiderata for Using Self-Attention\\\\nThe three key desiderata considered for using self-attention layers are:\\\\n\\\\n1. **Total Computational Complexity per Layer**: This involves evaluating how computationally expensive each layer is in terms of the operations required.\\\\n\\\\n2. **Amount of Computation that Can be Parallelized**: This considers the minimum number of sequential operations required, highlighting the potential for parallel processing which can significantly speed up the computation.\\\\n\\\\n3. **Minimum Number of Sequential Operations Required**: This aspect focuses on the ability to minimize the number of sequential steps, which is crucial for efficiency and scalability in processing variable-length sequences[1].\",\"The main architectural feature of the Transformer model, as proposed in the work \\\\\"Attention Is All You Need,\\\\\" is its reliance entirely on self-attention mechanisms to draw global dependencies between input and output, eliminating the need for recurrent or convolutional neural networks.\\\\n\\\\n### Key Features of the Transformer Model:\\\\n- The Transformer model uses an encoder-decoder architecture, where both the encoder and decoder consist of multiple layers that utilize self-attention mechanisms to process input sequences in parallel[3][4].\\\\n- This architecture allows for significant parallelization because the self-attention mechanism can compute the dependencies between all positions in the input sequence simultaneously, rather than sequentially.\\\\n\\\\n### Comparison to Other Models:\\\\n#### Extended Neural GPU, ByteNet, and ConvS2S:\\\\n- These models use convolutional neural networks (CNNs) as their basic building blocks.\\\\n  - **Extended Neural GPU** and **ConvS2S** compute hidden representations in parallel but have limitations:\\\\n    - In **ConvS2S**, the number of operations required to relate signals from two arbitrary input or output positions grows linearly with the distance between positions[3].\\\\n    - In **ByteNet**, this growth is logarithmic, which is an improvement but still not as efficient as the Transformer[3].\\\\n- Unlike the Transformer, these models do not eliminate sequential computation entirely and still face constraints related to the distance between input or output positions.\\\\n\\\\n### Parallelization:\\\\n- The Transformer model offers superior parallelization capabilities because it processes all positions in the input sequence simultaneously using self-attention. This allows for much faster training times and better scalability, as demonstrated by its ability to reach state-of-the-art translation quality in as little as twelve hours on eight P100 GPUs[3].\\\\n- In contrast, while models like Extended Neural GPU, ByteNet, and ConvS2S do allow for some parallelization, they are limited by their convolutional architecture and the inherent sequential dependencies it introduces. The Transformer&#x27;s pure self-attention approach avoids these limitations, making it more efficient and scalable for large-scale sequence transduction tasks.\",\"The purpose of multi-head attention in the model described is to enable the model to jointly attend to information from different representation subspaces at different positions. Here are the key points:\\\\n\\\\n## Purpose\\\\n- Multi-head attention allows the model to attend to parts of the sequence differently, such as focusing on longer-term dependencies versus shorter-term dependencies. This is achieved by running multiple independent attention mechanisms in parallel, which can capture various types of relationships within the input sequence[1][5].\\\\n\\\\n## Mechanism\\\\n- Each attention head computes attention weights based on different linear transformations of the query (`Q`), key (`K`), and value (`V`) vectors. The outputs from these heads are then concatenated and linearly transformed to produce the final output[1][5].\\\\n\\\\n## Number of Heads\\\\n- In the specific model described, \\\\\\\\( h = 8 \\\\\\\\) parallel attention layers or heads are employed. Each head uses reduced dimensionality, where \\\\\\\\( d_k = d_v = d_{model} / h = 64 \\\\\\\\), ensuring that the total computational cost remains similar to that of single-head attention with full dimensionality[1][5].\",\"The main method for achieving the capabilities described in the context, particularly in handling long-distance dependencies and capturing the nuances of language, is through the **attention mechanism**.\\\\n\\\\nHere\\\\u2019s a detailed explanation of how it works:\\\\n\\\\n## Dynamic Weighting\\\\nThe attention mechanism assigns weights to each word in the input sequence based on its relevance to the current task or context. This allows the model to dynamically adjust the importance of certain words, focusing more on the words that are crucial for understanding the sentence[1][5].\\\\n\\\\n## Self-Attention\\\\nSelf-attention layers enable elements in an entire sequence to attend to all other elements in the same sequence. This is particularly useful in tasks where the context and relationships between different elements are key to understanding the whole. For example, in the given context, the attention mechanism can focus on the verb &#x27;making&#x27; and its long-distance dependency to complete the phrase &#x27;making...more difficult&#x27;[3][5].\\\\n\\\\n## Contextual Understanding\\\\nAttention mechanisms compute context-dependent weights by analyzing the surrounding words and calculating attention scores. These scores determine the relevance of each word, allowing the model to capture semantic nuances and improve comprehension. This is evident in how the attention heads attend to distant dependencies, as shown in the example where many heads attend to the verb &#x27;making&#x27; to complete the phrase[1][3].\\\\n\\\\n## Long-Range Dependencies\\\\nThe attention mechanism makes it possible to capture relationships between words situated at a long distance, which is crucial for understanding complex sentences. This is illustrated in Figure 3, where the attention heads follow long-distance dependencies in the encoder self-attention layer[1][3].\\\\n\\\\nIn summary, the attention mechanism is the core method that enables models to focus on relevant parts of the input sequence, handle long-range dependencies, and capture contextual nuances, thereby enhancing their performance in various NLP tasks.\",\"The context provided appears to be related to the analysis of attention mechanisms in the context of anaphora resolution, particularly within neural network architectures.\\\\n\\\\n### Key Points:\\\\n- **Anaphora Resolution**: This is a task in Natural Language Processing (NLP) that involves identifying the antecedent (the noun or entity) to which an anaphor (usually a pronoun) refers[2][4][5].\\\\n- **Attention Mechanisms**: These are components of neural networks, such as transformers, that dynamically adjust focus to specific parts of the input data to improve the model&#x27;s performance on tasks like anaphora resolution[2][5].\\\\n\\\\n### Specific Context:\\\\n- The figure and text describe the behavior of attention heads in a neural network model, specifically in layer 5 of a 6-layer model. The attention heads are shown to be involved in anaphora resolution.\\\\n- The focus is on how the model attends to specific words, such as \\\\\"its,\\\\\" to resolve the anaphora. The attentions are described as \\\\\"very sharp\\\\\" for this word, indicating that the model is precisely focusing on this word to determine its antecedent.\\\\n\\\\n### Conclusion:\\\\nThe context is discussing the mechanism and behavior of attention heads within a neural network model, specifically how these attention heads are utilized to resolve anaphoras in text. This involves analyzing how the model focuses on specific words to determine the correct antecedent for an anaphor.\",\"The context provided revolves around the concept of **anaphora resolution** and the use of attention mechanisms in natural language processing (NLP).\\\\n\\\\n## What is it?\\\\n- **Anaphora resolution** is the process of identifying the antecedent (the noun or entity) to which an anaphor (typically a pronoun or other referring expression) refers in a text. This is a crucial task in NLP as it helps in understanding the meaning and context of the text[1][4][5].\\\\n\\\\n## Attention Mechanisms in Anaphora Resolution\\\\n- The specific context provided discusses the use of **attention mechanisms** within transformer architectures to aid in anaphora resolution. Attention mechanisms allow neural networks to focus on specific parts of the input data that are relevant for the task at hand. In the case of anaphora resolution, these mechanisms help the model to identify the correct antecedent for an anaphor by attending to the relevant parts of the text[2][5].\\\\n\\\\n## Where has it been applied?\\\\n- **Anaphora resolution using attention mechanisms** has been applied in various NLP tasks and datasets:\\\\n  - **Spanish dialogues**: An algorithm called ARDi was developed to resolve anaphora in Spanish dialogues, utilizing linguistic knowledge and dialogue-structure knowledge[1].\\\\n  - **Russian language**: A study compared human attention and neural attention mechanisms in the context of the Winograd schema challenge for the Russian language, using eye-tracking data to analyze how humans and machines focus on different parts of the text during anaphora resolution[2].\\\\n  - **Code-mixed social media data**: An approach using an encoder-decoder model with multi-headed attention was applied to resolve anaphora in Hindi-English code-mixed Twitter data[5].\\\\n  - **Question Answering systems**: Anaphora resolution tools like BRANQA have been integrated into Question Answering systems to improve their performance by accurately resolving pronoun references[4].\\\\n\\\\nThese applications highlight the versatility and importance of anaphora resolution in different linguistic and contextual settings.\",\"In the context provided, several datasets and settings were utilized for training and experimentation:\\\\n\\\\n## Penn Treebank\\\\n- This dataset consists of about 40,000 sentences from the Wall Street Journal (WSJ) and was used in a supervised setting[1][2][5].\\\\n\\\\n## Semi-Supervised Setting\\\\n- In addition to the Penn Treebank, a semi-supervised setting was employed using larger corpora:\\\\n  - The high-confidence corpus.\\\\n  - The BerkleyParser corpus.\\\\n  These corpora collectively contain approximately 17 million sentences[2].\\\\n\\\\n## Vocabulary\\\\n- For the WSJ only setting, a vocabulary of 16,000 tokens was used.\\\\n- For the semi-supervised setting, a vocabulary of 32,000 tokens was used.\\\\n\\\\n## Parameters and Experiments\\\\n- Experiments were conducted to select parameters such as dropout, attention, residual connections, learning rates, and beam size using the Section 22 development set. Other parameters remained unchanged from the English-to-German base translation model.\\\\n\\\\nSo, to summarize, both the Penn Treebank and the larger corpora (high-confidence and BerkleyParser) were utilized in different settings.\",\"In the context of the Transformer model and its attention mechanism, here are the key components and their functions as related to the provided context:\\\\n\\\\n## Attention Types\\\\nThe Transformer model employs several types of attention mechanisms:\\\\n\\\\n### Self-Attention (Intra-Attention)\\\\n- This is used in both the encoder and decoder layers. Self-attention allows the model to consider different positions of the same sequence when computing the representation of a word. It helps capture long-range dependencies and relationships within the input sequence[2][4].\\\\n\\\\n### Encoder-Decoder Attention (Cross-Attention)\\\\n- This is used in the decoder layers of the Transformer. It allows the decoder to focus on relevant parts of the input sequence (encoded by the encoder) when generating each word of the output sequence. This ensures the decoder has access to the entire input sequence, aiding in producing more accurate and contextually appropriate translations[2][4].\\\\n\\\\n## Multi-Head Attention\\\\n- The Transformer&#x27;s attention module repeats its computations multiple times in parallel, known as Multi-head attention. Each computation is called an Attention Head.\\\\n  - The Query, Key, and Value parameters are split across multiple heads.\\\\n  - Each head processes the split parameters independently.\\\\n  - The outputs from all heads are combined to produce a final attention score.\\\\n  - This mechanism allows the model to capture richer interpretations and multiple relationships within the sequence, as different heads can focus on different aspects of the input[1][2].\\\\n\\\\n## Attention Score Calculation\\\\n- The attention score is calculated through the following steps:\\\\n  - Matrix multiplication between the Query (Q) and Key (K) matrices.\\\\n  - Adding a mask value to handle padding or future tokens in the sequence.\\\\n  - Scaling the result by dividing by the square root of the Query size.\\\\n  - Applying a Softmax function.\\\\n  - Performing another matrix multiplication between the Softmax output and the Value (V) matrix[1][4].\\\\n\\\\n## Specific Behaviour of Attention Heads\\\\n- As illustrated in the context, different attention heads can exhibit behaviour related to the structure of the sentence. For example, some heads might focus on specific grammatical or semantic aspects of the sentence, such as subject-verb relationships or contextual dependencies. This is evident from the examples given, where different heads from the encoder self-attention at layer 5 of 6 learned to perform different tasks, indicating that each head can specialize in different aspects of the input sequence[1].\",\"In the context of the Transformer model, particularly focusing on the attention mechanism, here are the key components and their functions:\\\\n\\\\n## Attention Mechanism\\\\nThe attention mechanism is a crucial part of the Transformer architecture, allowing the model to focus on specific parts of the input sequence when generating each element of the output sequence.\\\\n\\\\n## Types of Attention in Transformers\\\\n\\\\n### Self-Attention\\\\nSelf-attention, also known as intra-attention, is applied in both the encoder and decoder layers. It enables the model to consider different positions of the same sequence when computing the representation of a word. This helps capture long-range dependencies and relationships within the input sequence[2][4].\\\\n\\\\n- **Encoder Self-Attention**: In the encoder, self-attention allows the input sequence to pay attention to itself. Each word in the input sequence is represented by vectors (Query, Key, and Value) and these are used to compute attention scores that reflect the importance of each word relative to others in the sequence[1][2].\\\\n\\\\n- **Decoder Self-Attention**: In the decoder, self-attention allows the target sequence to pay attention to itself. Similar to the encoder, this helps in capturing relationships within the target sequence[1][2].\\\\n\\\\n### Encoder-Decoder Attention\\\\nEncoder-Decoder attention, or cross-attention, is used in the decoder layers. It allows the decoder to focus on relevant parts of the input sequence (encoded by the encoder) when generating each word of the output sequence. This ensures the decoder has access to the entire input sequence, aiding in producing more accurate and contextually appropriate translations[1][2].\\\\n\\\\n## Multi-Head Attention\\\\nMulti-head attention is an extension of self-attention and encoder-decoder attention. Here\\\\u2019s how it works:\\\\n\\\\n- **Query, Key, and Value Parameters**: The input sequence is represented by three parameters: Query (Q), Key (K), and Value (V). These parameters are split into multiple heads, each of which processes a portion of the input sequence independently[1][5].\\\\n\\\\n- **Parallel Computation**: Each head computes attention scores separately by performing matrix multiplications between Q and K, applying a mask if necessary, scaling the result, and then applying a Softmax function. The output of the Softmax is then multiplied by V to get the weighted sum of the value vectors[1].\\\\n\\\\n- **Combining Heads**: The outputs from all heads are combined to produce a final attention score. This multi-head approach allows the model to capture multiple types of relationships and nuances within the input sequence, as different heads can learn different aspects of the input data[1][5].\\\\n\\\\n## Significance of Multi-Head Attention\\\\n\\\\n- **Richer Interpretations**: By splitting the embedding vectors across multiple heads, each head can learn different aspects of the meanings of each word, allowing the model to capture richer interpretations of the sequence[1].\\\\n\\\\n- **Task Specialization**: Different heads can specialize in different tasks or focus on different parts of the input sequence, as illustrated in Figure 5 where different heads exhibit behavior related to the structure of the sentence[Context].\\\\n\\\\nIn summary, the attention mechanism in Transformers, particularly through multi-head attention, enables the model to focus on relevant parts of the input sequence, capture long-range dependencies, and learn multiple aspects of the input data, thereby improving the model&#x27;s performance and interpretability.\",\"The context provided does not specify individuals or entities referred to as \\\\\"they.\\\\\" The discussion is centered around the mechanics and components of the Transformer model, particularly the attention mechanism, without mentioning any specific people or groups.\\\\n\\\\nTo clarify, the text explains various aspects of the Transformer architecture, such as self-attention, encoder-decoder attention, and the computation of attention scores, but it does not introduce or refer to any individuals or groups[1][3][5].\",\"The context provided appears to be related to the analysis of attention mechanisms in the context of anaphora resolution, particularly within neural network architectures.\\\\n\\\\n### Key Points:\\\\n- The text mentions \\\\\"attention heads\\\\\" and \\\\\"layer 5 of 6,\\\\\" which are terms associated with transformer architectures in natural language processing (NLP)[2].\\\\n- The figure described shows attention patterns, specifically highlighting how certain attention heads focus on specific words, such as \\\\\"its,\\\\\" in the process of anaphora resolution.\\\\n- Anaphora resolution involves identifying the antecedent of an anaphor (typically a pronoun) in a text.\\\\n\\\\n### Detailed Answer:\\\\nThe context is discussing the mechanism of **attention in transformer models** as applied to **anaphora resolution**. Here\\\\u2019s a breakdown:\\\\n\\\\n- **Attention Heads**: These are components of the transformer architecture that allow the model to focus on different parts of the input data when generating output. In this case, the attention heads in layer 5 are highlighted as being involved in anaphora resolution[2].\\\\n- **Anaphora Resolution**: This is the task of identifying the antecedent (the noun or phrase that the pronoun refers to) of an anaphor (usually a pronoun) in a sentence. For example, in the sentence \\\\\"The Law will never be perfect, but its application should be just,\\\\\" the word \\\\\"its\\\\\" is an anaphor, and the antecedent is \\\\\"The Law\\\\\"[1][2].\\\\n- **Figure Description**: The figure illustrates how specific attention heads in the model focus sharply on the word \\\\\"its\\\\\" when resolving the anaphora. This indicates that the model is using these attention mechanisms to correctly identify the antecedent of the anaphor.\\\\n\\\\nIn summary, the context is about how transformer models use attention mechanisms to resolve anaphoras in text, specifically highlighting the role of attention heads in identifying the correct antecedents for pronouns.\",\"The context provided does not directly mention or explain the specific scores you are asking about, but it appears you are inquiring about evaluation metrics for machine translation models, given the mention of training settings and parameters.\\\\n\\\\nIn the context of machine translation, a common metric used to evaluate the quality of translations is the **BLEU score**.\\\\n\\\\n## What is a BLEU Score?\\\\n- BLEU (Bilingual Evaluation Understudy) is a metric that measures the quality of machine-translated text by comparing it to one or more human reference translations[2][4].\\\\n- The score ranges from 0 to 1, with 1 indicating a perfect match between the machine translation and the human reference translation. Often, BLEU scores are expressed on a scale of 1 to 100 for simplicity, but this should not be confused with percentage accuracy[1][4].\\\\n\\\\n## Interpretation of BLEU Scores\\\\n- Here is a general interpretation of BLEU scores:\\\\n  - **30-40**: Understandable to good translations\\\\n  - **40-50**: High quality translations\\\\n  - **50-60**: Very high quality, adequate, and fluent translations\\\\n  - **Above 60**: Quality often better than human translations[4].\\\\n\\\\n## Calculation\\\\n- BLEU scores are calculated by comparing the machine translation output with a set of high-quality reference translations. The comparison involves n-gram precisions and a brevity penalty to ensure the translation is neither too short nor too long compared to the reference[1][4].\\\\n\\\\nIn your context, if you were evaluating the performance of your machine translation model using the Penn Treebank or other corpora, the BLEU score would be an appropriate metric to assess how well the model&#x27;s translations match the human reference translations. However, the specific scores from your experiments are not provided in the given context.\",\"The context provided involves two distinct topics: the voting rights landscape in the United States and the self-attention mechanism in transformer models for natural language processing (NLP). Here\\\\u2019s a breakdown of why the option related to the self-attention mechanism might be relevant or selected:\\\\n\\\\n## Self-Attention Mechanism\\\\n- The self-attention mechanism is a key component of transformer models, which are widely used in NLP tasks. This mechanism allows the model to capture long-range dependencies within a sequence of tokens, such as words in a sentence[5].\\\\n- In the provided context, Figure 3 illustrates an example of the attention mechanism in action, specifically showing how different attention heads in layer 5 of a transformer model attend to distant dependencies. For instance, the attention heads focus on the verb \\\\\"making\\\\\" and its completion in the phrase \\\\\"making...more difficult\\\\\"[context].\\\\n\\\\n## Selection Reason\\\\n- The option might be selected because it demonstrates how advanced NLP models handle complex dependencies within sentences. This is particularly relevant when analyzing text that involves nuanced relationships between words, such as in the given sentence: \\\\\"It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.\\\\\"\\\\n- The visualization of attention heads attending to the verb \\\\\"making\\\\\" and its related context helps in understanding how the model interprets and connects different parts of the sentence, which is crucial for accurate comprehension and analysis.\\\\n\\\\nIn summary, the option was likely selected to highlight the capabilities of the self-attention mechanism in transformer models to handle long-range dependencies, which is essential for understanding complex sentences and their underlying relationships.\"]],[\"reference_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Many of the attention heads exhibit behavior that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The paper proposes a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Using positional embedding instead of sinusoids results in a BLEU score of 4.92 compared to 25.7.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The attention heads in layer 5 of 6 appear to be involved in anaphora resolution.\",\"The base models were trained for a total of 100,000 steps or 12 hours. The big models were trained for 300,000 steps (3.5 days).\",\"To evaluate the importance of different components of the Transformer, measuring the change in performance on English-to-German translation.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"The dot products are scaled by 1/\\\\u221adk to counteract the effect of large values of dk, which cause the dot products to grow large in magnitude and push the softmax function into regions where it has extremely small gradients.\",\"The Transformer achieved an F1 score of 92.7 in the semi-supervised setting.\",\"The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\",\"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\",\"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor.\",\"The base models were trained for a total of 100,000 steps or 12 hours.\",\"Self-attention layers are faster than recurrent layers because they require O(1) sequential operations, whereas a recurrent layer requires O(n) sequential operations.\",\"Reducing the attention key size dk hurts model quality.\",\"The BLEU score for the &#x27;big&#x27; Transformer model variation is 26.4.\",\"The task used to evaluate if the Transformer can generalize to other tasks was English constituency parsing.\",\"The Transformer (big) model achieves BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The big model achieved a BLEU score of 41.0.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The purpose of masking in the decoder&#x27;s self-attention layers is to prevent leftward information flow in the decoder to preserve the auto-regressive property by masking out (setting to \\\\u2212\\\\u221e) all values in the input of the softmax which correspond to illegal connections.\",\"The model achieved a new state of the art in the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"Dot-product attention includes a scaling factor of 1/\\\\u221adk, whereas additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\",\"The Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences, and it performs surprisingly well compared to RNN sequence-to-sequence models.\",\"The dimensionality of input and output is dmodel = 512.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"Self-attention layers are considered faster than recurrent layers in terms of computational complexity because they require O(1) executed operations, whereas a recurrent layer requires O(n) sequential operations.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"Figure 3 illustrates the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6, where many of the attention heads attend to a distant dependency of the verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019.\",\"The third sub-layer in the decoder performs multi-head attention over the output of the encoder stack, and the decoder ensures that predictions for position i depend only on known outputs at positions less than i by modifying the self-attention sub-layer to prevent positions from attending to subsequent positions, and the output embeddings are offset by one position.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu explored the limits of language modeling in a 2016 arXiv preprint, and the title of the work by Nal Kalchbrenner and colleagues is &#x27;Neural machine translation in linear time.&#x27;\",\"The authors of the paper &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, and the title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions.&#x27;\",\"Separable convolutions decrease the complexity considerably to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2), even though with k = n, the complexity is equal to the combination of a self-attention layer and a point-wise feed-forward layer; a side benefit of self-attention is that it could yield more interpretable models, as individual attention heads learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences.\",\"The main point expressed is that the law will never be perfect, but its application should be just, which is considered to be missing; Figure 4 illustrates two attention heads in layer 5 of 6, involved in anaphora resolution, showing full attentions for head 5 and isolated attentions from the word &#x27;its&#x27; for attention heads 5 and 6, with very sharp attentions noted for this word.\",\"Self-attention layers are faster than recurrent layers in terms of computational complexity when the sequence is considered, and recurrent layers require O(n) sequential operations.\",\"Google grants permission to reproduce the tables and figures in the paper solely for use in journalistic or scholarly works, provided proper attribution is given, and the main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The researchers chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training, and the three desiderata considered for using self-attention are the total computational complexity per layer, the amount of computation that can be parallelized, and the minimum number of sequential operations required.\",\"The main architectural feature of the Transformer model is that it eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization compared to models like Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks and have operations that grow with the distance between input or output positions.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, and in this work, 8 parallel attention layers or heads are employed.\",\"The Transformer model relies entirely on an attention mechanism to draw global dependencies between input and output.\",\"Structured attention networks.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"The standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\",\"The three factors mentioned are: the total computational complexity per layer, the amount of computation that can be parallelized (measured by the minimum number of sequential operations required), and the path length between long-range dependencies in the network.\",\"The Transformer uses multi-head attention in three different ways: 1) In &#x27;encoder-decoder attention&#x27; layers, where queries come from the previous decoder layer and the memory keys and values come from the output of the encoder. 2) In encoder self-attention layers, where all keys, values, and queries come from the output of the previous layer in the encoder. 3) In decoder self-attention layers, allowing each position in the decoder to attend to all positions in the decoder up to and including that position.\",\"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\",\"Neural machine translation by jointly learning to align and translate.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The sinusoidal version was chosen because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"]],[\"id\",[1,51,39,29,0,36,16,14,45,34,26,50,28,32,45,15,37,38,8,3,28,24,34,33,35,29,31,26,30,16,18,39,14,38,19,36,24,30,7,49,11,43,41,26,50,24,0,22,6,16,6,43,7,27,23,17,47,40,29,22]],[\"content\",[\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\"]],[\"color\",[\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1588\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1589\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1584\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1585\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1586\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1602\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1593\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1594\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1595\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1603\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1604\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1599\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1600\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1601\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1556\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1571\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1572\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1573\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1574\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1579\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1580\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1547\",\"attributes\":{\"renderers\":[{\"id\":\"p1587\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1566\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1567\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1568\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1569\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1561\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1562\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1563\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1564\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1565\",\"attributes\":{\"axis\":{\"id\":\"p1561\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1570\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1566\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1590\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1591\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1587\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1592\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1587\"}],\"index\":0}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1608\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1605\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1606\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1607\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"MsVdQNX8hkCz0L0/P0X4Pw==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JQ4OQWx710AVgvJANvYVQQ==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"81f471ba-3d94-44ef-951e-04b3fb96fcdd\",\"roots\":{\"p1613\":\"ff96a8a9-5eb3-418a-ba91-5df5193a289f\"},\"root_ids\":[\"p1613\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"ff96a8a9-5eb3-418a-ba91-5df5193a289f\" data-root-id=\"p1613\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x178167e63f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.8\n",
       "conversational               0.1\n",
       "distracting element          0.7\n",
       "double                       1.0\n",
       "simple                       0.9\n",
       "situational                  0.9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 60/60 [02:52<00:00,  2.87s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 60/60 [00:56<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def answer_fn(query, history=None):\n",
    "    retrieved_results = vectorstore.similarity_search(query)\n",
    "    context = retrieved_results[0].page_content\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant trained to answer based on provided context.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(context=context, input=query)},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-sonar-small-128k-online\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "from giskard.rag import evaluate\n",
    "report = evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"b2c4509d-df2f-43c8-8de8-2e7ac0f90426\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b2c4509d-df2f-43c8-8de8-2e7ac0f90426\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b2c4509d-df2f-43c8-8de8-2e7ac0f90426\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        72.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=72.00000000000001 class=\" progress-green \">72.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        60.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=60.00000000000001 class=\" progress-orange \">60.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        40.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=40.0 class=\" progress-orange \">40.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        62.5%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=62.5 class=\" progress-orange \">62.5%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">60%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">Focus on improving the rewriter component, given the 0.0 score on conversational questions, to enhance performance across topics related to conversational contexts. Additionally, revisit the knowledge base related to &#34;Self-Attention Mechanism&#34; and &#34;Transformer Model in Machine Translation&#34; to ensure more balanced coverage and retrieval capability.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"7e2051f0-d85b-45ab-8f2d-c6fd522cb56b\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1927\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1929\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1936\",\"attributes\":{\"factors\":[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1937\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1938\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1934\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1956\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1924\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1925\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1926\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"t23btm3bUUCqqqqqqqpQQAAAAAAAwEJAAAAAAADAUkAAAAAAAABEQA==\"},\"shape\":[5],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Attention Mechanisms in Neural Networks\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\"]],[\"colors\",[\"#18954e\",\"#72c264\",\"#a50026\",\"#006837\",\"#c62026\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1957\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1958\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1953\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1954\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1955\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1965\",\"attributes\":{\"data_source\":{\"id\":\"p1924\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1966\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1967\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1962\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1963\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1964\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1975\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1969\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1970\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1971\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1976\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1977\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1972\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1973\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1974\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1935\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1949\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1944\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1945\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1946\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1947\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1939\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1940\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1941\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1942\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1943\",\"attributes\":{\"axis\":{\"id\":\"p1939\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1948\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1944\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1968\",\"attributes\":{\"location\":60.0,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1978\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1979\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1975\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"7e2051f0-d85b-45ab-8f2d-c6fd522cb56b\",\"roots\":{\"p1927\":\"e23218de-a07f-42e2-8dca-62113f9e500e\"},\"root_ids\":[\"p1927\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"e23218de-a07f-42e2-8dca-62113f9e500e\" data-root-id=\"p1927\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"915122a9-83b5-4118-9d98-84098b6755ab\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1858\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1788\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1739\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1748\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1749\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1750\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1751\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1746\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1779\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1736\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1737\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1738\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Others\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"best models from the literature. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"1 Introduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol posi...\",\"sequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral ...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\",\"Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully conn...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"3.2 Attention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\",\"Scaled Dot-Product Attention\\\\n Multi-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1 Scaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values o...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2 \\\\u00b7 d) O(1) O(1)\\\\nRecurrent O(n \\\\u00b7 d2) O(n) O(n)\\\\nConvolutional O(k \\\\u00b7 n \\\\u00b7 d2) O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r...\",\"tokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\\\nwhere pos is the position and...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"length n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npa...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850, 2013.\\\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition, pages 770\\\\u2013778, 2016.\\\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient flow in\\\\nrecurre...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"2017.\\\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nIn International Conference on Learning Representations, 2017.\\\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722, 2017.\\\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"and interpretable tree annotation. In Proceedings of the 21st International Conference on\\\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\\\\u2013440. ACL, July\\\\n2006.\\\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859, 2016.\\\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\\\n[32] Noam Shazeer, Azalia Mirhoseini,...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"[37] Vinyals &amp; Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\\\nAdvances in Neural Information Processing Systems, 2015.\\\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144, 2016.\\\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. D...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#ff7f0e\",\"#999\",\"#ff7f0e\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ffbb78\",\"#ff7f0e\",\"#999\",\"#999\",\"#999\",\"#ff7f0e\",\"#ff7f0e\",\"#ff7f0e\",\"#999\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#aec7e8\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\",\"#999\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1780\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1781\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1776\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1777\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1778\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1747\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1762\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1763\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1764\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1765\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1766\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1771\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1772\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1757\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1758\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1759\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1760\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1752\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1753\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1754\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1755\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1756\",\"attributes\":{\"axis\":{\"id\":\"p1752\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1761\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1757\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1782\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1783\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Attention Mechanisms in Neural Networks\"},\"renderers\":[{\"id\":\"p1779\"}],\"index\":12}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1784\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1779\"}],\"index\":2}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1785\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Self-Attention Mechanism\"},\"renderers\":[{\"id\":\"p1779\"}],\"index\":9}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1786\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Learning\"},\"renderers\":[{\"id\":\"p1779\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1787\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Transformer Model in Machine Translation\"},\"renderers\":[{\"id\":\"p1779\"}],\"index\":27}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1857\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1793\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1802\",\"attributes\":{\"start\":0.06536439061164856,\"end\":8.679097175598145}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1803\",\"attributes\":{\"start\":4.770615100860596,\"end\":11.87248945236206}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1804\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1805\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1800\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1832\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1789\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1790\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1791\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xypyQKQPbT+/loBAXjtkQCckPUCjf7FA4MEFQNNf6z+IZrBAvrk1QCUcpz51hmg/5917QDGfTUCIZrBA1m8RQBGRnUBEzo1AdDQ/QKKnl0Dn3XtAInq1P765NUBPGY1ASbmjQF47ZEBI/IZAJRynPtjqeEDgwQVAyjv4P7+WgEDTX+s/RM6NQDBwAECjf7FAInq1P9jqeEDtYw9AKsy1PrZ/5T90BYlAA9JgQCUcpz51hmg/Inq1PyckPUDbqKg/NP9KQODBBUA0/0pAdAWJQO1jD0Ca3ZhA1uNZP9UPBUDxxIRAhJCKQF47ZEDbqKg/\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"LBDxQKMsF0FTqgZBt2HHQOjDDEHpHeNAhvEOQS9oIUFtlhJBGJzhQJRwAkH/Jw5BspvIQEZsz0BtlhJBg4UcQb9T60A0evxAJwQEQQD3CUGym8hA2VkGQRic4UBOib5ANQ7+QLdhx0D88N5AlHACQY1G20CG8Q5BYRTkQFOqBkEvaCFBNHr8QETHyEDpHeNA2VkGQY1G20AJPAFBnucTQVh47kCVbhRBcB4cQZRwAkH/Jw5B2VkGQejDDEGPK/ZAZRv5QIbxDkFlG/lAlW4UQQk8AUHYW9dAdV8CQYVhB0GBBStB4hEiQbdhx0CPK/ZA\"},\"shape\":[60],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Learning\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Others\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Transformer Model in Machine Translation\",\"Others\",\"Transformer Model in Machine Translation\",\"Attention Mechanisms in Neural Networks\",\"Self-Attention Mechanism\",\"Others\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Translation\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Others\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Self-Attention Mechanism\",\"Transformer Model in Machine Learning\",\"Attention Mechanisms in Neural Networks\",\"Transformer Model in Machine Learning\",\"Others\",\"Others\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\",\"Others\",\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\"]],[\"correctness\",[true,true,true,false,true,true,true,true,true,false,true,true,false,true,true,true,true,true,true,false,false,false,true,false,true,false,true,false,true,false,true,true,true,true,false,true,true,true,true,false,true,true,true,true,true,false,true,false,true,true,false,false,false,false,false,false,false,false,false,false]],[\"questions\",[\"What is the BLEU score achieved by the Transformer model on the WMT 2014 English-to-German translation task?\",\"What does the attention head behavior in the encoder self-attention at layer 5 of 6 indicate?\",\"What translation tasks did the model achieve a new state of the art on?\",\"What BLEU scores does the Transformer (big) model achieve for English-to-German and English-to-French translations?\",\"What new network architecture is proposed in the paper &#x27;Attention Is All You Need&#x27;?\",\"What is the size of the vocabulary used in the semi-supervised setting?\",\"How many parallel attention layers or heads are employed in the described model?\",\"What are the two most commonly used attention functions mentioned in the context?\",\"Who are the authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27;?\",\"What is the impact of using positional embedding instead of sinusoids according to Table 3?\",\"Under what circumstances might using self-attention in models provide an additional advantage, specifically in terms of model interpretability, and how does this relate to the tasks performed by individual attention heads?\",\"In the context of anaphora resolution, what specific function do the attention heads in the fifth of six layers appear to perform, particularly when analyzing the word &#x27;its&#x27;?\",\"Considering the hardware and hyperparameters used, what was the total duration required to fully train both the base models and the big models, and how does the step time differ between them?\",\"In the context of evaluating English-to-German translation performance, why is it significant to alter the base model in various ways, and what specific insights or outcomes does this variation aim to reveal in the study?\",\"Identify the individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27;, ensuring to include all authors as listed in the provided context.\",\"In the context of attention mechanisms, particularly when dealing with large values of dk, why is it necessary to scale the dot product by 1/\\\\u221adk, considering the impact on the softmax function and gradient behavior?\",\"Under the semi-supervised setting for English constituency parsing, what F1 score did the Transformer model achieve, and how does it compare to the scores of other models listed in the same context?\",\"Considering the context, what is the primary benefit of using the Transformer model over other models like RNNs or convolutional layers specifically for translation tasks, and how does it impact training speed and performance?\",\"In the context of transduction models, what is the fundamental mechanism that the Transformer model uniquely utilizes, avoiding traditional sequence-aligned RNNs or convolution methods?\",\"Identify the individuals who were instrumental in both the design and implementation phases of tensor2tensor, and specify any affiliations they had during this work.\",\"Considering the auto-regressive nature of the model&#x27;s decoder, how long did it take to train the base models, and how many steps were involved in this process?\",\"Considering the use of multi-head attention in Transformers, why do self-attention layers exhibit faster computational complexity compared to recurrent layers, especially when dealing with large input sequences?\",\"In the context of the Transformer architecture that relies solely on attention mechanisms, how does reducing the attention key size dk impact the model quality, and are there any specific conditions under which this effect is observed?\",\"Given the constraint of using the Section 22 development set for hyperparameter tuning, what is the BLEU score achieved by the &#x27;big&#x27; Transformer model variation on the English-to-German translation task?\",\"Under the condition that the evaluation involves a comparison with neural machine translation architectures, what specific task was employed to assess whether the Transformer model can generalize to other tasks?\",\"Under the condition that proper attribution is provided, could you specify the BLEU scores achieved by the Transformer (big) model for both English-to-German and English-to-French translations?\",\"Under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, what BLEU score did it achieve on the WMT 2014 English-to-French translation task?\",\"Considering the ability of self-attention mechanisms to follow long-distance dependencies, as demonstrated in attention visualizations, what is a potential side benefit of using self-attention in models?\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, what is the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task?\",\"Considering the model&#x27;s decoder configuration with its unique sub-layer structure, how many parallel attention layers or heads are implemented in this model?\",\"Hi, I&#x27;m studying for my computer science exam and I&#x27;m trying to understand the role of masking in the decoder&#x27;s self-attention layers. Could you explain what its purpose is?\",\"As an AI researcher working on attention-based models for translating text to audio, I&#x27;m curious to know what tasks the model achieved a new state of the art in?\",\"Hi, I&#x27;m going through my lecture notes on neural networks and I&#x27;m a bit confused. Could you explain the main difference between dot-product attention and additive attention? I need to understand this for my upcoming exam.\",\"Hi, I&#x27;m diving into the latest research on neural network models for my graduate studies. Can you explain how the Transformer model performs compared to RNN sequence-to-sequence models and the Berkeley-Parser, especially in the context of translating English into German and French?\",\"Hi, I&#x27;m trying to understand a neural network architecture from a research paper I&#x27;m studying late at night. Could you tell me what the dimensionality of the input and output is in the FFN described in the document?\",\"Hello, as I&#x27;m working on improving sentence parsing accuracy in machine translation systems, could you tell me what the size of the vocabulary is in the semi-supervised setting?\",\"Hi, I&#x27;m reviewing my notes for an upcoming exam and I&#x27;m a bit confused. Why are self-attention layers considered faster than recurrent layers in terms of computational complexity?\",\"Hi, I&#x27;m working on my thesis about advancements in natural language processing, and I&#x27;m analyzing different machine translation models. Could you tell me what BLEU score the big transformer model achieved on the WMT 2014 English-to-German translation task?\",\"Hi, I&#x27;m a computer science student trying to grasp how self-attention mechanisms enhance tasks such as reading comprehension and learning sentence representations. Can you explain what self-attention is and in which tasks it has been used successfully?\",\"Hi, I&#x27;m working on a research paper about changes in voter registration laws and how they impact electoral processes. Could you explain what Figure 3 illustrates about the attention mechanism in the encoder?\",\"What is the purpose of the third sub-layer in the decoder and how does the decoder ensure that predictions for position i depend only on known outputs at positions less than i?\",\"Who explored the limits of language modeling in a 2016 preprint and what is the title of the work by Nal Kalchbrenner and colleagues related to neural machine translation?\",\"Who are the authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; and what is the title of the paper authored by Francois Chollet in 2016?\",\"How do separable convolutions affect computational complexity in neural networks and what is a potential benefit of using self-attention in models?\",\"What is the main point expressed about the law in the context and what does Figure 4 illustrate regarding attention heads in layer 5?\",\"How does the computational complexity of self-attention layers compare to recurrent layers and what kind of operations do recurrent layers require?\",\"What permission does Google grant regarding the reproduction of tables and figures in the paper &#x27;Attention Is All You Need&#x27;, and what is the main innovation proposed in the paper?\",\"Why did the researchers choose the sinusoidal version of positional embeddings and what are the three desiderata considered for using self-attention?\",\"What is the main architectural feature of the Transformer model proposed in the work and how does it compare to other models like Extended Neural GPU, ByteNet, and ConvS2S in terms of parallelization?\",\"What is the purpose of multi-head attention in the model described and how many parallel attention layers or heads are employed?\",\"What is the main method for achieving this?\",\"What is it?\",\"What is it and where has it been applied?\",\"Which one was utilized?\",\"What are they?\",\"What are they?\",\"Who are they?\",\"What is it?\",\"What are those scores?\",\"Why was that option selected?\"]],[\"agent_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, which is an improvement over the existing best results, including ensembles, by over 2 BLEU points[3].\",\"The attention head behavior in the encoder self-attention at layer 5 of 6 indicates that the attention mechanism has learned to perform different tasks related to the structure of the sentence. This is evident from the examples provided, where two different heads from the encoder self-attention at layer 5 of 6 exhibit behavior that is clearly related to the structure of the sentence. This suggests that the attention heads are specialized to focus on different aspects of the input sequence, such as capturing local dependencies, long-range dependencies, or even specific semantic roles within the sentence[1][3].\\\\n\\\\nIn more detail, the self-attention mechanism in the encoder layer involves multiple attention heads, each of which computes attention weights independently. These heads can be thought of as performing different attention tasks, such as focusing on different parts of the sentence or capturing different types of relationships between words. The fact that many of the attention heads exhibit behavior related to the sentence structure indicates that they are effectively learning to attend to various aspects of the input, contributing to a more comprehensive understanding of the text[1][3].\",\"The Transformer model achieved a new state-of-the-art on two specific translation tasks:\\\\n\\\\n1. **English-to-German translation**: The model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over existing best results by over 2 BLEU points[3].\\\\n\\\\n2. **English-to-French translation**: The model established a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task after training for 3.5 days on eight GPUs[3].\",\"The context provided does not explicitly state the BLEU scores for the Transformer (big) model on the English-to-German translation task. However, it does mention that on the WMT 2014 English-to-French translation task, the Transformer (big) model achieves a BLEU score of 41.0[1].\\\\n\\\\nFor the English-to-German translation task, the context does not provide a specific BLEU score for the Transformer (big) model. It only mentions that the Transformer model achieves 28.4 BLEU on this task, but it does not specify whether this is for the \\\\\"big\\\\\" model or not[1]. \\\\n\\\\nTherefore, based on the given context:\\\\n- The Transformer (big) model achieves a BLEU score of 41.0 on the WMT 2014 English-to-French translation task.\\\\n- The BLEU score for the English-to-German task is not specified for the \\\\\"big\\\\\" model in the provided context. However, it is mentioned that the Transformer model achieves 28.4 BLEU on this task[1].\",\"The paper \\\\\"Attention Is All You Need\\\\\" proposes a new network architecture called the **Transformer**. This architecture is based solely on attention mechanisms, eliminating the need for recurrence and convolutions in sequence transduction models[2][5]. The Transformer model consists of two main components: the encoder and the decoder, both composed of multiple identical layers that utilize self-attention mechanisms and feed-forward neural networks to process and generate sequences of data efficiently[1][2].\",\"In the semi-supervised setting, the vocabulary used is 32K tokens.\",\"In the described model, 8 parallel attention layers or heads are employed[3].\",\"The two most commonly used attention functions mentioned in the context are:\\\\n\\\\n1. **Additive Attention**: This computes attention scores by applying a feed-forward neural network to the concatenated query and key vectors[1][2].\\\\n2. **Dot-Product Attention**: This measures attention scores using the dot product between the query and key vectors[1][2]. \\\\n\\\\nAdditionally, Scaled Dot-Product Attention is specifically mentioned, which is a particular implementation of dot-product attention where the scores are scaled down by dividing them by the square root of the dimension of the query and key vectors[2].\",\"The authors of the paper titled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"The provided context does not directly address the impact of using positional embedding instead of sinusoids based on Table 3. However, we can infer the general implications of positional embeddings from the given sources.\\\\n\\\\n### Impact of Positional Embeddings\\\\n\\\\n1. **Positional Awareness**: Positional embeddings, whether sinusoidal or another method, are crucial for capturing the order of tokens in a sequence. This is essential for models like Transformers to understand sequential data effectively[1][2][4].\\\\n\\\\n2. **Extrapolation Capabilities**: Different positional embedding methods have varying abilities to extrapolate beyond the training length. Sinusoidal embeddings have limited extrapolation capabilities, while methods like Rotary Positional Embedding (RoPE) and relative position embeddings perform better in this regard[4].\\\\n\\\\n3. **Model Performance**: The choice of positional embedding can affect model performance. For instance, sinusoidal embeddings might not be as effective for longer sequences or complex positional relationships, whereas more advanced methods like RoPE or relative position embeddings can improve performance in these scenarios[2][4].\\\\n\\\\n4. **Computational Cost**: Different positional embedding methods have different computational costs. For example, sinusoidal embeddings are generally simpler and faster, while more advanced methods like RoPE or T5 bias methods can be computationally expensive[4].\\\\n\\\\n### Specific Impact in Table 3\\\\n\\\\nGiven the lack of specific details about the impact of using positional embedding instead of sinusoids in Table 3, we can only speculate based on general knowledge. If we were to replace sinusoidal embeddings with another method (like RoPE or relative position embeddings) in Table 3, we might expect:\\\\n\\\\n- **Improved Extrapolation**: If the model is dealing with longer sequences or complex positional relationships, using a more advanced method like RoPE could potentially improve performance.\\\\n- **Computational Cost**: If the model is already computationally intensive, switching to a more complex method like RoPE might increase computational cost, which could affect training time and resource usage.\\\\n- **Model Quality**: The choice of positional embedding could influence model quality, especially if the model is sensitive to positional relationships. For example, if sinusoidal embeddings are not capturing the necessary positional information effectively, switching to a more advanced method could improve BLEU scores or other evaluation metrics.\\\\n\\\\nHowever, without specific data from Table 3, these are just hypothetical improvements based on the general characteristics of different positional embedding methods.\",\"Using self-attention in models can provide an additional advantage in terms of model interpretability under several circumstances:\\\\n\\\\n1. **Capturing Complex Dependencies**: Self-attention allows the model to capture long-range dependencies within a sequence, which is particularly useful in tasks like reading comprehension, abstractive summarization, and textual entailment. By identifying the most relevant parts of the input sequence, self-attention helps in understanding complex patterns and relationships, making the model&#x27;s decision-making process more transparent[1][3].\\\\n\\\\n2. **Multi-Head Attention**: The use of multi-head attention further enhances interpretability. Each attention head computes a different attention score, allowing the model to attend to information from different representation subspaces at different positions. This multi-head approach can be seen as a form of feature extraction, where each head focuses on different aspects of the input, making it easier to understand which specific features are contributing to the final output[4].\\\\n\\\\n3. **Contextual Understanding**: By attending to different parts of the input sequence, self-attention helps the model understand the context and assign appropriate weights to each element based on its relevance. This contextual understanding is crucial for tasks like sentiment analysis and question answering, where the model needs to consider the entire sentence or paragraph to provide accurate responses[1][3].\\\\n\\\\n4. **Task-Specific Interpretability**: Individual attention heads can be analyzed to understand which specific parts of the input are most relevant for a particular task. For example, in a sentiment analysis task, one attention head might focus on words related to positive sentiment, while another might focus on words related to negative sentiment. This task-specific interpretability helps in understanding how the model is making decisions and what features it is relying on for its predictions[5].\\\\n\\\\nIn summary, self-attention provides an additional advantage in terms of model interpretability by allowing the model to capture complex dependencies, utilize multi-head attention for feature extraction, understand contextual relationships, and provide task-specific insights into which parts of the input are most relevant. These benefits make self-attention a powerful tool for understanding how machine learning models make decisions, particularly in NLP tasks.\",\"In the context of anaphora resolution, the attention heads in the fifth of six layers appear to perform a specific function related to identifying and resolving pronouns, particularly when analyzing the word &#x27;its&#x27;. Here\\\\u2019s a step-by-step breakdown of how this might work:\\\\n\\\\n1. **Anaphora Identification**: The first step in anaphora resolution is identifying potential anaphors, which are words or phrases that refer back to a previously mentioned noun or pronoun. In this case, &#x27;its&#x27; is a pronoun that could be referring to a previously mentioned noun.\\\\n\\\\n2. **Attention Mechanism**: The attention mechanism in neural networks is used to focus on specific parts of the input data that are relevant to the task at hand. In the context of anaphora resolution, attention heads help to pinpoint the exact words or phrases that are likely to be anaphors.\\\\n\\\\n3. **Layer Analysis**: The fifth layer of a six-layer model is likely involved in processing and integrating information from earlier layers. This layer would have received input from the fourth layer and would be processing this information to make more refined decisions about anaphora resolution.\\\\n\\\\n4. **Word &#x27;its&#x27; Analysis**: When analyzing the word &#x27;its&#x27;, the attention heads in the fifth layer are very sharp, indicating that this word is crucial for resolving the anaphora. The sharp attentions suggest that the model is focusing intensely on &#x27;its&#x27; to determine whether it is referring back to a previously mentioned noun.\\\\n\\\\n5. **Resolution Process**: By focusing on &#x27;its&#x27;, the model can then compare it with potential antecedents (the nouns or pronouns it might be referring to) and make a decision about whether &#x27;its&#x27; is indeed an anaphor and what it is referring to. This process involves semantic and syntactic analysis to ensure that the resolution is accurate and coherent.\\\\n\\\\nIn summary, the attention heads in the fifth of six layers, particularly when analyzing the word &#x27;its&#x27;, appear to perform a critical function in identifying and resolving anaphors by focusing intensely on this pronoun to determine its correct antecedent. This sharp focus indicates that &#x27;its&#x27; is a key element in the anaphora resolution process, helping the model to make accurate decisions about pronoun references[1][3].\",\"To determine the total duration required to fully train both the base models and the big models, we need to consider the number of steps and the step time for each type of model.\\\\n\\\\n### Base Models\\\\n- **Number of Steps**: 100,000 steps\\\\n- **Step Time**: 0.4 seconds per step\\\\n- **Total Training Time**: \\\\\\\\(100,000 \\\\\\\\text{ steps} \\\\\\\\times 0.4 \\\\\\\\text{ seconds/step} = 40,000 \\\\\\\\text{ seconds}\\\\\\\\)\\\\n- **Total Training Time in Hours**: \\\\\\\\(40,000 \\\\\\\\text{ seconds} / 3600 \\\\\\\\text{ seconds/hour} = 11.11 \\\\\\\\text{ hours}\\\\\\\\)\\\\n\\\\n### Big Models\\\\n- **Number of Steps**: 300,000 steps\\\\n- **Step Time**: 1.0 second per step\\\\n- **Total Training Time**: \\\\\\\\(300,000 \\\\\\\\text{ steps} \\\\\\\\times 1.0 \\\\\\\\text{ second/step} = 300,000 \\\\\\\\text{ seconds}\\\\\\\\)\\\\n- **Total Training Time in Hours**: \\\\\\\\(300,000 \\\\\\\\text{ seconds} / 3600 \\\\\\\\text{ seconds/hour} = 83.33 \\\\\\\\text{ hours}\\\\\\\\)\\\\n\\\\n### Combined Training Time\\\\nThe total duration required to fully train both the base models and the big models is the sum of their individual training times:\\\\n\\\\\\\\[11.11 \\\\\\\\text{ hours} + 83.33 \\\\\\\\text{ hours} = 94.44 \\\\\\\\text{ hours}\\\\\\\\]\\\\n\\\\n### Step Time Difference\\\\nThe step time differs significantly between the base models and the big models:\\\\n- **Base Models**: 0.4 seconds per step\\\\n- **Big Models**: 1.0 second per step\\\\n\\\\nThis indicates that training a big model takes twice as long as training a base model per step.\",\"In the context of evaluating English-to-German translation performance, altering the base Transformer model in various ways is significant for several reasons:\\\\n\\\\n1. **Understanding Component Importance**: By varying the base model, researchers aim to understand the relative importance of different components of the Transformer architecture. This includes modifying the number of layers, testing various regularization techniques, and evaluating the optimal number of heads for attention[1][3].\\\\n\\\\n2. **Performance Optimization**: The goal is to identify which components contribute most significantly to the translation performance. For instance, the choice of subword model, such as using SentencePiece models with unigram and BPE approaches, is shown to be a major driver of translation performance[1][3].\\\\n\\\\n3. **Efficiency and Cost Analysis**: By measuring the change in performance with different variations, researchers can estimate the number of floating point operations used to train the model, which helps in comparing training costs and efficiency across different model architectures[4].\\\\n\\\\n4. **Comparative Analysis**: This approach allows for a comparative analysis with other model architectures from the literature, providing insights into how different design choices affect translation quality and training costs[4].\\\\n\\\\n5. **Practical Applications**: The findings from these variations can be applied to real-world scenarios, such as improving the performance of translation engines and reducing post-editing efforts. For example, a Transformer optimized model demonstrated a BLEU score improvement of 7.8 points when compared with a baseline RNN model, indicating substantial improvements in translation quality[1][3].\\\\n\\\\nIn summary, altering the base model in various ways helps in understanding the critical components of the Transformer architecture, optimizing performance, analyzing efficiency and costs, and providing practical insights for improving translation quality.\",\"The individuals credited as the creators of the research paper entitled &#x27;A deep reinforced model for abstractive summarization&#x27; are Romain Paulus, Caiming Xiong, and Richard Socher.\",\"In the context of attention mechanisms, particularly when dealing with large values of \\\\\\\\(dk\\\\\\\\), it is necessary to scale the dot product by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\) to counteract the effect of large dot products on the softmax function and gradient behavior. Here\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Dot Product Growth**: When the dimensions of the keys (\\\\\\\\(k\\\\\\\\)) and queries (\\\\\\\\(q\\\\\\\\)) are large (\\\\\\\\(dk\\\\\\\\)), the dot product \\\\\\\\(q \\\\\\\\cdot k\\\\\\\\) can grow very large in magnitude. This is because the dot product is the sum of the products of corresponding elements of \\\\\\\\(q\\\\\\\\) and \\\\\\\\(k\\\\\\\\), and as \\\\\\\\(dk\\\\\\\\) increases, this sum can become very large[3].\\\\n\\\\n2. **Softmax Function Sensitivity**: The softmax function is used to normalize the dot products to obtain attention weights. However, when the dot products are very large, the softmax function can be pushed into regions where it has extremely small gradients. This makes it difficult for the model to learn effectively because the gradients of the loss function with respect to the model parameters become very small[3].\\\\n\\\\n3. **Scaling the Dot Product**: To mitigate this issue, the dot products are scaled by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\). This scaling factor reduces the magnitude of the dot products, preventing them from becoming too large and thus avoiding the softmax function from being pushed into regions with small gradients[3].\\\\n\\\\n4. **Improved Gradient Behavior**: By scaling the dot products, the softmax function operates in a more stable region, ensuring that the gradients of the loss function remain reasonable and useful for training the model. This improves the overall performance and stability of the attention mechanism, especially when dealing with large input dimensions[3].\\\\n\\\\nIn summary, scaling the dot product by \\\\\\\\(1/\\\\\\\\sqrt{dk}\\\\\\\\) helps to maintain a stable and effective softmax function, which is crucial for obtaining meaningful attention weights and ensuring that the gradients used for training are reasonable and useful.\",\"Under the semi-supervised setting for English constituency parsing, the Transformer model achieved an F1 score of 92.7. This score is higher than most of the other models listed in the context, which include:\\\\n\\\\n- Vinyals &amp; Kaiser et al. (2014) with a semi-supervised F1 score of 92.1\\\\n- Zhu et al. (2013) with a semi-supervised F1 score of 91.3\\\\n- Huang &amp; Harper (2009) with a semi-supervised F1 score of 91.3\\\\n- McClosky et al. (2006) with a semi-supervised F1 score of 92.1\\\\n\\\\nOnly Luong et al. (2015) achieved a higher F1 score of 93.0 in the multi-task setting, but not in the semi-supervised setting. Therefore, the Transformer model&#x27;s semi-supervised F1 score of 92.7 is the highest among the semi-supervised models listed[1][3].\",\"The primary benefit of using the Transformer model over other models like RNNs or convolutional layers for translation tasks is its ability to handle complex dependencies between input words efficiently. Here are the key points from the context:\\\\n\\\\n1. **Attention Mechanism**: The Transformer model is based solely on attention mechanisms, which allow it to focus on previously generated tokens and develop a long memory. This is particularly useful in language translation tasks where understanding the context of words is crucial[3][4].\\\\n\\\\n2. **Parallelization**: The Transformer model is more parallelizable than traditional models. This means that it can process multiple words simultaneously, significantly reducing the training time compared to sequential processing in RNNs[3][4].\\\\n\\\\n3. **Performance Improvement**: The Transformer model demonstrates superior quality in translation tasks. For example, on the WMT 2014 English-to-German translation task, it achieved a BLEU score of 28.4, improving over existing best results by over 2 BLEU. On the WMT 2014 English-to-French translation task, it established a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs[3].\\\\n\\\\n4. **Training Speed**: The Transformer model requires significantly less time to train compared to traditional models. This is due to its parallelizable nature and the efficiency of the attention mechanism[3][4].\\\\n\\\\nIn summary, the Transformer model&#x27;s primary benefits for translation tasks include its ability to handle complex dependencies through attention mechanisms, its high parallelizability which reduces training time, and its superior performance in achieving high BLEU scores on various translation tasks.\",\"In the context of transduction models, the fundamental mechanism that the Transformer model uniquely utilizes, avoiding traditional sequence-aligned RNNs or convolution methods, is the **self-attention mechanism**. This mechanism allows the Transformer to weigh the importance of different parts of the input sequence differently, capturing dependencies regardless of their distance within the sequence. This is in contrast to traditional RNNs, which process data sequentially and struggle with long-range dependencies[1][2][5]. The self-attention mechanism enables the Transformer to process the entire sequence in parallel, making it highly efficient and effective for tasks involving sequential data[1][2][5].\",\"The individuals who were instrumental in both the design and implementation phases of tensor2tensor are Lukasz Kaiser and Aidan. During this work, they were affiliated with Google Brain and Google Research, respectively.\\\\n\\\\n- **Lukasz Kaiser**: Affiliated with Google Brain.\\\\n- **Aidan**: Affiliated with Google Research.\\\\n\\\\nTheir contributions included designing various parts of tensor2tensor, implementing it, and replacing the earlier codebase, which significantly improved results and accelerated research[2][4].\",\"To answer the question, we need to focus on the information provided about the training process of the base models.\\\\n\\\\n1. **Training Time**: The base models were trained for a total of 12 hours.\\\\n2. **Training Steps**: Each training step took about 0.4 seconds.\\\\n3. **Total Steps**: To find the total number of steps, we can use the formula:\\\\n   \\\\\\\\[\\\\n   \\\\\\\\text{Total Steps} = \\\\\\\\frac{\\\\\\\\text{Total Time}}{\\\\\\\\text{Time per Step}}\\\\n   \\\\\\\\]\\\\n   Given that the total time is 12 hours and the time per step is 0.4 seconds, we convert 12 hours to seconds:\\\\n   \\\\\\\\[\\\\n   12 \\\\\\\\text{ hours} = 12 \\\\\\\\times 3600 \\\\\\\\text{ seconds} = 43,200 \\\\\\\\text{ seconds}\\\\n   \\\\\\\\]\\\\n   Then, we calculate the total number of steps:\\\\n   \\\\\\\\[\\\\n   \\\\\\\\text{Total Steps} = \\\\\\\\frac{43,200}{0.4} = 108,000\\\\n   \\\\\\\\]\\\\n\\\\nSo, it took approximately 108,000 steps to train the base models, and this process took 12 hours.\\\\n\\\\nIn summary:\\\\n- **Training Time**: 12 hours\\\\n- **Total Steps**: 108,000 steps\",\"To understand why self-attention layers, particularly those using multi-head attention, exhibit faster computational complexity compared to recurrent neural network (RNN) layers, especially when dealing with large input sequences, let&#x27;s break down the key points:\\\\n\\\\n1. **Self-Attention Mechanism**:\\\\n   - **Parallel Processing**: Self-attention mechanisms compute attention scores for all tokens in parallel. This means that for a sequence of length \\\\\\\\( N \\\\\\\\), the computation is done simultaneously across all tokens, resulting in a complexity of \\\\\\\\( O(N^2 \\\\\\\\cdot d_{model}) \\\\\\\\) for scaled dot-product attention[1][3].\\\\n   - **Linear Complexity**: The parallel computation allows self-attention to scale linearly with the number of tokens, making it more efficient for large sequences.\\\\n\\\\n2. **Recurrent Neural Networks (RNNs)**:\\\\n   - **Sequential Processing**: RNNs process the sequence sequentially, where each time step&#x27;s output depends on the previous time step. This sequential processing requires \\\\\\\\( O(n) \\\\\\\\) operations for each token in the sequence, leading to a higher computational complexity as the sequence length increases[2][4].\\\\n   - **Backpropagation Through Time (BPTT)**: RNNs use BPTT to update parameters, which involves summing errors at each time step. This process is more computationally intensive and less parallelizable compared to self-attention[2][4].\\\\n\\\\n3. **Multi-Head Attention**:\\\\n   - **Increased Expressiveness**: Multi-head attention extends the single-head attention by running multiple attention heads in parallel. This allows the model to capture different types of relationships and patterns within the input data simultaneously, enhancing its expressive power[3].\\\\n   - **Computational Complexity**: Despite the increased expressiveness, the overall complexity of multi-head attention remains \\\\\\\\( O(N^2 \\\\\\\\cdot d_{model} + N \\\\\\\\cdot d_{model}^2) \\\\\\\\). The parallel computation across multiple heads does not significantly alter the linear complexity with respect to the sequence length[1][3].\\\\n\\\\nIn summary, self-attention layers, including those using multi-head attention, exhibit faster computational complexity compared to recurrent layers when dealing with large input sequences because they perform parallel computations across all tokens simultaneously. This parallel processing allows self-attention to scale more efficiently with the sequence length, making it more suitable for handling large datasets. In contrast, RNNs process sequences sequentially, which leads to higher computational complexity as the sequence length increases.\",\"In the context of the Transformer architecture, reducing the attention key size `dk` can impact the model quality in several ways, particularly in terms of its ability to capture long-range dependencies and its computational efficiency. Here\\\\u2019s a step-by-step analysis:\\\\n\\\\n1. **Capturing Long-Range Dependencies:**\\\\n   - The Transformer&#x27;s self-attention mechanism relies on scaled dot-product attention to weigh the importance of different words in a sentence. This mechanism allows the model to capture relationships between words that are far apart in a sequence, which is crucial for tasks like machine translation and text summarization[2][5].\\\\n   - Reducing the attention key size `dk` can limit the model&#x27;s ability to capture these long-range dependencies. A smaller key size means that the model has fewer dimensions to project the input embeddings into, which can result in a reduced capacity to encode contextual information across the entire sequence[2][5].\\\\n\\\\n2. **Computational Efficiency:**\\\\n   - While reducing `dk` might seem like a way to decrease computational complexity, it actually has a more nuanced effect. The computational cost of self-attention is primarily driven by the quadratic relationship between sequence length and attention computation, not directly by the key size. However, smaller key sizes can lead to less effective attention weights, which might necessitate more iterations or layers to achieve similar performance, potentially increasing overall computational requirements[3][5].\\\\n\\\\n3. **Specific Conditions:**\\\\n   - The impact of reducing `dk` is more pronounced in scenarios where the model needs to capture complex, long-range dependencies. For example, in tasks requiring detailed contextual understanding such as machine translation or text summarization, a smaller key size could lead to suboptimal performance.\\\\n   - On the other hand, if the task involves shorter sequences or simpler dependencies, the effect of reducing `dk` might be less noticeable. However, even in these cases, the model&#x27;s ability to generalize and adapt to unseen data could still be compromised due to the reduced capacity for contextual encoding[2][5].\\\\n\\\\nIn summary, reducing the attention key size `dk` in the Transformer architecture can negatively impact the model&#x27;s ability to capture long-range dependencies and may lead to suboptimal performance in tasks requiring complex contextual understanding. This effect is more pronounced in scenarios where detailed contextual information is crucial, such as machine translation or text summarization.\",\"The &#x27;big&#x27; Transformer model variation achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, as reported in the context provided[1]. This score was established as a new state-of-the-art BLEU score, surpassing all previously reported models and ensembles[1].\",\"The specific task employed to assess whether the Transformer model can generalize to other tasks, as mentioned in the context, is **English constituency parsing**. This task was used to demonstrate the Transformer&#x27;s ability to generalize well beyond machine translation, even with both large and limited training data[3].\",\"Based on the provided context, the Transformer (big) model achieved a BLEU score of 41.8 on the WMT 2014 English-to-French translation task[1]. However, there is no specific mention of the BLEU score for the English-to-German translation task in the provided context.\\\\n\\\\nTo summarize:\\\\n- **English-to-French**: The Transformer (big) model achieved a BLEU score of 41.8[1].\\\\n- **English-to-German**: No specific BLEU score is mentioned in the provided context.\",\"Under the condition that the big model&#x27;s training cost is less than 2.5 \\\\u00b7 10^19 FLOPs, the big model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task[1]. This is in line with the provided context, which states that the big model outperformed all previously published single models with a BLEU score of 41.0, despite being trained at a significantly lower cost compared to the previous state-of-the-art model.\",\"Considering the ability of self-attention mechanisms to follow long-distance dependencies, a potential side benefit of using self-attention in models is **improved contextual understanding**. Self-attention allows the model to weigh the importance of different parts of the input sequence by attending to itself, which enables it to capture complex patterns and relationships between distant elements in a sequence. This capability enhances the model&#x27;s ability to understand the context of the input data, making it more effective in tasks such as natural language processing and computer vision[1][3][5].\\\\n\\\\nAdditionally, self-attention mechanisms can also provide **better parallel computation efficiency**. Since self-attention can be computed in parallel for each element in the sequence, it makes the model computationally efficient and scalable for large datasets[1][4]. This parallel computation capability further supports the model&#x27;s ability to handle complex tasks with long-range dependencies.\\\\n\\\\nLastly, self-attention&#x27;s ability to **capture long-range dependencies** also implies that it can **resolve ambiguities** by attending to informative parts of the sentence. This is particularly useful in tasks like machine translation, text summarization, and question answering, where understanding the context and relationships between different parts of the text is crucial[3][5].\",\"Under the condition that the model&#x27;s parameters remained unchanged from the English-to-German base translation model, the BLEU score achieved by the big transformer model on the WMT 2014 English-to-German translation task is 28.4[1].\",\"The model&#x27;s decoder configuration, as described, includes a third sub-layer that performs multi-head attention over the output of the encoder stack. The multi-head attention mechanism is a key component in this sub-layer, which allows the model to attend to different parts of the sequence in parallel.\\\\n\\\\nIn the context of multi-head attention, the number of parallel attention layers or heads is typically denoted by the variable `h` in the formula:\\\\n\\\\n\\\\\\\\[ \\\\\\\\text{MultiHead}(\\\\\\\\textbf{Q}, \\\\\\\\textbf{K}, \\\\\\\\textbf{V}) = \\\\\\\\left[\\\\\\\\text{head}_{1},\\\\\\\\dots,\\\\\\\\text{head}_{h}\\\\\\\\right]\\\\\\\\textbf{W}_{0} \\\\\\\\]\\\\n\\\\nHowever, the specific number of heads (`h`) is not explicitly mentioned in the provided context. Typically, in standard transformer architectures, the number of attention heads (`h`) is set to a value such as 8 or 12, but this can vary depending on the specific implementation and requirements of the model.\\\\n\\\\nGiven the lack of specific information about the number of heads (`h`) in the provided context, we cannot definitively state how many parallel attention layers or heads are implemented in this model. However, it is common for transformer models to use multiple attention heads (e.g., 8 or 12) to capture a wide range of dependencies in the input sequence[1][2][4].\",\"Certainly The purpose of masking in the decoder&#x27;s self-attention layers in a Transformer model is to prevent the model from attending to subsequent positions in the sequence. This is crucial for ensuring that the predictions for a given position `i` can only depend on the known outputs at positions less than `i`.\\\\n\\\\nHere\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Self-Attention Mechanism**: In the decoder, each layer includes a self-attention mechanism that allows different parts of the input sequence to interact with each other. This mechanism is designed to capture dependencies within the sequence.\\\\n\\\\n2. **Masking**: To prevent the model from attending to future positions, a masking technique is applied. This involves setting the weights of the attention mechanism to zero for positions that have not yet been processed.\\\\n\\\\n3. **Output Embeddings Offset**: Additionally, the output embeddings are offset by one position. This means that the model can only use the embeddings of previous positions when making predictions for the current position.\\\\n\\\\n4. **Dependency on Known Outputs**: By masking and offsetting the embeddings, the model ensures that it can only depend on the known outputs at positions less than the current position `i`. This prevents the model from using future information that has not yet been generated, which is essential for generating coherent and contextually relevant outputs.\\\\n\\\\nIn summary, masking in the decoder&#x27;s self-attention layers is essential for maintaining the causal flow of information in sequence-to-sequence tasks, ensuring that each position&#x27;s prediction is based only on previously generated outputs[4].\",\"The Transformer model, as described in the context, achieved a new state of the art in several tasks, particularly in machine translation. Specifically, it demonstrated superior performance on the following tasks:\\\\n\\\\n1. **English-to-German Translation**: The model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, which is an improvement over existing best results, including ensembles, by over 2 BLEU points[3].\\\\n\\\\n2. **English-to-French Translation**: On the WMT 2014 English-to-French translation task, the model established a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, significantly reducing training costs compared to the best models from the literature[3].\\\\n\\\\n3. **English Constituency Parsing**: The Transformer model also generalized well to other tasks by successfully applying it to English constituency parsing, both with large and limited training data[3].\\\\n\\\\nThese achievements highlight the Transformer&#x27;s effectiveness in handling complex sequence transduction tasks, particularly in machine translation and parsing tasks.\",\"Certainly Let&#x27;s break down the main differences between dot-product (scaled dot-product) attention and additive attention:\\\\n\\\\n### Dot-Product Attention\\\\n1. **Calculation**:\\\\n   - **Formula**: \\\\\\\\( \\\\\\\\text{Attention}(Q, K, V) = \\\\\\\\text{softmax}\\\\\\\\left(\\\\\\\\frac{QK^T}{\\\\\\\\sqrt{d_k}}\\\\\\\\right)V \\\\\\\\)\\\\n   - **Explanation**: The attention weights are calculated by taking the dot product of the query matrix \\\\\\\\(Q\\\\\\\\) and the key matrix \\\\\\\\(K\\\\\\\\), then dividing by the square root of the dimension of the key matrix \\\\\\\\(d_k\\\\\\\\). The softmax function is applied to normalize the weights[3][4].\\\\n\\\\n2. **Implementation**:\\\\n   - **Efficiency**: It is highly optimized for matrix multiplication, making it faster and more space-efficient in practice[3].\\\\n\\\\n3. **Usage**:\\\\n   - **Common Usage**: This is the most commonly used type of attention in transformer architecture, particularly in self-attention mechanisms[1][3].\\\\n\\\\n### Additive Attention\\\\n1. **Calculation**:\\\\n   - **Formula**: The attention weights are computed by taking the dot product of the query vector with the key vector and passing it through a feed-forward neural network (FFNN) with a single hidden layer[2][4].\\\\n   - **Explanation**: The compatibility function is computed using an FFNN, which produces a scalar value that is used as the attention weight. This scalar value is then used to weigh the value vector and compute a weighted sum[1][2].\\\\n\\\\n2. **Implementation**:\\\\n   - **Complexity**: While theoretically similar in complexity, additive attention involves an additional FFNN layer, which can make it less efficient in terms of computational resources compared to dot-product attention[3].\\\\n\\\\n3. **Usage**:\\\\n   - **Specific Scenarios**: Additive attention is less widely used but can be beneficial in scenarios where a non-linear similarity measure between the query and key vectors is required, such as in some conversational models[1][2].\\\\n\\\\n### Summary\\\\n- **Dot-Product Attention**:\\\\n  - Fast and efficient due to optimized matrix multiplication.\\\\n  - Commonly used in transformer architecture.\\\\n  - Uses softmax normalization.\\\\n\\\\n- **Additive Attention**:\\\\n  - Computes compatibility using an FFNN.\\\\n  - More complex due to additional neural network layer.\\\\n  - Useful for non-linear similarity measures.\\\\n\\\\nUnderstanding these differences will help you better grasp the concepts and prepare for your exam.\",\"The Transformer model significantly outperforms RNN sequence-to-sequence models and the Berkeley-Parser in translation tasks, particularly in the context of translating English into German and French. Here\\\\u2019s a step-by-step explanation of the differences and advantages of the Transformer model:\\\\n\\\\n1. **Architecture**:\\\\n   - **Transformer vs. RNN**: The Transformer model uses a self-attention mechanism to weigh the significance of every word in a sentence simultaneously, allowing it to capture relationships and dependencies without sequential processing like in traditional RNNs (Recurrent Neural Networks) [1][5].\\\\n   - **Self-Attention Mechanism**: Unlike RNNs, which process data sequentially and struggle with long-term dependencies due to the vanishing gradient problem, the Transformer model can consider the entire sequence when computing attention scores. This enables it to capture relationships between distant words more effectively [1][5].\\\\n\\\\n2. **Parallel Processing**:\\\\n   - **Efficiency**: The Transformer model can process and analyze input data in parallel, making it faster and more efficient than sequential models like RNNs. This parallel processing capability drastically reduces the time required for training and inference, especially for large datasets [5].\\\\n\\\\n3. **Performance in Translation Tasks**:\\\\n   - **WSJ Training Set**: In the context of translating English into German and French, the Transformer model outperforms the Berkeley-Parser even when trained only on the WSJ training set of 40K sentences. This indicates its robustness and efficiency in handling translation tasks with limited training data [context].\\\\n\\\\n4. **State-of-the-Art Results**:\\\\n   - **WMT 2014 Tasks**: On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the Transformer achieves a new state of the art. This demonstrates its superior performance in real-world translation tasks compared to previous models [context].\\\\n\\\\n5. **Comparison with RNN Grammar**:\\\\n   - **Surprising Performance**: Despite not being task-specifically tuned, the Transformer model performs surprisingly well, yielding better results than all previously reported models except for the Recurrent Neural Network Grammar [context].\\\\n\\\\nIn summary, the Transformer model&#x27;s use of self-attention mechanisms and parallel processing capabilities allows it to outperform RNN sequence-to-sequence models and the Berkeley-Parser in translation tasks, particularly in the context of translating English into German and French. Its ability to capture long-term dependencies and process data efficiently makes it a superior choice for NLP tasks.\",\"The dimensionality of the input and output in a Feed-Forward Network (FFN) described in the context of neural network complexity analysis can vary depending on the specific architecture and application. However, a typical FFN layer, such as a dense layer, generally takes an input of size \\\\\\\\(n \\\\\\\\times i\\\\\\\\) (where \\\\\\\\(n\\\\\\\\) is the batch size and \\\\\\\\(i\\\\\\\\) is the number of features in the input vector) and produces an output of size \\\\\\\\(n \\\\\\\\times o\\\\\\\\) (where \\\\\\\\(o\\\\\\\\) is the number of neurons in the layer) [1].\\\\n\\\\nFor example, in a dense layer, the input is a matrix \\\\\\\\(\\\\\\\\mathbf{x}\\\\\\\\) of size \\\\\\\\(n \\\\\\\\times i\\\\\\\\), and the output is a matrix \\\\\\\\(\\\\\\\\mathbf{y}\\\\\\\\) of size \\\\\\\\(n \\\\\\\\times o\\\\\\\\), where each element \\\\\\\\(y_{ji}\\\\\\\\) is computed as \\\\\\\\(y_{ji} = w_{ji}x_j + b_j\\\\\\\\), with \\\\\\\\(w_{ji}\\\\\\\\) being the weight connecting the \\\\\\\\(i\\\\\\\\)-th feature of the \\\\\\\\(j\\\\\\\\)-th input to the \\\\\\\\(j\\\\\\\\)-th neuron in the layer, and \\\\\\\\(b_j\\\\\\\\) being the bias term for the \\\\\\\\(j\\\\\\\\)-th neuron [1].\\\\n\\\\nTherefore, the dimensionality of the input and output in a typical FFN layer is:\\\\n\\\\n- **Input**: \\\\\\\\(n \\\\\\\\times i\\\\\\\\)\\\\n- **Output**: \\\\\\\\(n \\\\\\\\times o\\\\\\\\)\\\\n\\\\nWhere \\\\\\\\(n\\\\\\\\) is the batch size, \\\\\\\\(i\\\\\\\\) is the number of features in the input vector, and \\\\\\\\(o\\\\\\\\) is the number of neurons in the layer.\",\"In the semi-supervised setting, the vocabulary size is 32K tokens[5]. This is mentioned in the context provided, where it is noted that a vocabulary of 32K tokens is used for the semi-supervised setting, as opposed to 16K tokens for the WSJ only setting.\",\"Self-attention layers are considered faster than recurrent layers in terms of computational complexity for several reasons, which can be broken down step by step:\\\\n\\\\n1. **Parallel Computation**:\\\\n   - **Self-Attention**: Self-attention mechanisms allow for parallel computation. Each token in the sequence can attend to all other tokens simultaneously, which means that the computation can be parallelized across all tokens. This is because the attention mechanism computes compatibility scores between all pairs of tokens and then aggregates these scores to produce the final output for each token. This parallelization reduces the overall computational complexity[1][5].\\\\n\\\\n   - **Recurrent Layers**: Recurrent Neural Networks (RNNs), on the other hand, process sequences sequentially. Each time step depends on the previous time step, which means that the computation is sequential. This sequential processing makes it harder to parallelize the operations, leading to longer training times and higher computational complexity[2][5].\\\\n\\\\n2. **Computational Complexity**:\\\\n   - **Self-Attention**: The computational complexity of self-attention is generally \\\\\\\\(\\\\\\\\mathcal{O}(n^2d)\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the length of the input sequence and \\\\\\\\(d\\\\\\\\) is the dimension of the input. This is because the attention mechanism involves matrix multiplications and dot products that scale quadratically with the sequence length[1][3].\\\\n\\\\n   - **Recurrent Layers**: The computational complexity of recurrent layers is typically \\\\\\\\(\\\\\\\\mathcal{O}(nd^2)\\\\\\\\), where \\\\\\\\(n\\\\\\\\) is the sequence length and \\\\\\\\(d\\\\\\\\) is the dimension of the input. This is because each time step involves matrix multiplications and updates that scale linearly with the sequence length[2][5].\\\\n\\\\n3. **Sequential Operations**:\\\\n   - **Self-Attention**: Self-attention layers have \\\\\\\\(\\\\\\\\mathcal{O}(1)\\\\\\\\) sequential operations. This means that the number of sequential operations required to compute the attention is constant and does not depend on the sequence length, allowing for efficient parallelization[1].\\\\n\\\\n   - **Recurrent Layers**: Recurrent layers have \\\\\\\\(\\\\\\\\mathcal{O}(n)\\\\\\\\) sequential operations. Each time step depends on the previous time step, which means that the number of sequential operations scales linearly with the sequence length, making it harder to parallelize and increasing computational complexity[2][5].\\\\n\\\\nIn summary, self-attention layers are faster than recurrent layers in terms of computational complexity because they allow for parallel computation, have a lower number of sequential operations, and their computational complexity scales more efficiently with the sequence length.\",\"The Transformer model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, which is an improvement over the existing best results, including ensembles, by over 2 BLEU points[3].\",\"Self-attention is a mechanism used in machine learning, particularly in natural language processing (NLP) and computer vision tasks, to capture dependencies and relationships within input sequences. It allows the model to identify and weigh the importance of different parts of the input sequence by attending to itself.\\\\n\\\\n### How Self-Attention Works\\\\n\\\\n1. **Transformation into Query, Key, and Value Vectors**: The input sequence is transformed into three vectors: query, key, and value. These vectors are obtained through linear transformations of the input[1][2].\\\\n\\\\n2. **Attention Mechanism Calculation**: The attention mechanism calculates a weighted sum of the values based on the similarity between the query and key vectors. This weighted sum, along with the original input, is then passed through a feed-forward neural network to produce the final output[1].\\\\n\\\\n3. **Multi-Head Attention**: To improve the model&#x27;s ability to capture different aspects of the input sequence, multi-head attention is used. This involves linearly projecting the set of queries, keys, and values multiple times with different linear projections, allowing the model to attend to information from different representation subspaces at different positions[2][4].\\\\n\\\\n### Tasks Where Self-Attention Has Been Used Successfully\\\\n\\\\n1. **Natural Language Processing (NLP)**:\\\\n   - **Reading Comprehension**: Self-attention helps models understand the context and relationships between different parts of a sentence, enhancing reading comprehension tasks[1][3].\\\\n   - **Abstractive Summarization**: By focusing on the most important words in a sentence, self-attention aids in generating summaries that capture the essence of the original text[1][3].\\\\n   - **Textual Entailment**: Self-attention helps models determine whether one sentence entails another by analyzing the relationships between words in both sentences[1][3].\\\\n   - **Learning Task-Independent Sentence Representations**: Self-attention enables models to learn representations of sentences that are useful across various tasks, improving overall NLP performance[1][3].\\\\n\\\\n2. **Computer Vision**:\\\\n   - **Image Classification**: Self-attention helps models capture long-range dependencies between image regions, improving image classification accuracy[1].\\\\n   - **Object Detection**: By focusing on relevant parts of an image, self-attention enhances object detection tasks[1].\\\\n   - **Image Captioning**: Self-attention aids in generating captions that accurately describe the content of an image by capturing relationships between different image regions[1].\\\\n\\\\n3. **Recommender Systems**:\\\\n   - **Personalized Recommendations**: Self-attention helps capture user preferences and item relationships, improving personalized recommendation systems[1].\\\\n\\\\nIn summary, self-attention is a powerful mechanism that enhances tasks like reading comprehension and learning sentence representations by allowing models to focus on relevant information and capture long-range dependencies within input sequences. Its successful applications span various domains, including NLP and computer vision.\",\"Figure 3 illustrates the attention mechanism in the encoder self-attention layer of a transformer model, specifically highlighting how it handles long-distance dependencies in natural language processing (NLP) tasks, such as understanding the impact of voter registration laws on electoral processes.\\\\n\\\\nHere\\\\u2019s a step-by-step explanation of what Figure 3 shows:\\\\n\\\\n1. **Contextual Understanding**: The figure demonstrates how the attention mechanism allows the model to understand the context of a specific word or phrase by considering its relationship with all other words in the input sequence. In this case, the word \\\\\"making\\\\\" is central to the attention visualization.\\\\n\\\\n2. **Long-Range Dependencies**: The attention heads (represented by different colors) are shown attending to distant dependencies of the verb \\\\\"making.\\\\\" This indicates that the model is capturing relationships between words that are not adjacent but are crucial for understanding the complete phrase or sentence. For example, the phrase \\\\\"making the registration or voting process more difficult\\\\\" requires understanding the context of \\\\\"making\\\\\" in relation to other words like \\\\\"registration,\\\\\" \\\\\"voting,\\\\\" and \\\\\"difficult.\\\\\"\\\\n\\\\n3. **Attention Heads**: Each color represents a different attention head, which is a set of query, key, and value vectors used to compute attention scores. These heads allow the model to attend to different parts of the sequence simultaneously, capturing a richer set of dependencies. In this example, multiple heads are attending to various parts of the sentence to complete the phrase \\\\\"making...more difficult.\\\\\"\\\\n\\\\n4. **Enhanced Contextual Understanding**: The visualization shows how the attention mechanism enhances the model&#x27;s ability to understand context by focusing on relevant parts of the input sequence. This is particularly beneficial in NLP tasks where understanding complex relationships between words is essential, such as in analyzing changes in voter registration laws and their impact on electoral processes.\\\\n\\\\nIn summary, Figure 3 illustrates how the attention mechanism in the encoder self-attention layer of a transformer model captures long-range dependencies and enhances contextual understanding, which is crucial for analyzing complex NLP tasks like the impact of voter registration laws on electoral processes[1][5].\",\"The third sub-layer in the decoder of a Transformer model is designed to perform multi-head attention over the output of the encoder stack. This sub-layer is crucial for ensuring that the predictions for position \\\\\\\\( i \\\\\\\\) in the output sequence depend only on the known outputs at positions less than \\\\\\\\( i \\\\\\\\).\\\\n\\\\nHere\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Multi-Head Attention in the Decoder**: The third sub-layer in the decoder performs multi-head attention. This involves running the attention mechanism multiple times in parallel, each time focusing on different aspects of the input sequence. The outputs from these multiple attention heads are then concatenated and linearly transformed to produce a single output vector for each position in the sequence[2][4].\\\\n\\\\n2. **Preventing Positions from Attending to Subsequent Positions**: To ensure that the predictions for position \\\\\\\\( i \\\\\\\\) do not depend on future positions, the decoder employs masking. This masking prevents the model from attending to positions beyond the current position \\\\\\\\( i \\\\\\\\). For example, if the model is generating the \\\\\\\\( i^{th} \\\\\\\\) token, it can only attend to tokens up to and including the \\\\\\\\( (i-1)^{th} \\\\\\\\) token[4].\\\\n\\\\n3. **Output Embeddings Offset**: Additionally, the output embeddings are offset by one position. This means that the embeddings for position \\\\\\\\( i \\\\\\\\) are generated based on the embeddings of positions \\\\\\\\( 0 \\\\\\\\) through \\\\\\\\( (i-1) \\\\\\\\). This offset ensures that the model&#x27;s predictions are based solely on the information available up to and including the previous position, thereby preventing it from using future information[4].\\\\n\\\\nBy combining these two mechanisms\\\\u2014masking and output embeddings offset\\\\u2014the decoder ensures that the predictions for position \\\\\\\\( i \\\\\\\\) depend only on the known outputs at positions less than \\\\\\\\( i \\\\\\\\), which is essential for generating coherent and contextually relevant output sequences.\",\"The limits of language modeling were explored by Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu in their 2016 preprint titled \\\\\"Exploring the limits of language modeling\\\\\" (arXiv:1602.02410).\\\\n\\\\nThe title of the work by Nal Kalchbrenner and colleagues related to neural machine translation is \\\\\"Neural machine translation in linear time\\\\\" (arXiv:1610.10099v2) .\",\"The authors of the paper titled &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio[5].\\\\n\\\\nThe title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions&#x27;.\",\"### Computational Complexity of Separable Convolutions\\\\n\\\\nSeparable convolutions significantly reduce the computational complexity of convolutional layers. The original complexity of a convolutional layer is generally \\\\\\\\(O(k \\\\\\\\cdot n \\\\\\\\cdot d + n \\\\\\\\cdot d^2)\\\\\\\\), where \\\\\\\\(k\\\\\\\\) is the number of filters, \\\\\\\\(n\\\\\\\\) is the number of input elements, and \\\\\\\\(d\\\\\\\\) is the dimensionality of the input features. However, separable convolutions break down this process into two separate operations: a depthwise convolution and a pointwise convolution. The depthwise convolution applies a filter to each element of the input independently, and the pointwise convolution applies a 1x1 convolution to each element of the output of the depthwise convolution. This approach reduces the complexity to \\\\\\\\(O(k \\\\\\\\cdot n \\\\\\\\cdot d + n \\\\\\\\cdot d^2)\\\\\\\\), which is a significant improvement over the original \\\\\\\\(O(k^2 \\\\\\\\cdot n \\\\\\\\cdot d)\\\\\\\\) complexity of standard convolutions[1][4].\\\\n\\\\n### Potential Benefit of Using Self-Attention\\\\n\\\\nA potential benefit of using self-attention in models is that it can yield more interpretable models. Self-attention mechanisms allow individual attention heads to learn different tasks and exhibit behavior related to the syntactic and semantic structure of the sentences. This is demonstrated by inspecting attention distributions from models, which show that individual attention heads clearly learn to perform different tasks and often exhibit behavior that aligns with the structural elements of the input data[1][5].\\\\n\\\\nIn summary, separable convolutions reduce the computational complexity of convolutional layers by breaking them down into simpler operations, while self-attention mechanisms can lead to more interpretable models by allowing individual attention heads to learn specific tasks and capture structural elements of the input data.\",\"### Main Point Expressed About the Law\\\\n\\\\nThe main point expressed about the law in the context is that while the law itself may never be perfect, its application should strive for justice. The speaker emphasizes that the issue lies not in the perfection of the law but in how it is applied, suggesting a need for more just application.\\\\n\\\\n### Figure 4 Illustration\\\\n\\\\nFigure 4 illustrates the involvement of two attention heads in layer 5 of a neural network architecture, specifically in the context of anaphora resolution. The figure shows:\\\\n\\\\n- **Full Attentions for Head 5**: This represents the overall attention distribution for the fifth attention head in layer 5.\\\\n- **Isolated Attentions from Just the Word \\\\u2018its\\\\u2019**: This part of the figure isolates the attention distribution specifically for the word \\\\u2018its\\\\u2019 within the fifth and sixth attention heads. The sharp attentions indicate that the model is focusing intensely on the word \\\\u2018its\\\\u2019, suggesting that it is crucial for resolving anaphora in this context.\\\\n\\\\nIn summary, Figure 4 highlights how the neural network is allocating attention to specific words, particularly \\\\u2018its\\\\u2019, to help resolve anaphora and understand the discourse structure of the text. This is relevant because anaphora resolution involves identifying and linking pronouns or other anaphors to their antecedents, which is essential for coherent text comprehension[2].\",\"To answer the question, let&#x27;s break down the computational complexity of both self-attention layers and recurrent layers based on the provided context.\\\\n\\\\n### Self-Attention Layers\\\\n\\\\nSelf-attention layers, which are a key component of transformer architectures, have a computational complexity that is generally quadratic in the input length. Specifically, the time complexity of self-attention is \\\\\\\\( O(n^2) \\\\\\\\), where \\\\\\\\( n \\\\\\\\) is the number of input tokens. This quadratic dependence on \\\\\\\\( n \\\\\\\\) poses significant computational challenges, especially for very long input sequences. Despite this, several approaches have been proposed to speed up self-attention mechanisms, such as using windowing, striding, hashing, or kernelization, to achieve sub-quadratic running times[1][4].\\\\n\\\\n### Recurrent Layers\\\\n\\\\nRecurrent Neural Networks (RNNs) introduce a mechanism where the output from one step is fed back as input to the next, allowing them to retain information from previous inputs. The computational complexity of RNNs is different from that of self-attention layers. Specifically, recurrent layers require \\\\\\\\( O(n) \\\\\\\\) sequential operations, where \\\\\\\\( n \\\\\\\\) is the number of time steps in the sequence. This is because each time step in an RNN depends on the previous state, and the network processes the sequence sequentially[3].\\\\n\\\\n### Comparison\\\\n\\\\n- **Self-Attention Layers**: \\\\\\\\( O(n^2) \\\\\\\\) time complexity, where \\\\\\\\( n \\\\\\\\) is the number of input tokens.\\\\n- **Recurrent Layers**: \\\\\\\\( O(n) \\\\\\\\) time complexity, where \\\\\\\\( n \\\\\\\\) is the number of time steps in the sequence.\\\\n\\\\nIn terms of computational complexity, self-attention layers are generally slower than recurrent layers for long sequences because their quadratic time complexity grows much faster than the linear time complexity of RNNs. However, it&#x27;s important to note that while RNNs are efficient for sequential data with a fixed length, they can struggle with very long sequences due to their sequential nature. In contrast, self-attention mechanisms are more scalable for longer sequences but require more computational resources due to their quadratic complexity[1][4].\\\\n\\\\n### Operations Required\\\\n\\\\n- **Self-Attention Layers**: These layers require computing attention scores between all pairs of tokens in the input sequence, which involves matrix multiplications and exponentiations. The exact operations include computing dot products, applying softmax, and scaling by attention weights[1].\\\\n- **Recurrent Layers**: These layers require updating the hidden state at each time step based on the current input and previous state. The operations include matrix multiplications, element-wise operations, and activation functions. The core formula for updating the hidden state is \\\\\\\\( h = \\\\\\\\sigma(U \\\\\\\\cdot X + W \\\\\\\\cdot h_{t-1} + B) \\\\\\\\), where \\\\\\\\( h \\\\\\\\) is the current hidden state, \\\\\\\\( U \\\\\\\\) and \\\\\\\\( W \\\\\\\\) are weight matrices, \\\\\\\\( X \\\\\\\\) is the current input, and \\\\\\\\( B \\\\\\\\) is a bias term[3].\\\\n\\\\nIn summary, while self-attention layers have a higher computational complexity due to their quadratic nature, recurrent layers are more efficient for sequential data with a fixed length due to their linear time complexity. However, both types of layers have their own strengths and are used in different contexts depending on the specific requirements of the task at hand.\",\"**Permission Granted by Google:**\\\\nGoogle grants permission to reproduce the tables and figures in the paper \\\\\"Attention Is All You Need\\\\\" solely for use in journalistic or scholarly works[1].\\\\n\\\\n**Main Innovation Proposed in the Paper:**\\\\nThe main innovation proposed in the paper \\\\\"Attention Is All You Need\\\\\" is the introduction of a new simple network architecture called the Transformer, which is based solely on attention mechanisms. This architecture dispenses with recurrence and convolutions, which were previously used in sequence transduction models[1][2]. The Transformer model uses self-attention mechanisms to weigh the importance of different parts of the input sequence, allowing it to capture long-range dependencies and contextual relationships more efficiently than traditional recurrent or convolutional neural networks[1][4].\",\"### Why the Researchers Chose the Sinusoidal Version of Positional Embeddings\\\\n\\\\nThe researchers chose the sinusoidal version of positional embeddings because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. This is particularly relevant in scenarios where the model needs to handle sequences of varying lengths, and the sinusoidal embeddings provide a more flexible and generalizable way to handle positional information[2].\\\\n\\\\n### Three Desiderata Considered for Using Self-Attention\\\\n\\\\nThe researchers considered the following three desiderata when using self-attention layers:\\\\n\\\\n1. **Total Computational Complexity per Layer**: The computational cost of each layer is a critical factor. Self-attention layers, particularly multi-head self-attention, can be computationally expensive but offer parallelization opportunities that can mitigate this cost[3].\\\\n\\\\n2. **Amount of Computation that Can Be Parallelized**: Self-attention layers allow for a significant amount of computation to be parallelized. This is because the attention mechanism computes attention weights for all elements in the sequence simultaneously, which can be done in parallel. This parallelization capability makes self-attention layers more efficient in terms of computational resources[3].\\\\n\\\\n3. **Mapping Variable-Length Sequences**: Self-attention layers are particularly useful for mapping variable-length sequences of symbol representations to another sequence of equal length. This is a common requirement in sequence transduction tasks such as machine translation or text summarization, where the input sequence can vary in length[3].\",\"The main architectural feature of the Transformer model proposed in the work is its reliance entirely on attention mechanisms to draw global dependencies between input and output, rather than using recurrence or convolutional neural networks. This allows the Transformer to significantly more parallelization compared to other models like Extended Neural GPU, ByteNet, and ConvS2S.\\\\n\\\\nHere\\\\u2019s a step-by-step comparison:\\\\n\\\\n1. **Transformer Model**:\\\\n   - **Architecture**: The Transformer model uses attention mechanisms to compute representations of its input and output without using sequence-aligned RNNs or convolutional neural networks[1][3].\\\\n   - **Parallelization**: The Transformer allows for significantly more parallelization due to its reliance on attention mechanisms, which enables it to process all positions in parallel. This results in faster training times and improved performance[3].\\\\n\\\\n2. **Extended Neural GPU**:\\\\n   - **Architecture**: The Extended Neural GPU uses convolutional neural networks as basic building blocks, computing hidden representations in parallel for all input and output positions[3].\\\\n   - **Parallelization**: While it computes hidden representations in parallel, the number of operations required to relate signals from two arbitrary input or output positions grows linearly with the distance between positions, which can limit its parallelization efficiency[3].\\\\n\\\\n3. **ByteNet**:\\\\n   - **Architecture**: ByteNet uses convolutional neural networks and employs a hierarchical approach to process sequences, with the number of operations growing logarithmically with the distance between positions[3].\\\\n   - **Parallelization**: Similar to the Extended Neural GPU, ByteNet&#x27;s parallelization is limited by the logarithmic growth of operations with distance, which can still be less efficient than the Transformer&#x27;s attention-based approach[3].\\\\n\\\\n4. **ConvS2S**:\\\\n   - **Architecture**: ConvS2S also uses convolutional neural networks and computes hidden representations in parallel for all input and output positions, with the number of operations growing linearly with the distance between positions[3].\\\\n   - **Parallelization**: Like the Extended Neural GPU and ByteNet, ConvS2S&#x27;s parallelization is limited by the linear growth of operations with distance, making it less efficient than the Transformer&#x27;s attention-based architecture[3].\\\\n\\\\nIn summary, the Transformer model&#x27;s reliance on attention mechanisms allows for significantly more parallelization compared to models like Extended Neural GPU, ByteNet, and ConvS2S, which are limited by their use of convolutional neural networks and sequential computation. This results in faster training times and improved performance for the Transformer model[3].\",\"**Purpose of Multi-Head Attention:**\\\\nThe purpose of multi-head attention in the model described is to allow the model to jointly attend to information from different representation subspaces at different positions. This is achieved by running the attention mechanism several times in parallel, each time with different weights and projections. This approach enables the model to capture a variety of dependencies and patterns in the input data, such as both short-term and long-term dependencies, by using multiple attention heads[1][3].\\\\n\\\\n**Number of Parallel Attention Layers or Heads:**\\\\nIn the model described, 8 parallel attention layers or heads are employed. Each head uses a reduced dimensionality, with \\\\\\\\(d_k = d_v = d_{\\\\\\\\text{model}} / h = 64\\\\\\\\), where \\\\\\\\(d_{\\\\\\\\text{model}}\\\\\\\\) is the embedding dimension and \\\\\\\\(h\\\\\\\\) is the number of heads[1][5]. This setup allows for efficient computation while enabling the model to attend to different aspects of the input data simultaneously.\",\"The main method for achieving the attention mechanism in the context of the provided text is through the use of **self-attention layers**. These layers allow elements in an entire sequence to attend to all other elements in the same sequence, which is particularly powerful in tasks where the context is crucial and the relationship between different elements is key to understanding the whole[3][5].\\\\n\\\\nIn the specific example given, the attention mechanism in layer 5 of the encoder self-attention helps to capture long-distance dependencies by focusing on relevant parts of the input sequence. For instance, the verb &#x27;making&#x27; is attended to by multiple heads, which helps in completing the phrase &#x27;making...more difficult&#x27; by considering the broader context of the sentence[1][5].\\\\n\\\\nThis dynamic weighting and contextual understanding enable the model to grasp complex language, long-range connections, and word ambiguity, making it more effective in processing complex texts[1][3].\",\"The context provided involves a detailed analysis of attention mechanisms in anaphora resolution, particularly focusing on the use of attention heads in transformer architectures. Here\\\\u2019s a step-by-step breakdown to understand the context and answer the question:\\\\n\\\\n1. **Understanding Anaphora Resolution**:\\\\n   - Anaphora resolution is a task in Natural Language Processing (NLP) that involves identifying the antecedent of an anaphor (usually a pronoun) in a sentence. This is crucial for maintaining coherence in text and dialogue[1][2].\\\\n\\\\n2. **Attention Mechanisms in NLP**:\\\\n   - Attention mechanisms are used in transformer architectures to dynamically allocate focus to specific parts of the input data. This helps in tasks like anaphora resolution by identifying the relevant parts of the sentence that the anaphor refers to[2].\\\\n\\\\n3. **Contextual Analysis**:\\\\n   - The provided context shows a figure (Figure 4) with two attention heads, specifically in layer 5 of 6, which are involved in anaphora resolution. The top part of the figure displays full attentions for head 5, while the bottom part isolates attentions from just the word \\\\u2018its\\\\u2019 for attention heads 5 and 6[2].\\\\n\\\\n4. **Interpretation of Attention Heads**:\\\\n   - The sharp attentions for the word \\\\u2018its\\\\u2019 indicate that the attention mechanism is focusing intensely on this specific word. This suggests that the model is trying to resolve the anaphor \\\\u2018its\\\\u2019 by identifying its antecedent, which is likely \\\\u2018the Law\\\\u2019 in this context[2].\\\\n\\\\n5. **Conclusion**:\\\\n   - The context indicates that the attention heads are being used to resolve an anaphor by focusing on specific words within the sentence. In this case, the attention is sharp for the word \\\\u2018its\\\\u2019, suggesting that it is being used to identify its antecedent in the sentence.\\\\n\\\\nGiven this analysis, the question \\\\\"What is it?\\\\\" likely refers to the specific word or phrase that the attention mechanism is focusing on, which in this context is the word \\\\u2018its\\\\u2019. The attention heads are sharpened for this word, indicating that it is a crucial part of the anaphora resolution process.\",\"The context provided involves anaphora resolution, which is a process in Natural Language Processing (NLP) aimed at identifying the antecedent of a pronoun or anaphor in a sentence. This process is crucial for understanding the relationships between different parts of a text and ensuring that pronouns refer to the correct entities.\\\\n\\\\n### What is Anaphora Resolution?\\\\n\\\\nAnaphora resolution involves identifying the noun phrase to which a pronoun or anaphor refers. This can be challenging because pronouns often lack explicit references, requiring the algorithm to use various sources of information such as morphological agreement, syntactic parallelism, semantic information, discourse structure, and topical knowledge[1].\\\\n\\\\n### Where Has It Been Applied?\\\\n\\\\nAnaphora resolution has been applied in various contexts:\\\\n\\\\n1. **Spanish Dialogue**: An algorithm called ARDi (anaphora resolution in dialogues) was developed to resolve pronouns and adjectival anaphors in Spanish dialogues. This algorithm was implemented in Prolog and uses a combination of linguistic knowledge and dialogue-structure knowledge to identify the correct antecedents[1].\\\\n\\\\n2. **Question Answering**: In the context of Question Answering, anaphora resolution is essential for providing precise answers. A tool called BRANQA was developed to improve the performance of Question Answering systems by resolving pronoun references using syntactic analysis and high-precision heuristic rules[4].\\\\n\\\\n3. **Code-Mixed Social Media**: Anaphora resolution has also been explored in code-mixed social media data, particularly for Hindi-English Twitter data. A modified encoder-decoder with attention model was proposed to address the challenges of anaphora resolution in this setting[5].\\\\n\\\\n4. **Transformer Architecture**: Recent studies have analyzed the attention mechanisms within transformer architectures to understand how they allocate focus during anaphora resolution. These studies aim to integrate insights from human attention patterns to enhance the performance of neural networks in resolving anaphoras effectively[2].\\\\n\\\\nIn summary, anaphora resolution is a critical task in NLP that has been applied in various contexts, including Spanish dialogues, Question Answering systems, and code-mixed social media data, using different algorithms and models to improve its accuracy.\",\"Based on the provided context, the Penn Treebank was utilized for training and semi-supervised settings. Specifically, it was used with approximately 40,000 training sentences from the WSJ newspaper text. Additionally, in a semi-supervised setting, it was combined with larger high-confidence and BerkleyParser corpora containing approximately 17 million sentences. The vocabulary used varied between 16,000 tokens for the WSJ only setting and 32,000 tokens for the semi-supervised setting.\\\\n\\\\nHere is a step-by-step breakdown:\\\\n1. **Penn Treebank**: The primary dataset used for training, consisting of about 40,000 sentences from the WSJ newspaper text.\\\\n2. **Semi-supervised setting**: The Penn Treebank was combined with larger datasets (high-confidence and BerkleyParser corpora) containing approximately 17 million sentences.\\\\n3. **Vocabulary**: The vocabulary size varied depending on the setting:\\\\n   - For the WSJ only setting, a vocabulary of 16,000 tokens was used.\\\\n   - For the semi-supervised setting, a vocabulary of 32,000 tokens was used.\\\\n\\\\nTherefore, the Penn Treebank was utilized in both its standalone form and as part of a larger semi-supervised dataset.\",\"The context provided describes a scenario where multiple attention heads in the Transformer architecture are analyzing an input sequence. Specifically, it mentions that many of the attention heads exhibit behavior related to the structure of the sentence. This is a characteristic of **Multi-head Attention**, a technique used in the Transformer model to capture richer interpretations of the input sequence by splitting the attention mechanism into multiple parallel attention heads.\\\\n\\\\nHere\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Input Sequence**: The input sequence is \\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion.\\\\\"\\\\n\\\\n2. **Encoder Self-Attention**: The input sequence is fed into the Encoder, which applies self-attention to itself. This means each word in the sequence is compared to every other word to determine the relevance of each word to the context.\\\\n\\\\n3. **Multi-head Attention**: The self-attention mechanism is split into multiple parallel attention heads. Each head processes a different aspect of the input sequence, allowing the model to capture a variety of relationships and nuances within the text.\\\\n\\\\n4. **Behavior of Attention Heads**: The context mentions that many of the attention heads exhibit behavior related to the structure of the sentence. This implies that each head is focusing on different aspects of the sentence, such as grammatical structure, semantic meaning, or contextual relevance.\\\\n\\\\n5. **Examples**: The figure mentioned shows two examples from different heads in the encoder self-attention at layer 5 of 6. These heads have clearly learned to perform different tasks, indicating that they are capturing distinct aspects of the input sequence.\\\\n\\\\nIn summary, the attention heads in this context are performing **Multi-head Attention**, which allows the Transformer model to analyze the input sequence in a more nuanced and detailed manner by splitting the attention mechanism into multiple parallel heads[1][2].\",\"The context provided describes the behavior of attention heads in the Transformer model, specifically in the encoder self-attention at layer 5 of 6. The attention heads are exhibiting behavior related to the structure of the sentence, indicating that they have learned to perform different tasks.\\\\n\\\\nTo answer the question step by step:\\\\n\\\\n1. **Understanding Attention Heads**: In the Transformer model, the attention mechanism is repeated multiple times in parallel, each repetition being called an attention head. These heads process the input sequence independently and then combine their outputs to produce a final attention score[1][2].\\\\n\\\\n2. **Encoder Self-Attention**: In the encoder, self-attention is used to allow the model to consider different positions of the same sequence when computing the representation of a word. This enables the model to capture long-range dependencies and relationships within the input sequence[1][2].\\\\n\\\\n3. **Behavior of Attention Heads**: The provided context mentions that many of the attention heads exhibit behavior related to the structure of the sentence. This means that each head has learned to focus on different aspects or relationships within the sentence, such as grammatical structure, semantic meaning, or contextual relevance.\\\\n\\\\n4. **Example from the Context**: The specific example from layer 5 of 6 shows that different heads have clearly learned to perform different tasks. This is indicative of the multi-head attention mechanism&#x27;s ability to capture richer interpretations of the sequence by splitting the embedding vectors across multiple heads[1].\\\\n\\\\nTherefore, the attention heads in this context are learning to focus on various aspects of the sentence structure, such as grammatical relationships, semantic meaning, and contextual relevance, which are essential for tasks like language understanding and translation.\",\"The context provided does not mention specific individuals or entities. It appears to be a description of a text input to a model, likely for natural language processing (NLP) purposes, and discusses the behavior of attention heads in the encoder self-attention layer of a transformer model. The text input is a repetition of a sentence about the imperfection of a law and its application.\\\\n\\\\nTherefore, there is no clear answer to the question \\\\\"Who are they?\\\\\" based on the given context. If you have more specific information or context, please provide it for a more accurate response.\",\"The context provided involves a discussion about anaphora resolution, which is a task in Natural Language Processing (NLP) that deals with identifying and resolving pronouns or other anaphors to their corresponding antecedents in a text. Here\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Anaphora Resolution**: This is the process of identifying pronouns or other anaphors and linking them to their antecedents in a text. For example, in the sentence \\\\\"The Law will never be perfect, but its application should be just - this is what we are missing, in my opinion,\\\\\" the pronoun \\\\\"its\\\\\" is an anaphor that needs to be resolved to its antecedent, which is \\\\\"The Law.\\\\\"\\\\n\\\\n2. **Attention Mechanisms**: The provided context mentions \\\\\"attention heads\\\\\" and \\\\\"full attentions\\\\\" for layer 5 of 6, indicating that attention mechanisms are being used to focus on specific parts of the input data. This is particularly relevant in transformer architectures, where attention mechanisms help neural networks allocate focus to specific aspects of the input data, such as identifying the antecedent of an anaphor[2].\\\\n\\\\n3. **Anaphora Resolution in Context**: The figure described shows two attention heads involved in anaphora resolution, with sharp attentions for the word \\\\\"its.\\\\\" This suggests that the model is using attention to pinpoint the correct antecedent for the pronoun \\\\\"its,\\\\\" which is essential for resolving anaphora correctly.\\\\n\\\\n4. **Human Attention vs. Machine Attention**: Research has shown that integrating insights from human attention patterns can significantly improve the performance of neural networks in tasks like anaphora resolution. This involves analyzing human eye movements and fixations during tasks like reading and comprehension to better understand how humans process anaphora[2].\\\\n\\\\nIn summary, the context provided is about using attention mechanisms within transformer architectures to resolve anaphora, specifically focusing on identifying the antecedent of a pronoun like \\\\\"its\\\\\" in a given text. This process involves dynamic allocation of focus to specific parts of the input data to ensure accurate resolution of anaphora.\",\"The context provided does not explicitly mention specific scores related to BLEU or any other evaluation metric. It discusses the training and experimental setup for a machine translation model, including the use of Penn Treebank and other corpora, but it does not provide numerical results or scores.\\\\n\\\\nHowever, if we were to interpret the context in terms of potential performance metrics, it would typically involve evaluating the model&#x27;s performance using metrics like BLEU scores. BLEU scores measure the similarity between machine-translated text and human reference translations, with higher scores indicating better quality translations.\\\\n\\\\nTo answer the question directly based on the provided context:\\\\n- **BLEU Scores**: These are not explicitly mentioned in the given context. However, if the model were evaluated using BLEU, it would typically involve comparing the machine-translated output to human reference translations and calculating a score between 0 and 1, with higher values indicating better similarity to the reference translations[1][2][4]. \\\\n\\\\nFor a detailed understanding of BLEU scores and their calculation, you would need to refer to specific experimental results or evaluations conducted on the model, which are not provided in the given context.\",\"The option selected for the attention mechanism in the context provided is likely due to its ability to handle long-range dependencies effectively. Here\\\\u2019s a step-by-step explanation:\\\\n\\\\n1. **Understanding Long-Range Dependencies**: The attention mechanism is particularly useful in natural language processing (NLP) tasks because it can capture relationships between tokens that are far apart in the sequence. This is crucial for tasks like understanding the meaning of a sentence where the relationship between words may not be sequential[2].\\\\n\\\\n2. **Traditional Models&#x27; Limitations**: Traditional recurrent neural networks (RNNs) and long short-term memory networks (LSTMs) struggle with capturing long-range dependencies due to their sequential processing nature and the vanishing gradient problem. These models update the hidden state at each step, which makes it difficult to preserve information from earlier tokens as the sequence length increases[2].\\\\n\\\\n3. **Self-Attention Mechanism**: The self-attention mechanism introduced in the transformer model addresses these limitations by allowing the model to directly compute the relevance of each token to every other token in the sequence, regardless of their distance. This is achieved through the computation of attention scores, which determine the importance of each token relative to others[2].\\\\n\\\\n4. **Application in NLP Tasks**: In the context of NLP, the self-attention mechanism is particularly useful for tasks like completing phrases or understanding complex sentences. For example, in the phrase \\\\\"making...more difficult,\\\\\" the attention mechanism can attend to the distant dependency of the verb &#x27;making&#x27; to complete the phrase correctly. This is illustrated in Figure 3, where different attention heads attend to different parts of the sentence to capture long-range dependencies[2].\\\\n\\\\n5. **Advantages Over Traditional Models**: The self-attention mechanism offers several advantages over traditional models, including parallelization, flexibility, scalability, and the ability to handle long-range dependencies effectively. These advantages make it a cornerstone of modern NLP architectures[2].\\\\n\\\\nGiven these points, the attention mechanism was selected because it is well-suited for handling long-range dependencies in NLP tasks, which is essential for understanding and completing phrases like \\\\\"making...more difficult.\\\\\"\"]],[\"reference_answer\",[\"The Transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Many of the attention heads exhibit behavior that seems related to the structure of the sentence. The heads clearly learned to perform different tasks.\",\"The model achieved a new state of the art on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The paper proposes a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The two most commonly used attention functions are additive attention and dot-product (multiplicative) attention.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"Using positional embedding instead of sinusoids results in a BLEU score of 4.92 compared to 25.7.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The attention heads in layer 5 of 6 appear to be involved in anaphora resolution.\",\"The base models were trained for a total of 100,000 steps or 12 hours. The big models were trained for 300,000 steps (3.5 days).\",\"To evaluate the importance of different components of the Transformer, measuring the change in performance on English-to-German translation.\",\"Romain Paulus, Caiming Xiong, and Richard Socher.\",\"The dot products are scaled by 1/\\\\u221adk to counteract the effect of large values of dk, which cause the dot products to grow large in magnitude and push the softmax function into regions where it has extremely small gradients.\",\"The Transformer achieved an F1 score of 92.7 in the semi-supervised setting.\",\"The Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\",\"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\",\"Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor.\",\"The base models were trained for a total of 100,000 steps or 12 hours.\",\"Self-attention layers are faster than recurrent layers because they require O(1) sequential operations, whereas a recurrent layer requires O(n) sequential operations.\",\"Reducing the attention key size dk hurts model quality.\",\"The BLEU score for the &#x27;big&#x27; Transformer model variation is 26.4.\",\"The task used to evaluate if the Transformer can generalize to other tasks was English constituency parsing.\",\"The Transformer (big) model achieves BLEU scores of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The big model achieved a BLEU score of 41.0.\",\"As a side benefit, self-attention could yield more interpretable models.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"In this work we employ h = 8 parallel attention layers, or heads.\",\"The purpose of masking in the decoder&#x27;s self-attention layers is to prevent leftward information flow in the decoder to preserve the auto-regressive property by masking out (setting to \\\\u2212\\\\u221e) all values in the input of the softmax which correspond to illegal connections.\",\"The model achieved a new state of the art in the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\",\"Dot-product attention includes a scaling factor of 1/\\\\u221adk, whereas additive attention computes the compatibility function using a feed-forward network with a single hidden layer.\",\"The Transformer outperforms the Berkeley-Parser even when training only on the WSJ training set of 40K sentences, and it performs surprisingly well compared to RNN sequence-to-sequence models.\",\"The dimensionality of input and output is dmodel = 512.\",\"A vocabulary of 32K tokens was used for the semi-supervised setting.\",\"Self-attention layers are considered faster than recurrent layers in terms of computational complexity because they require O(1) executed operations, whereas a recurrent layer requires O(n) sequential operations.\",\"The big transformer model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"Figure 3 illustrates the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6, where many of the attention heads attend to a distant dependency of the verb \\\\u2018making\\\\u2019, completing the phrase \\\\u2018making...more difficult\\\\u2019.\",\"The third sub-layer in the decoder performs multi-head attention over the output of the encoder stack, and the decoder ensures that predictions for position i depend only on known outputs at positions less than i by modifying the self-attention sub-layer to prevent positions from attending to subsequent positions, and the output embeddings are offset by one position.\",\"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu explored the limits of language modeling in a 2016 arXiv preprint, and the title of the work by Nal Kalchbrenner and colleagues is &#x27;Neural machine translation in linear time.&#x27;\",\"The authors of the paper &#x27;Learning phrase representations using rnn encoder-decoder for statistical machine translation&#x27; are Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio, and the title of the paper authored by Francois Chollet in 2016 is &#x27;Xception: Deep learning with depthwise separable convolutions.&#x27;\",\"Separable convolutions decrease the complexity considerably to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2), even though with k = n, the complexity is equal to the combination of a self-attention layer and a point-wise feed-forward layer; a side benefit of self-attention is that it could yield more interpretable models, as individual attention heads learn to perform different tasks and exhibit behavior related to the syntactic and semantic structure of sentences.\",\"The main point expressed is that the law will never be perfect, but its application should be just, which is considered to be missing; Figure 4 illustrates two attention heads in layer 5 of 6, involved in anaphora resolution, showing full attentions for head 5 and isolated attentions from the word &#x27;its&#x27; for attention heads 5 and 6, with very sharp attentions noted for this word.\",\"Self-attention layers are faster than recurrent layers in terms of computational complexity when the sequence is considered, and recurrent layers require O(n) sequential operations.\",\"Google grants permission to reproduce the tables and figures in the paper solely for use in journalistic or scholarly works, provided proper attribution is given, and the main innovation proposed in the paper is a new simple network architecture called the Transformer, which is based solely on attention mechanisms, dispensing with recurrence and convolutions.\",\"The researchers chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training, and the three desiderata considered for using self-attention are the total computational complexity per layer, the amount of computation that can be parallelized, and the minimum number of sequential operations required.\",\"The main architectural feature of the Transformer model is that it eschews recurrence and relies entirely on an attention mechanism to draw global dependencies between input and output, allowing for significantly more parallelization compared to models like Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks and have operations that grow with the distance between input or output positions.\",\"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, and in this work, 8 parallel attention layers or heads are employed.\",\"The Transformer model relies entirely on an attention mechanism to draw global dependencies between input and output.\",\"Structured attention networks.\",\"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It has been used successfully in tasks including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\",\"The standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.\",\"The three factors mentioned are: the total computational complexity per layer, the amount of computation that can be parallelized (measured by the minimum number of sequential operations required), and the path length between long-range dependencies in the network.\",\"The Transformer uses multi-head attention in three different ways: 1) In &#x27;encoder-decoder attention&#x27; layers, where queries come from the previous decoder layer and the memory keys and values come from the output of the encoder. 2) In encoder self-attention layers, where all keys, values, and queries come from the output of the previous layer in the encoder. 3) In decoder self-attention layers, allowing each position in the decoder to attend to all positions in the decoder up to and including that position.\",\"Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\",\"Neural machine translation by jointly learning to align and translate.\",\"The Transformer (big) model achieves a BLEU score of 28.4 for English-to-German and 41.8 for English-to-French translations.\",\"The sinusoidal version was chosen because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"]],[\"id\",[1,51,39,29,0,36,16,14,45,34,26,50,28,32,45,15,37,38,8,3,28,24,34,33,35,29,31,26,30,16,18,39,14,38,19,36,24,30,7,49,11,43,41,26,50,24,0,22,6,16,6,43,7,27,23,17,47,40,29,22]],[\"content\",[\"mechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On t...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"inference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different componen...\",\"[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\\\\u2013330, 1993.\\\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\\\npages 152\\\\u2013159. ACL, June 2006.\\\\n[27] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel...\",\"dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\\\\u221adk\\\\n.\\\\n3.2.2 Multi-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h tim...\",\"Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser Training WSJ 23 F1\\\\nVinyals &amp; Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\\\nTransformer (4 layers) WSJ only, discriminative 91.3\\\\nZhu et al. (2013) [40] semi-supervised 91.3\\\\nHuang &amp; Harper (2009) [14] semi-supervis...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using se...\",\"efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\",\"We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3 Optimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the le...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, ...\",\"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d model dff h d k dv Pdrop \\\\u03f5ls\\\\ntrain PPL BLEU params\\\\nsteps (dev) (dev) \\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)\\\\n1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 3...\",\"bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3 English Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"the competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For t...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"encoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3 Position-wise Feed-Forward Networks\\\\nIn ad...\",\"on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms t...\",\"into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V) = softmax(QKT\\\\n\\\\u221adk\\\\n)V (1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof 1\\\\u221adk\\\\n. Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While t...\",\"for both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8].\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7 Conclusion\\\\nIn this work, we presente...\",\"FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3.4 Embeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vector...\",\"Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development s...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing During training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6 Results\\\\n6.1 Machine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including e...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"Attention Visualizations\\\\nInput-Input Layer5\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nIt\\\\nis\\\\nin\\\\nthis\\\\nspirit\\\\nthat\\\\na\\\\nmajority\\\\nof\\\\nAmerican\\\\ngovernments\\\\nhave\\\\npassed\\\\nnew\\\\nlaws\\\\nsince\\\\n2009\\\\nmaking\\\\nthe\\\\nregistration\\\\nor\\\\nvoting\\\\nprocess\\\\nmore\\\\ndifficult\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\n&lt;pad&gt;\\\\nFigure 3: An example of the attention mechanism following long-dista...\",\"itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by la...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR, abs/1406.1078, 2014.\\\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357, 2016.\\\\n[7] Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeli...\",\"or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2). Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe ap...\",\"Input-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nInput-Input Layer5\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat\\\\nwe\\\\nare\\\\nmissing\\\\n,\\\\nin\\\\nmy\\\\nopinion\\\\n.\\\\n&lt;EOS&gt;\\\\n&lt;pad&gt;\\\\nThe\\\\nLaw\\\\nwill\\\\nnever\\\\nbe\\\\nperfect\\\\n,\\\\nbut\\\\nits\\\\napplication\\\\nshould\\\\nbe\\\\njust\\\\n-\\\\nthis\\\\nis\\\\nwhat...\",\"executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\",\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217 \\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"output values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\\\nwhere headi = Attention(QWQ\\\\ni , KWK\\\\ni , V WV\\\\ni )\\\\nWhere the projections are parameter matricesWQ\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WK\\\\ni \\\\u2208 Rdmodel\\\\u00d7dk , WV\\\\ni \\\\u2208 Rd...\",\"In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ...\",\"across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\\\nLanguage Processing, pages 832\\\\u2013841. ACL, August 2009.\\\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\\\n[16] \\\\u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS), 2016.\\\\n[17] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs l...\",\"in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism rela...\",\"and semantic structure of the sentences.\\\\n5 Training\\\\nThis section describes the training regime for our models.\\\\n5.1 Training Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens...\",\"One is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the netwo...\",\"The Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention la...\",\"nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\\\nLearning Research, 15(1):1929\\\\u20131958, 2014.\\\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28, pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with ...\",\"comments, corrections and inspiration.\\\\nReferences\\\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450, 2016.\\\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\\\n[4] Jianpeng Cheng,...\",\"Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU Training Cost (FLOPs)\\\\nEN-DE EN-FR EN-DE EN-FR\\\\nByteNet [18] 23.75\\\\nDeep-Att + PosUnk [39] 39.2 1.0 \\\\u00b7 1020\\\\nGNMT + RL [38] 24.6 39.92 2.3 \\\\u00b7 1019 1.4 \\\\u00b7 1020\\\\nConvS2S [9] 25.16 40.46 9.6 \\\\u00b7 1018 1.5 \\\\u00b7 1020\\\\nMoE [32] 26.03 40.56 2.0 \\\\u00b7 1019 1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 \\\\u00b7 1020\\\\nGNMT + ...\",\"P Epos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length s...\"]],[\"color\",[\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1833\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1834\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1829\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1830\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1831\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1847\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1838\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1839\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1840\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JyQ9QMcqckDkoYtAoqeXQDN1TEBOQ1FANP9KQO1jD0B0ND9AyxPKPweQIUC2f+U/qhLXP+ip1j/TX+s/1m8RQODBBUDVDwVAyjv4PzBwAEAOoiY/XJmZP9uoqD/W41k/Inq1P0hEuj4lHKc+mt2YQOfde0BeO2RA2Op4QEj8hkAxn01ATxmNQL65NUBJuaNAo3+xQBGRnUBEzo1Av5aAQISQikAD0mBADtZWQHQFiUClwH5AiGawQH7RoEDxxIRAa76fQCrMtT51hmg/pA9tPw==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"6MMMQSwQ8UBvyAFBAPcJQbzrFEEj9gpBZRv5QAk8AUEnBARBn+HUQA3P7kBYeO5A4MMcQabAEkEvaCFBg4UcQYbxDkGFYQdBYRTkQETHyEA5v+5A6eDOQI8r9kB1XwJB2VkGQUXO4kCUcAJB2FvXQLKbyEC3YcdAjUbbQPzw3kBGbM9ATom+QBic4UA1Dv5A6R3jQL9T60A0evxAU6oGQeIRIkFwHhxBGb8nQZVuFEFGjxpBbZYSQb9RIkGBBStBWAwZQZ7nE0H/Jw5BoywXQQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1848\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1849\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1844\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1845\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1846\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1801\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1816\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1817\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1818\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1819\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1824\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1825\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1792\",\"attributes\":{\"renderers\":[{\"id\":\"p1832\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1811\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1812\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1813\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1814\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1806\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1807\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1808\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1809\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1810\",\"attributes\":{\"axis\":{\"id\":\"p1806\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1815\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1811\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1835\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1836\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1832\"}],\"index\":3}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1837\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1832\"}],\"index\":0}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1853\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1850\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1851\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1852\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"MsVdQNX8hkCz0L0/P0X4Pw==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"JQ4OQWx710AVgvJANvYVQQ==\"},\"shape\":[4],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Transformer Model in Machine Learning\",\"Transformer Model in Machine Translation\",\"Self-Attention Mechanism\",\"Attention Mechanisms in Neural Networks\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"915122a9-83b5-4118-9d98-84098b6755ab\",\"roots\":{\"p1858\":\"a52925a5-0097-49c2-bfe5-f29057999165\"},\"root_ids\":[\"p1858\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"a52925a5-0097-49c2-bfe5-f29057999165\" data-root-id=\"p1858\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x17816cba0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      0.8\n",
       "conversational               0.0\n",
       "distracting element          0.4\n",
       "double                       0.8\n",
       "simple                       0.8\n",
       "situational                  0.8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
